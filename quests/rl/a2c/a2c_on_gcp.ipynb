{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients and A2C\n",
    "\n",
    "In the <a href=\"../dqn/dqns_on_gcp.ipynb\">previous notebook</a>, we learned how to use hyperparameter tuning to help DQN agents balance a pole on a cart. In this notebook, we'll explore two other types of alogrithms: Policy Gradients and A2C.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Hypertuning takes some time, and in this case, it can take anywhere between **10 - 30 minutes**. If this hasn't been done already, run the cell below to kick off the training job now. We'll step through what the code is doing while our agents learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.5\n",
    "# !pip install numpy==1.21.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please ignore any incompatibility warnings and errors. Restart the kernel if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "BUCKET=<your-bucket-here> # Change to your bucket name\n",
    "JOB_NAME=pg_on_gcp_$(date -u +%y%m%d_%H%M%S)\n",
    "REGION='us-central1' # Change to your bucket region\n",
    "IMAGE_URI=gcr.io/cloud-training-prod-bucket/pg:latest\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --scale-tier=BASIC \\\n",
    "    --job-dir=gs://$BUCKET/$JOB_NAME \\\n",
    "    --config=templates/hyperparam.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install gym==0.12.5 --user"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart the kernel if the above libraries needed to be installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, we can use the same environment for these algorithms as DQN, so this notebook will focus less on the operational work of feeding our agents the data, and more on the theory behind these algorthims. Let's start by loading our libraries and environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:42:01.992636Z",
     "start_time": "2024-05-24T09:41:54.670250Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "CLIP_EDGE = 1e-8\n",
    "\n",
    "def print_state(state, step, reward=None):\n",
    "    format_string = 'Step {0} - Cart X: {1:.3f}, Cart V: {2:.3f}, Pole A: {3:.3f}, Pole V:{4:.3f}, Reward:{5}'\n",
    "    print(format_string.format(step, *tuple(state), reward))\n",
    "    \n",
    "env = gym.make('CartPole-v0')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 09:41:55.771972: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 09:41:56.581749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory Behind Policy Gradients\n",
    "\n",
    "Whereas Q-learning attempts to assign each state a value, Policy Gradients tries to find actions directly, increasing or decreaing a chance to take an action depending on how an episode plays out.\n",
    "\n",
    "To compare, Q-learning has a table that keeps track of the value of each combination of state and action:\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 1 | .5 | -1 |\n",
    "| Hungry | .5 | 1 | 0 |\n",
    "| Full | -1 | -.5 | 1.5 |\n",
    "\n",
    "Instead for Policy Gradients, we can imagine that we have a similar table, but instead of recording the values, we'll keep track of the probability to take the column action given the row state.\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 70% | 20% | 10% |\n",
    "| Hungry | 30% | 50% | 20% |\n",
    "| Full | 5% | 15% | 80% |\n",
    "\n",
    "With Q learning, whenever we take one step in our environment, we can update the value of the old state based on the value of the new state plus any rewards we picked up based on the [Q equation](https://en.wikipedia.org/wiki/Q-learning):\n",
    "\n",
    "<img style=\"background-color:white;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637\">\n",
    "\n",
    "Could we do the same thing if we have a table of probabilities instead values? No, because we don't have a way to calculate the value of each state from our table. Instead, we'll use a different <a href=\"http://incompleteideas.net/papers/sutton-88-with-erratum.pdf\"> Temporal Difference Learning</a> strategy.\n",
    "\n",
    "Q Learning is an evolution of TD(0), and for Policy Gradients, we'll use TD(1). We'll calculate TD(1) accross and entire episode, and use that to indicate whether to increase or decrease the probability correspoding to the action we took. Let's look at a full day of eating.\n",
    "\n",
    "| Hour | State | Action | Reward |\n",
    "|-|-|-|-|\n",
    "|9| Hangry | Wait | -.9 |\n",
    "|10| Hangry | Meal | 1.2 |\n",
    "|11| Full | Wait | .5 |\n",
    "|12| Full | Snack | -.6 |\n",
    "|13| Full | Wait | 1 |\n",
    "|14| Full | Wait | .6 |\n",
    "|15| Full | Wait | .2 |\n",
    "|16| Hungry | Wait | 0 |\n",
    "|17| Hungry | Meal | .4 |\n",
    "|18| Full | Wait| .5 |\n",
    "\n",
    "We'll work backwards from the last day, using the same discount, or `gamma`, as we did with DQNs. The `total_rewards` variable is equivalent to the value of state prime. Using the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation), everytime we calculate the value of a state, s<sub>t</sub>, we'll set that as the value of state prime for the state before, s<sub>t-1</sub>. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:51:27.432355Z",
     "start_time": "2024-05-24T09:51:27.418516Z"
    }
   },
   "source": [
    "test_gamma = .5  # Please change me to be between zero and one\n",
    "episode_rewards = [-.9, 1.2, .5, -.6, 1, .6, .2, 0, .4, .5]\n",
    "\n",
    "def discount_episode(rewards, gamma):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        total_rewards = rewards[t] + total_rewards * gamma\n",
    "        discounted_rewards[t] = total_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "discount_episode(episode_rewards, test_gamma)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16308594,  1.47382812,  0.54765625,  0.0953125 ,  1.390625  ,\n",
       "        0.78125   ,  0.3625    ,  0.325     ,  0.65      ,  0.5       ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wherever our discounted reward is positive, we'll increase the probability corresponding to the action we took. Similarly, wherever our discounted reward is negative, we'll decrease the probabilty.\n",
    "\n",
    "However, with this strategy, any actions with a positive reward will have it's probability increase, not necessarily the most optimal action. This puts us in a feedback loop, where we're more likely to pick less optimal actions which could further increase their probability. To counter this, we'll divide the size of our increases by the probability to choose the corresponding action, which will slow the growth of popular actions to give other actions a chance.\n",
    "\n",
    "Here is our update rule for our neural network, where alpha is our learning rate, and pi is our optimal policy, or the probability to take the optimal action, a<sup>*</sup>, given our current state, s. \n",
    "\n",
    "<img src=\"images/weight_update.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "Doing some fancy calculus, we can combine the numerator and denominator with a log function. Since it's not clear what the optimal action is, we'll instead use our discounted rewards, or G, to increase or decrease the weights of the respective action the agent took. A full breakdown of the math can be found in [this article by Chris Yoon](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63).\n",
    "\n",
    "<img src=\"images/weight_update_calculus.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Below is what it looks like in code. `y_true` is the [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) of the action that was taken. `y_pred` is the probabilty to take each action given the state the agent was in."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:51:28.572300Z",
     "start_time": "2024-05-24T09:51:28.566916Z"
    }
   },
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE)\n",
    "    log_likelihood = y_true * K.log(y_pred_clipped)\n",
    "    return K.sum(-log_likelihood*g)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have the discounted rewards, or `g`, when our agent is acting in the environment. No problem, we'll have one neural network with two types of pathways. One pathway, `predict`, will be the probability to take an action given an inputed state. It's only used for prediction and is not used for backpropogation. The other pathway, `policy`, will take both a state and a discounted reward, so it can be used for training.\n",
    "\n",
    "The code in its entirety looks like this. As with Deep Q Networks, the hidden layers of a Policy Gradient can use a CNN if the input state is pixels, but the last layer is typically a [Dense](https://keras.io/layers/core/) layer with a [Softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function to convert the output into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:51:29.705697Z",
     "start_time": "2024-05-24T09:51:29.697141Z"
    }
   },
   "source": [
    "def build_networks(\n",
    "        state_shape, action_size, learning_rate, hidden_neurons):\n",
    "    \"\"\"Creates a Policy Gradient Neural Network.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by the discounted reward, g.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden\n",
    "            layer.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    g = layers.Input((1,), name='g')\n",
    "\n",
    "    hidden_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    hidden_2 = layers.Dense(hidden_neurons, activation='relu')(hidden_1)\n",
    "    probabilities = layers.Dense(action_size, activation='softmax')(hidden_2)\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE)\n",
    "        log_lik = y_true*K.log(y_pred_clipped)\n",
    "        return K.sum(-log_lik*g)\n",
    "\n",
    "    policy = models.Model(\n",
    "        inputs=[state_input, g], outputs=[probabilities])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    policy.compile(loss=custom_loss, optimizer=optimizer)\n",
    "\n",
    "    predict = models.Model(inputs=[state_input], outputs=[probabilities])\n",
    "    return policy, predict"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a taste of how these networks function. Run the below cell to build our test networks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:51:31.045590Z",
     "start_time": "2024-05-24T09:51:30.861426Z"
    }
   },
   "source": [
    "space_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Feel free to play with these\n",
    "test_learning_rate = .2\n",
    "test_hidden_neurons = 10\n",
    "\n",
    "test_policy, test_predict = build_networks(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't use the policy network until we build our learning function, but we can feed a state to the predict network so we can see our chances to pick our actions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:51:32.800970Z",
     "start_time": "2024-05-24T09:51:32.497771Z"
    }
   },
   "source": [
    "state = env.reset()[0]\n",
    "test_predict.predict(np.expand_dims(state, axis=0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 186ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49636373, 0.50363624]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the numbers should be close to `[.5, .5]`, with a little bit of variance due to the randomization of initializing the weights and the cart's starting position. In order to train, we'll need some memories to train on. The memory buffer here is simpler than DQN, as we don't have to worry about random sampling. We'll clear the buffer every time we train as we'll only hold one episode's worth of memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:51:33.915523Z",
     "start_time": "2024-05-24T09:51:33.909556Z"
    }
   },
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay buffer for Policy Gradient methods.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess TD(1) values.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: a (state, action, reward) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns the list of episode experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure (\n",
    "                [states], [actions], [rewards]\n",
    "            }\n",
    "        \"\"\"\n",
    "        batch = np.array(self.buffer, dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a fake buffer to get a sense of the data we'll be training on. The cell below initializes our memory and runs through one episode of the game by alternating pushing the cart left and right.\n",
    "\n",
    "Try running it to see the data we'll be using for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:59:00.763062Z",
     "start_time": "2024-05-24T09:59:00.637370Z"
    }
   },
   "source": [
    "test_memory = Memory(test_gamma)\n",
    "actions = [x % 2 for x in range(200)]\n",
    "state = env.reset()[0]\n",
    "step = 0\n",
    "episode_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done and step < len(actions):\n",
    "    action = actions[step]  # In the future, our agents will define this.\n",
    "    state_prime, reward, done1, done2, info = env.step(action)\n",
    "    done = done1 or done2\n",
    "    episode_reward += reward\n",
    "    test_memory.add((state, action, reward))\n",
    "    step += 1\n",
    "    state = state_prime\n",
    "\n",
    "test_memory.sample()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(33, 4), dtype=float32, numpy=\n",
       " array([[ 4.26567346e-02, -3.37042026e-02, -1.45590852e-03,\n",
       "          8.59619118e-03],\n",
       "        [ 4.19826508e-02, -2.28805244e-01, -1.28398463e-03,\n",
       "          3.00819397e-01],\n",
       "        [ 3.74065451e-02, -3.36650126e-02,  4.73240344e-03,\n",
       "          7.73181301e-03],\n",
       "        [ 3.67332473e-02, -2.28854507e-01,  4.88703977e-03,\n",
       "          3.01904112e-01],\n",
       "        [ 3.21561545e-02, -3.38025540e-02,  1.09251216e-02,\n",
       "          1.07664447e-02],\n",
       "        [ 3.14801037e-02, -2.29079470e-01,  1.11404508e-02,\n",
       "          3.06876272e-01],\n",
       "        [ 2.68985145e-02, -3.41180228e-02,  1.72779765e-02,\n",
       "          1.77274588e-02],\n",
       "        [ 2.62161549e-02, -2.29483441e-01,  1.76325254e-02,\n",
       "          3.15811336e-01],\n",
       "        [ 2.16264855e-02, -3.46170329e-02,  2.39487514e-02,\n",
       "          2.87408382e-02],\n",
       "        [ 2.09341459e-02, -2.30074093e-01,  2.45235674e-02,\n",
       "          3.28882605e-01],\n",
       "        [ 1.63326636e-02, -3.53096835e-02,  3.11012212e-02,\n",
       "          4.40330543e-02],\n",
       "        [ 1.56264696e-02, -2.30863497e-01,  3.19818817e-02,\n",
       "          3.46364200e-01],\n",
       "        [ 1.10091995e-02, -3.62107158e-02,  3.89091671e-02,\n",
       "          6.39354065e-02],\n",
       "        [ 1.02849854e-02, -2.31868297e-01,  4.01878729e-02,\n",
       "          3.68636072e-01],\n",
       "        [ 5.64761972e-03, -3.73397283e-02,  4.75605950e-02,\n",
       "          8.88908654e-02],\n",
       "        [ 4.90082474e-03, -2.33109996e-01,  4.93384115e-02,\n",
       "          3.96191746e-01],\n",
       "        [ 2.38624969e-04, -3.87215279e-02,  5.72622456e-02,\n",
       "          1.19463474e-01],\n",
       "        [-5.35805535e-04, -2.34615162e-01,  5.96515164e-02,\n",
       "          4.29648221e-01],\n",
       "        [-5.22810873e-03, -4.03864458e-02,  6.82444796e-02,\n",
       "          1.56350479e-01],\n",
       "        [-6.03583781e-03, -2.36415759e-01,  7.13714883e-02,\n",
       "          4.69758302e-01],\n",
       "        [-1.07641527e-02, -4.23707590e-02,  8.07666555e-02,\n",
       "          2.00397223e-01],\n",
       "        [-1.16115678e-02, -2.38549396e-01,  8.47745985e-02,\n",
       "          5.17425478e-01],\n",
       "        [-1.63825564e-02, -4.47169878e-02,  9.51231122e-02,\n",
       "          2.52614915e-01],\n",
       "        [-1.72768962e-02, -2.41059408e-01,  1.00175411e-01,\n",
       "          5.73721170e-01],\n",
       "        [-2.20980849e-02, -4.74740900e-02,  1.11649834e-01,\n",
       "          3.14201266e-01],\n",
       "        [-2.30475664e-02, -2.43994743e-01,  1.17933854e-01,\n",
       "          6.39903724e-01],\n",
       "        [-2.79274601e-02, -5.06973043e-02,  1.30731940e-01,\n",
       "          3.86563838e-01],\n",
       "        [-2.89414078e-02, -2.47409254e-01,  1.38463214e-01,\n",
       "          7.17439651e-01],\n",
       "        [-3.38895917e-02, -5.44474386e-02,  1.52812004e-01,\n",
       "          4.71345335e-01],\n",
       "        [-3.49785425e-02, -2.51360148e-01,  1.62238911e-01,\n",
       "          8.08024526e-01],\n",
       "        [-4.00057435e-02, -5.87890707e-02,  1.78399399e-01,\n",
       "          5.70450425e-01],\n",
       "        [-4.11815234e-02, -2.55905062e-01,  1.89808413e-01,\n",
       "          9.13603425e-01],\n",
       "        [-4.62996252e-02, -6.37870952e-02,  2.08080471e-01,\n",
       "          6.86071217e-01]], dtype=float32)>,\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=int8),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to start putting together the agent! Let's start by giving it the ability to act. Here, we don't need to worry about exploration vs exploitation because we already have a random chance to take each of our actions. As the agent learns, it will naturally shift from exploration to exploitation. How conveient!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:59:03.270843Z",
     "start_time": "2024-05-24T09:59:03.264138Z"
    }
   },
   "source": [
    "class Partial_Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "    def __init__(self, policy, predict, memory, action_size):\n",
    "        \"\"\"Initializes the agent with Policy Gradient networks\n",
    "            and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            policy: The policy network created from build_networks().\n",
    "            predict: The predict network created from build_networks().\n",
    "            memory: A Memory class object.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.policy = policy\n",
    "        self.predict = predict\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        probabilities = self.predict.predict(state_batch)[0]\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the act function in action. First, let's build our agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:59:04.504972Z",
     "start_time": "2024-05-24T09:59:04.500288Z"
    }
   },
   "source": [
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the below cell a few times to test the `act` method. Is it about a 50/50 chance to push right instead of left?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:59:05.649339Z",
     "start_time": "2024-05-24T09:59:05.451849Z"
    }
   },
   "source": [
    "action = test_agent.act(state)\n",
    "print(\"Push Right\" if action else \"Push Left\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 53ms/step\n",
      "Push Left\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the most important part. We need to give our agent a way to learn! To start, we'll [one-hot encode](https://en.wikipedia.org/wiki/One-hot) our actions. Since the output of our network is a probability for each action, we'll have a 1 corresponding to the action that was taken and 0's for the actions we didn't take.\n",
    "\n",
    "That doesn't give our agent enough information on whether the action that was taken was actually a good idea, so we'll also use our `discount_episode` to calculate the TD(1) value of each step within the episode. \n",
    "\n",
    "One thing to note, is that CartPole doesn't have any negative rewards, meaning, even if it does terribly, the agent will still think the run went well. To help counter this, we'll take the mean and standard deviation of our discounted rewards, or `discount_mb`, and use that to find the [Standard Score](https://en.wikipedia.org/wiki/Standard_score) for each discounted reward. With this, steps close to dropping the poll will have a negative reward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:59:10.869586Z",
     "start_time": "2024-05-24T09:59:10.864229Z"
    }
   },
   "source": [
    "def learn(self, print_variables=False):\n",
    "    \"\"\"Trains a Policy Gradient policy network based on stored experiences.\"\"\"\n",
    "    state_mb, action_mb, reward_mb = self.memory.sample()\n",
    "    # One hot enocde actions\n",
    "    actions = np.zeros([len(action_mb), self.action_size])\n",
    "    actions[np.arange(len(action_mb)), action_mb] = 1\n",
    "    if print_variables:\n",
    "        print(\"action_mb:\", action_mb)\n",
    "        print(\"actions:\", actions)\n",
    "\n",
    "    # Apply TD(1) and normalize\n",
    "    discount_mb = discount_episode(reward_mb, self.memory.gamma)\n",
    "    discount_mb = (discount_mb - np.mean(discount_mb)) / np.std(discount_mb)\n",
    "    if print_variables:\n",
    "        print(\"reward_mb:\", reward_mb)\n",
    "        print(\"discount_mb:\", discount_mb)\n",
    "    return self.policy.train_on_batch([state_mb, discount_mb], actions)\n",
    "\n",
    "Partial_Agent.learn = learn\n",
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding in some print statements to the code above to get a sense of how the data is transformed before feeding it into the model, then run the below code to see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to put it all together. Policy Gradient Networks have less hypertuning parameters than DQNs, but since our custom loss constructs a [TensorFlow Graph](https://www.tensorflow.org/api_docs/python/tf/Graph) under the hood, we'll set up lazy execution by wrapping our traing steps in a default graph.\n",
    "\n",
    "By changing `test_gamma`, `test_learning_rate`, and `test_hidden_neurons`, can you help the agent reach a score of 200 within 200 episodes? It takes a little bit of thinking and a little bit of luck.\n",
    "\n",
    "Hover the curser  <b title=\"gamma=.9, learning rate=0.002, neurons=50\">on this bold text</b> to see a solution to the challenge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:59:38.201649Z",
     "start_time": "2024-05-24T09:59:37.701887Z"
    }
   },
   "source": [
    "test_gamma = .5\n",
    "test_learning_rate = .01\n",
    "test_hidden_neurons = 100\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma)\n",
    "    test_policy, test_predict = build_networks(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)\n",
    "    test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)\n",
    "    for episode in range(200):  \n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = test_agent.act(state)\n",
    "            state_prime, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            test_agent.memory.add((state, action, reward))\n",
    "            state = state_prime\n",
    "\n",
    "        test_agent.learn()\n",
    "        print(\"Episode\", episode, \"Score =\", episode_reward)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 16\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mtest_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     state_prime, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     18\u001B[0m     episode_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "Cell \u001B[0;32mIn[40], line 29\u001B[0m, in \u001B[0;36mPartial_Agent.act\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# If not acting randomly, take action with highest predicted value.\u001B[39;00m\n\u001B[1;32m     28\u001B[0m state_batch \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(state, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 29\u001B[0m probabilities \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_batch\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     30\u001B[0m action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_size, p\u001B[38;5;241m=\u001B[39mprobabilities)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m action\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:503\u001B[0m, in \u001B[0;36mDatasetV2.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m iterator_ops\u001B[38;5;241m.\u001B[39mOwnedIterator(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 503\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.Dataset` only supports Python-style \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    504\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration in eager mode or within tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Theory Behind Actor - Critic\n",
    "\n",
    "Now that we have the hang of Policy Gradients, let's combine this strategy with Deep Q Agents. We'll have one architecture to rule them all!\n",
    "\n",
    "Below is the setup for our neural networks. There are plenty of ways to go combining the two strategies. We'll be focusing on one varient called A2C, or Advantage Actor Critic. \n",
    "\n",
    "<img src=\"images/a2c_equation.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Here's the philosophy: We'll use our critic pathway to estimate the value of a state, or V(s). Given a state-action-new state transition, we can use our critic and the Bellman Equation to calculate the discounted value of the new state, or r + &gamma; * V(s').\n",
    "\n",
    "Like DQNs, this discounted value is the label the critic will train on. While that is happening, we can subtract V(s) and the discounted value of the new state to get the advantage, or A(s,a). In human terms, how much value was the action the agent took? This is what the actor, or the policy gradient portion or our network, will train on.\n",
    "\n",
    "Too long, didn't read: the critic's job is to learn how to asses the value of a state. The actor's job is to assign probabilities to it's available actions such that it increases its chance to move into a higher valued state.\n",
    "\n",
    "Below is our new `build_networks` function. Each line has been tagged with whether it comes from Deep Q Networks (`# DQN`), Policy Gradients (`# PG`), or is something new (`# New`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:47:45.101724Z",
     "start_time": "2024-05-24T09:47:45.094126Z"
    }
   },
   "source": [
    "def build_networks(state_shape, action_size, learning_rate, critic_weight, hidden_neurons, entropy):\n",
    "    \"\"\"Creates Actor Critic Neural Networks.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by an action's advantage.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        critic_weight (float): how much to weigh the critic's training loss.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden layer.\n",
    "        entropy (float): how much to enourage exploration versus exploitation.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    advantages = layers.Input((1,), name='advantages')  # PG, A instead of G\n",
    "\n",
    "    # PG\n",
    "    actor_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    actor_2 = layers.Dense(hidden_neurons, activation='relu')(actor_1)\n",
    "    probabilities = layers.Dense(action_size, activation='softmax')(actor_2)\n",
    "\n",
    "    # DQN\n",
    "    critic_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    critic_2 = layers.Dense(hidden_neurons, activation='relu')(critic_1)\n",
    "    values = layers.Dense(1, activation='linear')(critic_2)\n",
    "\n",
    "    def actor_loss(y_true, y_pred):  # PG\n",
    "        y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE)\n",
    "        log_lik = y_true*K.log(y_pred_clipped)\n",
    "        entropy_loss = y_pred * K.log(K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE))  # New\n",
    "        return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))\n",
    "\n",
    "    # Train both actor and critic at the same time.\n",
    "    actor = models.Model(\n",
    "        inputs=[state_input, advantages], outputs=[probabilities, values])\n",
    "    actor.compile(\n",
    "        loss=[actor_loss, 'mean_squared_error'],  # [PG, DQN]\n",
    "        loss_weights=[1, critic_weight],  # [PG, DQN]\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "    critic = models.Model(inputs=[state_input], outputs=[values])\n",
    "    policy = models.Model(inputs=[state_input], outputs=[probabilities])\n",
    "    return actor, critic, policy"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is one way to go about combining both of the algorithms. Here, we're combining training of both pwathways into on operation. Keras allows for the [training against multiple outputs](https://keras.io/models/model/). They can even have their own loss functions as we have above. When minimizing the loss, Keras will take the weighted sum of all the losses, with the weights provided in `loss_weights`. The `critic_weight` is now another hyperparameter for us to tune.\n",
    "\n",
    "We could even have completely separate networks for the actor and the critic, and that type of design choice is going to be problem dependent. Having shared nodes and training between the two will be more efficient to train per batch, but more complicated problems could justify keeping the two separate.\n",
    "\n",
    "The loss function we used here is also slightly different than the one for Policy Gradients. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:47:47.591115Z",
     "start_time": "2024-05-24T09:47:47.585362Z"
    }
   },
   "source": [
    "def actor_loss(y_true, y_pred):  # PG\n",
    "    y_pred_clipped = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "    log_lik = y_true*K.log(y_pred_clipped)\n",
    "    entropy_loss = y_pred * K.log(K.clip(y_pred, 1e-8, 1-1e-8))  # New\n",
    "    return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've added a new tool called [entropy](https://arxiv.org/pdf/1912.01557.pdf). We're calculating the [log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) again, but instead of comparing the probabilities of our actions versus the action that was taken, we calculating it for the probabilities of our actions against themselves.\n",
    "\n",
    "Certainly a mouthful, but the idea is to encourage exploration: if our probability prediction is very confident (or close to 1), our entropy will be close to 0. Similary, if our probability isn't confident at all (or close to 0), our entropy will again be zero. Anywhere inbetween, our entropy will be non-zero. This encourages exploration versus exploitation, as the entropy will discourage overconfident predictions.\n",
    "\n",
    "Now that the networks are out of the way, let's look at the `Memory`. We could go with Experience Replay, like with DQNs, or we could calculate TD(1) like with Policy Gradients. This time, we'll do something in between. We'll give our memory a `batch_size`. Once there are enough experiences in the buffer, we'll use all the experiences to train and then clear the buffer to start fresh.\n",
    "\n",
    "In order to speed up training, instead of recording state_prime, we'll record the value of state prime in `state_prime_values` or `next_values`. This will give us enough information to calculate the discounted values and advantages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:47:49.878208Z",
     "start_time": "2024-05-24T09:47:49.871901Z"
    }
   },
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay for actor-critic training.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess state values.\n",
    "        batch_size (int): The number of elements to include in the buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, batch_size):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: (state, action, reward, state_prime_value, done) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def check_full(self):\n",
    "        return len(self.buffer) >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns formated experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure [\n",
    "                [states], [actions], [rewards], [state_prime_values], [dones]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # Columns have different data types, so numpy array would be awkward.\n",
    "        batch = np.asarray(self.buffer, dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        dones_mb = np.array(batch[3], dtype=np.int8)\n",
    "        value_mb = np.squeeze(np.array(batch[4], dtype=np.float32))\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb, dones_mb, value_mb"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to build out the agent! The `act` method is the exact same as it was for Policy Gradients. Nice! The `learn` method is where things get interesting. We'll find the discounted future state like we did for DQN to train our critic. We'll then subtract the value of the discount state from the value of the current state to find the advantage, which is what the actor will train on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:47:51.102207Z",
     "start_time": "2024-05-24T09:47:51.092377Z"
    }
   },
   "source": [
    "class Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "    def __init__(self, actor, critic, policy, memory, action_size):\n",
    "        \"\"\"Initializes the agent with DQN and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            network: A neural network created from deep_q_network().\n",
    "            memory: A Memory class object.\n",
    "            epsilon_decay (float): The rate at which to decay random actions.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.policy = policy\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "            traning (bool): True if the agent is training.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        probabilities = self.policy.predict(state_batch)[0]\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def learn(self, print_variables=False):\n",
    "        \"\"\"Trains the Deep Q Network based on stored experiences.\"\"\"\n",
    "        gamma = self.memory.gamma\n",
    "        experiences = self.memory.sample()\n",
    "        state_mb, action_mb, reward_mb, dones_mb, next_value = experiences\n",
    "\n",
    "        # One hot enocde actions\n",
    "        actions = np.zeros([len(action_mb), self.action_size])\n",
    "        actions[np.arange(len(action_mb)), action_mb] = 1\n",
    "\n",
    "        #Apply TD(0)\n",
    "        discount_mb = reward_mb + next_value * gamma * (1 - dones_mb)\n",
    "        state_values = self.critic.predict([state_mb])\n",
    "        advantages = discount_mb - np.squeeze(state_values)\n",
    "        if print_variables:\n",
    "            print(\"discount_mb\", discount_mb)\n",
    "            print(\"next_value\", next_value)\n",
    "            print(\"state_values\", state_values)\n",
    "            print(\"advantages\", advantages)\n",
    "        else:\n",
    "            self.actor.train_on_batch(\n",
    "                [state_mb, advantages], [actions, discount_mb])"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to initialize an agent, and the cell after that to see the variables used for training. Since it's early, the critic hasn't learned to estimate the values yet, and the advatanges are mostly positive because of it.\n",
    "\n",
    "Once the crtic has learned how to properly assess states, the actor will start to see negative advantages. Try playing around with the variables to help the agent see this change sooner."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T09:47:53.181209Z",
     "start_time": "2024-05-24T09:47:52.753138Z"
    }
   },
   "source": [
    "# Change me please.\n",
    "test_gamma = .9\n",
    "test_batch_size = 32\n",
    "test_learning_rate = .02\n",
    "test_hidden_neurons = 50\n",
    "test_critic_weight = 0.5\n",
    "test_entropy = 0.0001\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma, test_batch_size)\n",
    "    test_actor, test_critic, test_policy = build_networks(\n",
    "        space_shape, action_size,\n",
    "        test_learning_rate, test_critic_weight,\n",
    "        test_hidden_neurons, test_entropy)\n",
    "    test_agent = Agent(\n",
    "        test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = test_agent.act(state)\n",
    "        state_prime, reward, done1, done2, _ = env.step(action)\n",
    "        done = done1 or done2\n",
    "        episode_reward += reward\n",
    "        next_value = test_agent.critic.predict([[state_prime]])\n",
    "        test_agent.memory.add((state, action, reward, done, next_value))\n",
    "        state = state_prime\n",
    "        test_agent.learn(print_variables=True)\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.02}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGraph()\u001B[38;5;241m.\u001B[39mas_default():\n\u001B[1;32m     10\u001B[0m     test_memory \u001B[38;5;241m=\u001B[39m Memory(test_gamma, test_batch_size)\n\u001B[0;32m---> 11\u001B[0m     test_actor, test_critic, test_policy \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_networks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspace_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest_learning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_critic_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest_hidden_neurons\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_entropy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     test_agent \u001B[38;5;241m=\u001B[39m Agent(\n\u001B[1;32m     16\u001B[0m         test_actor, test_critic, test_policy, test_memory, action_size)\n\u001B[1;32m     18\u001B[0m     state \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "Cell \u001B[0;32mIn[20], line 41\u001B[0m, in \u001B[0;36mbuild_networks\u001B[0;34m(state_shape, action_size, learning_rate, critic_weight, hidden_neurons, entropy)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Train both actor and critic at the same time.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m actor \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mModel(\n\u001B[1;32m     37\u001B[0m     inputs\u001B[38;5;241m=\u001B[39m[state_input, advantages], outputs\u001B[38;5;241m=\u001B[39m[probabilities, values])\n\u001B[1;32m     38\u001B[0m actor\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m     39\u001B[0m     loss\u001B[38;5;241m=\u001B[39m[actor_loss, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m],  \u001B[38;5;66;03m# [PG, DQN]\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     loss_weights\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m1\u001B[39m, critic_weight],  \u001B[38;5;66;03m# [PG, DQN]\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m     optimizer\u001B[38;5;241m=\u001B[39m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     43\u001B[0m critic \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mModel(inputs\u001B[38;5;241m=\u001B[39m[state_input], outputs\u001B[38;5;241m=\u001B[39m[values])\n\u001B[1;32m     44\u001B[0m policy \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mModel(inputs\u001B[38;5;241m=\u001B[39m[state_input], outputs\u001B[38;5;241m=\u001B[39m[probabilities])\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/optimizers/adam.py:62\u001B[0m, in \u001B[0;36mAdam.__init__\u001B[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     45\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m     61\u001B[0m ):\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclipnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclipnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclipvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclipvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[43mglobal_clipnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglobal_clipnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_ema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_ema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43mema_momentum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mema_momentum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m        \u001B[49m\u001B[43mema_overwrite_frequency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mema_overwrite_frequency\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_scale_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_scale_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_1 \u001B[38;5;241m=\u001B[39m beta_1\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_2 \u001B[38;5;241m=\u001B[39m beta_2\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:22\u001B[0m, in \u001B[0;36mTFOptimizer.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distribution_strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:37\u001B[0m, in \u001B[0;36mBaseOptimizer.__init__\u001B[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     35\u001B[0m     )\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument(s) not recognized: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m     name \u001B[38;5;241m=\u001B[39m auto_name(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Argument(s) not recognized: {'lr': 0.02}"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a set of variables you're happy with? Ok, time to shine! Run the below cell to see how the agent trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Score = 13.0\n",
      "Episode 1 Score = 13.0\n",
      "Episode 2 Score = 13.0\n",
      "Episode 3 Score = 11.0\n",
      "Episode 4 Score = 10.0\n",
      "Episode 5 Score = 9.0\n",
      "Episode 6 Score = 11.0\n",
      "Episode 7 Score = 9.0\n",
      "Episode 8 Score = 11.0\n",
      "Episode 9 Score = 10.0\n",
      "Episode 10 Score = 10.0\n",
      "Episode 11 Score = 9.0\n",
      "Episode 12 Score = 9.0\n",
      "Episode 13 Score = 9.0\n",
      "Episode 14 Score = 10.0\n",
      "Episode 15 Score = 10.0\n",
      "Episode 16 Score = 9.0\n",
      "Episode 17 Score = 9.0\n",
      "Episode 18 Score = 10.0\n",
      "Episode 19 Score = 10.0\n",
      "Episode 20 Score = 10.0\n",
      "Episode 21 Score = 10.0\n",
      "Episode 22 Score = 9.0\n",
      "Episode 23 Score = 9.0\n",
      "Episode 24 Score = 10.0\n",
      "Episode 25 Score = 10.0\n",
      "Episode 26 Score = 9.0\n",
      "Episode 27 Score = 8.0\n",
      "Episode 28 Score = 9.0\n",
      "Episode 29 Score = 10.0\n",
      "Episode 30 Score = 10.0\n",
      "Episode 31 Score = 10.0\n",
      "Episode 32 Score = 9.0\n",
      "Episode 33 Score = 9.0\n",
      "Episode 34 Score = 10.0\n",
      "Episode 35 Score = 10.0\n",
      "Episode 36 Score = 10.0\n",
      "Episode 37 Score = 10.0\n",
      "Episode 38 Score = 10.0\n",
      "Episode 39 Score = 11.0\n",
      "Episode 40 Score = 9.0\n",
      "Episode 41 Score = 10.0\n",
      "Episode 42 Score = 10.0\n",
      "Episode 43 Score = 9.0\n",
      "Episode 44 Score = 11.0\n",
      "Episode 45 Score = 10.0\n",
      "Episode 46 Score = 9.0\n",
      "Episode 47 Score = 10.0\n",
      "Episode 48 Score = 10.0\n",
      "Episode 49 Score = 9.0\n",
      "Episode 50 Score = 9.0\n",
      "Episode 51 Score = 9.0\n",
      "Episode 52 Score = 9.0\n",
      "Episode 53 Score = 10.0\n",
      "Episode 54 Score = 9.0\n",
      "Episode 55 Score = 9.0\n",
      "Episode 56 Score = 10.0\n",
      "Episode 57 Score = 9.0\n",
      "Episode 58 Score = 10.0\n",
      "Episode 59 Score = 9.0\n",
      "Episode 60 Score = 10.0\n",
      "Episode 61 Score = 9.0\n",
      "Episode 62 Score = 8.0\n",
      "Episode 63 Score = 10.0\n",
      "Episode 64 Score = 11.0\n",
      "Episode 65 Score = 9.0\n",
      "Episode 66 Score = 9.0\n",
      "Episode 67 Score = 10.0\n",
      "Episode 68 Score = 10.0\n",
      "Episode 69 Score = 8.0\n",
      "Episode 70 Score = 10.0\n",
      "Episode 71 Score = 9.0\n",
      "Episode 72 Score = 9.0\n",
      "Episode 73 Score = 9.0\n",
      "Episode 74 Score = 8.0\n",
      "Episode 75 Score = 9.0\n",
      "Episode 76 Score = 8.0\n",
      "Episode 77 Score = 9.0\n",
      "Episode 78 Score = 10.0\n",
      "Episode 79 Score = 9.0\n",
      "Episode 80 Score = 9.0\n",
      "Episode 81 Score = 10.0\n",
      "Episode 82 Score = 11.0\n",
      "Episode 83 Score = 9.0\n",
      "Episode 84 Score = 8.0\n",
      "Episode 85 Score = 9.0\n",
      "Episode 86 Score = 8.0\n",
      "Episode 87 Score = 10.0\n",
      "Episode 88 Score = 10.0\n",
      "Episode 89 Score = 9.0\n",
      "Episode 90 Score = 10.0\n",
      "Episode 91 Score = 10.0\n",
      "Episode 92 Score = 10.0\n",
      "Episode 93 Score = 9.0\n",
      "Episode 94 Score = 10.0\n",
      "Episode 95 Score = 8.0\n",
      "Episode 96 Score = 9.0\n",
      "Episode 97 Score = 10.0\n",
      "Episode 98 Score = 9.0\n",
      "Episode 99 Score = 10.0\n",
      "Episode 100 Score = 9.0\n",
      "Episode 101 Score = 11.0\n",
      "Episode 102 Score = 9.0\n",
      "Episode 103 Score = 9.0\n",
      "Episode 104 Score = 9.0\n",
      "Episode 105 Score = 10.0\n",
      "Episode 106 Score = 9.0\n",
      "Episode 107 Score = 10.0\n",
      "Episode 108 Score = 9.0\n",
      "Episode 109 Score = 9.0\n",
      "Episode 110 Score = 10.0\n",
      "Episode 111 Score = 10.0\n",
      "Episode 112 Score = 9.0\n",
      "Episode 113 Score = 9.0\n",
      "Episode 114 Score = 8.0\n",
      "Episode 115 Score = 9.0\n",
      "Episode 116 Score = 9.0\n",
      "Episode 117 Score = 8.0\n",
      "Episode 118 Score = 10.0\n",
      "Episode 119 Score = 8.0\n",
      "Episode 120 Score = 10.0\n",
      "Episode 121 Score = 10.0\n",
      "Episode 122 Score = 9.0\n",
      "Episode 123 Score = 10.0\n",
      "Episode 124 Score = 8.0\n",
      "Episode 125 Score = 9.0\n",
      "Episode 126 Score = 9.0\n",
      "Episode 127 Score = 10.0\n",
      "Episode 128 Score = 10.0\n",
      "Episode 129 Score = 10.0\n",
      "Episode 130 Score = 10.0\n",
      "Episode 131 Score = 8.0\n",
      "Episode 132 Score = 10.0\n",
      "Episode 133 Score = 9.0\n",
      "Episode 134 Score = 9.0\n",
      "Episode 135 Score = 9.0\n",
      "Episode 136 Score = 11.0\n",
      "Episode 137 Score = 9.0\n",
      "Episode 138 Score = 10.0\n",
      "Episode 139 Score = 10.0\n",
      "Episode 140 Score = 9.0\n",
      "Episode 141 Score = 10.0\n",
      "Episode 142 Score = 8.0\n",
      "Episode 143 Score = 10.0\n",
      "Episode 144 Score = 10.0\n",
      "Episode 145 Score = 8.0\n",
      "Episode 146 Score = 9.0\n",
      "Episode 147 Score = 9.0\n",
      "Episode 148 Score = 10.0\n",
      "Episode 149 Score = 10.0\n",
      "Episode 150 Score = 9.0\n",
      "Episode 151 Score = 10.0\n",
      "Episode 152 Score = 8.0\n",
      "Episode 153 Score = 10.0\n",
      "Episode 154 Score = 10.0\n",
      "Episode 155 Score = 9.0\n",
      "Episode 156 Score = 10.0\n",
      "Episode 157 Score = 9.0\n",
      "Episode 158 Score = 9.0\n",
      "Episode 159 Score = 10.0\n",
      "Episode 160 Score = 9.0\n",
      "Episode 161 Score = 9.0\n",
      "Episode 162 Score = 10.0\n",
      "Episode 163 Score = 10.0\n",
      "Episode 164 Score = 9.0\n",
      "Episode 165 Score = 10.0\n",
      "Episode 166 Score = 10.0\n",
      "Episode 167 Score = 8.0\n",
      "Episode 168 Score = 10.0\n",
      "Episode 169 Score = 10.0\n",
      "Episode 170 Score = 10.0\n",
      "Episode 171 Score = 9.0\n",
      "Episode 172 Score = 9.0\n",
      "Episode 173 Score = 11.0\n",
      "Episode 174 Score = 8.0\n",
      "Episode 175 Score = 10.0\n",
      "Episode 176 Score = 9.0\n",
      "Episode 177 Score = 10.0\n",
      "Episode 178 Score = 11.0\n",
      "Episode 179 Score = 9.0\n",
      "Episode 180 Score = 10.0\n",
      "Episode 181 Score = 9.0\n",
      "Episode 182 Score = 9.0\n",
      "Episode 183 Score = 10.0\n",
      "Episode 184 Score = 10.0\n",
      "Episode 185 Score = 10.0\n",
      "Episode 186 Score = 8.0\n",
      "Episode 187 Score = 9.0\n",
      "Episode 188 Score = 9.0\n",
      "Episode 189 Score = 9.0\n",
      "Episode 190 Score = 9.0\n",
      "Episode 191 Score = 10.0\n",
      "Episode 192 Score = 9.0\n",
      "Episode 193 Score = 9.0\n",
      "Episode 194 Score = 9.0\n",
      "Episode 195 Score = 9.0\n",
      "Episode 196 Score = 8.0\n",
      "Episode 197 Score = 9.0\n",
      "Episode 198 Score = 11.0\n",
      "Episode 199 Score = 9.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma, test_batch_size)\n",
    "    test_actor, test_critic, test_policy = build_networks(\n",
    "        space_shape, action_size,\n",
    "        test_learning_rate, test_critic_weight,\n",
    "        test_hidden_neurons, test_entropy)\n",
    "    test_agent = Agent(\n",
    "        test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "    for episode in range(200):  \n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = test_agent.act(state)\n",
    "            state_prime, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            next_value = test_agent.critic.predict([[state_prime]]) \n",
    "            test_agent.memory.add((state, action, reward, done, next_value))\n",
    "\n",
    "            #if test_agent.memory.check_full():\n",
    "            #test_agent.learn(print_variables=True)\n",
    "            state = state_prime\n",
    "        test_agent.learn()\n",
    "        print(\"Episode\", episode, \"Score =\", episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any luck? No sweat if not! It turns out that by combining the power of both algorithms, we also combined some of their setbacks. For instance, actor-critic can fall into local minimums like Policy Gradients, and has a large number of hyperparameters to tune like DQNs.\n",
    "\n",
    "Time to check how our agents did [in the cloud](https://console.cloud.google.com/ai-platform/jobs)! Any lucky winners? Find it in [your bucket](https://console.cloud.google.com/storage/browser) to watch a recording of it play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
