{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients and A2C\n",
    "\n",
    "In the <a href=\"../dqn/dqns_on_gcp.ipynb\">previous notebook</a>, we learned how to use hyperparameter tuning to help DQN agents balance a pole on a cart. In this notebook, we'll explore two other types of alogrithms: Policy Gradients and A2C.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Hypertuning takes some time, and in this case, it can take anywhere between **10 - 30 minutes**. If this hasn't been done already, run the cell below to kick off the training job now. We'll step through what the code is doing while our agents learn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !pip install tensorflow==2.16.2\n",
    "# !pip install numpy==1.21.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please ignore any incompatibility warnings and errors. Restart the kernel if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%bash\n",
    "BUCKET=<your-bucket-here> # Change to your bucket name\n",
    "JOB_NAME=pg_on_gcp_$(date -u +%y%m%d_%H%M%S)\n",
    "REGION='us-central1' # Change to your bucket region\n",
    "IMAGE_URI=gcr.io/cloud-training-prod-bucket/pg:latest\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --scale-tier=BASIC \\\n",
    "    --job-dir=gs://$BUCKET/$JOB_NAME \\\n",
    "    --config=templates/hyperparam.yaml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# !pip install gym==0.12.5 --user",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart the kernel if the above libraries needed to be installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, we can use the same environment for these algorithms as DQN, so this notebook will focus less on the operational work of feeding our agents the data, and more on the theory behind these algorthims. Let's start by loading our libraries and environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "from tensorflow.keras.layers import Layer, Input\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "CLIP_EDGE = 1e-8\n",
    "\n",
    "\n",
    "def print_state(state, step, reward=None):\n",
    "    format_string = 'Step {0} - Cart X: {1:.3f}, Cart V: {2:.3f}, Pole A: {3:.3f}, Pole V:{4:.3f}, Reward:{5}'\n",
    "    print(format_string.format(step, *tuple(state), reward))\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory Behind Policy Gradients\n",
    "\n",
    "Whereas Q-learning attempts to assign each state a value, Policy Gradients tries to find actions directly, increasing or decreaing a chance to take an action depending on how an episode plays out.\n",
    "\n",
    "To compare, Q-learning has a table that keeps track of the value of each combination of state and action:\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 1 | .5 | -1 |\n",
    "| Hungry | .5 | 1 | 0 |\n",
    "| Full | -1 | -.5 | 1.5 |\n",
    "\n",
    "Instead for Policy Gradients, we can imagine that we have a similar table, but instead of recording the values, we'll keep track of the probability to take the column action given the row state.\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 70% | 20% | 10% |\n",
    "| Hungry | 30% | 50% | 20% |\n",
    "| Full | 5% | 15% | 80% |\n",
    "\n",
    "With Q learning, whenever we take one step in our environment, we can update the value of the old state based on the value of the new state plus any rewards we picked up based on the [Q equation](https://en.wikipedia.org/wiki/Q-learning):\n",
    "\n",
    "<img style=\"background-color:white;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637\">\n",
    "\n",
    "Could we do the same thing if we have a table of probabilities instead values? No, because we don't have a way to calculate the value of each state from our table. Instead, we'll use a different <a href=\"http://incompleteideas.net/papers/sutton-88-with-erratum.pdf\"> Temporal Difference Learning</a> strategy.\n",
    "\n",
    "Q Learning is an evolution of TD(0), and for Policy Gradients, we'll use TD(1). We'll calculate TD(1) accross and entire episode, and use that to indicate whether to increase or decrease the probability correspoding to the action we took. Let's look at a full day of eating.\n",
    "\n",
    "| Hour | State | Action | Reward |\n",
    "|-|-|-|-|\n",
    "|9| Hangry | Wait | -.9 |\n",
    "|10| Hangry | Meal | 1.2 |\n",
    "|11| Full | Wait | .5 |\n",
    "|12| Full | Snack | -.6 |\n",
    "|13| Full | Wait | 1 |\n",
    "|14| Full | Wait | .6 |\n",
    "|15| Full | Wait | .2 |\n",
    "|16| Hungry | Wait | 0 |\n",
    "|17| Hungry | Meal | .4 |\n",
    "|18| Full | Wait| .5 |\n",
    "\n",
    "We'll work backwards from the last day, using the same discount, or `gamma`, as we did with DQNs. The `total_rewards` variable is equivalent to the value of state prime. Using the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation), everytime we calculate the value of a state, s<sub>t</sub>, we'll set that as the value of state prime for the state before, s<sub>t-1</sub>. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_gamma = .5  # Please change me to be between zero and one\n",
    "episode_rewards = [-.9, 1.2, .5, -.6, 1, .6, .2, 0, .4, .5]\n",
    "\n",
    "\n",
    "def discount_episode(rewards, gamma):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        total_rewards = rewards[t] + total_rewards * gamma\n",
    "        discounted_rewards[t] = total_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "discount_episode(episode_rewards, test_gamma)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wherever our discounted reward is positive, we'll increase the probability corresponding to the action we took. Similarly, wherever our discounted reward is negative, we'll decrease the probabilty.\n",
    "\n",
    "However, with this strategy, any actions with a positive reward will have it's probability increase, not necessarily the most optimal action. This puts us in a feedback loop, where we're more likely to pick less optimal actions which could further increase their probability. To counter this, we'll divide the size of our increases by the probability to choose the corresponding action, which will slow the growth of popular actions to give other actions a chance.\n",
    "\n",
    "Here is our update rule for our neural network, where alpha is our learning rate, and pi is our optimal policy, or the probability to take the optimal action, a<sup>*</sup>, given our current state, s. \n",
    "\n",
    "<img src=\"images/weight_update.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "Doing some fancy calculus, we can combine the numerator and denominator with a log function. Since it's not clear what the optimal action is, we'll instead use our discounted rewards, or G, to increase or decrease the weights of the respective action the agent took. A full breakdown of the math can be found in [this article by Chris Yoon](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63).\n",
    "\n",
    "<img src=\"images/weight_update_calculus.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Below is what it looks like in code. `y_true` is the [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) of the action that was taken. `y_pred` is the probabilty to take each action given the state the agent was in."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def custom_loss_test(y_true, y_pred):\n",
    "    y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1 - CLIP_EDGE)\n",
    "    log_likelihood = y_true * K.log(y_pred_clipped)\n",
    "    return K.sum(-log_likelihood * g)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have the discounted rewards, or `g`, when our agent is acting in the environment. No problem, we'll have one neural network with two types of pathways. One pathway, `predict`, will be the probability to take an action given an inputed state. It's only used for prediction and is not used for backpropogation. The other pathway, `policy`, will take both a state and a discounted reward, so it can be used for training.\n",
    "\n",
    "The code in its entirety looks like this. As with Deep Q Networks, the hidden layers of a Policy Gradient can use a CNN if the input state is pixels, but the last layer is typically a [Dense](https://keras.io/layers/core/) layer with a [Softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function to convert the output into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CustomLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        # Perform TensorFlow operations\n",
    "        return tf.convert_to_tensor(inputs)\n",
    "def build_networks(\n",
    "        state_shape, action_size, learning_rate, hidden_neurons):\n",
    "    \"\"\"Creates a Policy Gradient Neural Network.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by the discounted reward, g.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden\n",
    "            layer.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    g = layers.Input((1,), name='g')\n",
    "    print(\"g\",g)\n",
    "    \n",
    "    hidden_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    hidden_2 = layers.Dense(hidden_neurons, activation='relu')(hidden_1)\n",
    "    probabilities = layers.Dense(action_size, activation='softmax')(hidden_2)\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1-CLIP_EDGE)\n",
    "        log_lik = y_true*K.log(y_pred_clipped)\n",
    "        return K.sum(-log_lik*g)\n",
    "\n",
    "    policy = models.Model(\n",
    "        inputs=[state_input, g], outputs=probabilities)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    policy.compile(loss=custom_loss, optimizer=optimizer)\n",
    "    policy.summary()\n",
    "\n",
    "    predict = models.Model(inputs=[state_input], outputs=probabilities)\n",
    "    return policy, predict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a taste of how these networks function. Run the below cell to build our test networks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "space_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Feel free to play with these\n",
    "test_learning_rate = .2\n",
    "test_hidden_neurons = 10\n",
    "\n",
    "test_policy, test_predict = build_networks(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't use the policy network until we build our learning function, but we can feed a state to the predict network so we can see our chances to pick our actions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "state = env.reset()[0]\n",
    "test_predict.predict(np.expand_dims(state, axis=0))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the numbers should be close to `[.5, .5]`, with a little bit of variance due to the randomization of initializing the weights and the cart's starting position. In order to train, we'll need some memories to train on. The memory buffer here is simpler than DQN, as we don't have to worry about random sampling. We'll clear the buffer every time we train as we'll only hold one episode's worth of memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay buffer for Policy Gradient methods.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess TD(1) values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: a (state, action, reward) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns the list of episode experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure (\n",
    "                [states], [actions], [rewards]\n",
    "            }\n",
    "        \"\"\"\n",
    "        batch = np.asarray(self.buffer, dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a fake buffer to get a sense of the data we'll be training on. The cell below initializes our memory and runs through one episode of the game by alternating pushing the cart left and right.\n",
    "\n",
    "Try running it to see the data we'll be using for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_memory = Memory(test_gamma)\n",
    "actions = [x % 2 for x in range(200)]\n",
    "state = env.reset()[0]\n",
    "step = 0\n",
    "episode_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done and step < len(actions):\n",
    "    action = actions[step]  # In the future, our agents will define this.\n",
    "    state_prime, reward, done1, done2, info = env.step(action)\n",
    "    done = done1 or done2\n",
    "    episode_reward += reward\n",
    "    test_memory.add((state, action, reward))\n",
    "    step += 1\n",
    "    state = state_prime\n",
    "\n",
    "test_memory.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to start putting together the agent! Let's start by giving it the ability to act. Here, we don't need to worry about exploration vs exploitation because we already have a random chance to take each of our actions. As the agent learns, it will naturally shift from exploration to exploitation. How conveient!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Partial_Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "\n",
    "    def __init__(self, policy, predict, memory, action_size):\n",
    "        \"\"\"Initializes the agent with Policy Gradient networks\n",
    "            and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            policy: The policy network created from build_networks().\n",
    "            predict: The predict network created from build_networks().\n",
    "            memory: A Memory class object.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.policy = policy\n",
    "        self.predict = predict\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        result = self.predict.predict(state_batch, verbose=0)\n",
    "        probabilities = result[0]\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the act function in action. First, let's build our agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the below cell a few times to test the `act` method. Is it about a 50/50 chance to push right instead of left?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "action = test_agent.act(state)\n",
    "print(\"Push Right\" if action else \"Push Left\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the most important part. We need to give our agent a way to learn! To start, we'll [one-hot encode](https://en.wikipedia.org/wiki/One-hot) our actions. Since the output of our network is a probability for each action, we'll have a 1 corresponding to the action that was taken and 0's for the actions we didn't take.\n",
    "\n",
    "That doesn't give our agent enough information on whether the action that was taken was actually a good idea, so we'll also use our `discount_episode` to calculate the TD(1) value of each step within the episode. \n",
    "\n",
    "One thing to note, is that CartPole doesn't have any negative rewards, meaning, even if it does terribly, the agent will still think the run went well. To help counter this, we'll take the mean and standard deviation of our discounted rewards, or `discount_mb`, and use that to find the [Standard Score](https://en.wikipedia.org/wiki/Standard_score) for each discounted reward. With this, steps close to dropping the poll will have a negative reward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def learn(self, print_variables=False):\n",
    "    \"\"\"Trains a Policy Gradient policy network based on stored experiences.\"\"\"\n",
    "    state_mb, action_mb, reward_mb = self.memory.sample()\n",
    "    # One hot enocde actions\n",
    "    action_mb = tf.convert_to_tensor(action_mb, dtype=tf.int32)\n",
    "    action_hot = tf.one_hot(action_mb, self.action_size)\n",
    "    if print_variables:\n",
    "        print(\"action_mb:\", action_mb)\n",
    "        print(\"actions:\", action_hot)\n",
    "\n",
    "    # Apply TD(1) and normalize\n",
    "    discount_mb = discount_episode(reward_mb, self.memory.gamma)\n",
    "    discount_mb = (discount_mb - np.mean(discount_mb)) / np.std(discount_mb)\n",
    "    discount_mb = tf.convert_to_tensor(discount_mb, dtype=tf.int32)\n",
    "    if print_variables:\n",
    "        print(\"reward_mb:\", reward_mb)\n",
    "        print(\"discount_mb:\", discount_mb)\n",
    "        print(\"state_mb:\", state_mb)\n",
    "    return self.policy.train_on_batch([state_mb, discount_mb], action_hot)\n",
    "\n",
    "\n",
    "Partial_Agent.learn = learn\n",
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding in some print statements to the code above to get a sense of how the data is transformed before feeding it into the model, then run the below code to see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to put it all together. Policy Gradient Networks have less hypertuning parameters than DQNs, but since our custom loss constructs a [TensorFlow Graph](https://www.tensorflow.org/api_docs/python/tf/Graph) under the hood, we'll set up lazy execution by wrapping our traing steps in a default graph.\n",
    "\n",
    "By changing `test_gamma`, `test_learning_rate`, and `test_hidden_neurons`, can you help the agent reach a score of 200 within 200 episodes? It takes a little bit of thinking and a little bit of luck.\n",
    "\n",
    "Hover the curser  <b title=\"gamma=.9, learning rate=0.002, neurons=50\">on this bold text</b> to see a solution to the challenge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T03:53:27.952682Z",
     "start_time": "2024-05-25T03:53:24.609591Z"
    }
   },
   "source": [
    "test_gamma = .5\n",
    "test_learning_rate = .01\n",
    "test_hidden_neurons = 100\n",
    "\n",
    "test_memory = Memory(test_gamma)\n",
    "test_policy, test_predict = build_networks(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)\n",
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)\n",
    "for episode in range(200):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    print(\"============================== Play ============================\")\n",
    "    for i in range(200):\n",
    "        action = test_agent.act(state)\n",
    "        state_prime, reward, done1, done2, info = env.step(action)\n",
    "        done = done1 or done2\n",
    "        episode_reward += reward\n",
    "        test_agent.memory.add((state, action, reward))\n",
    "        state = state_prime\n",
    "        if done: break\n",
    "    print(\"============================== Learn ============================\")\n",
    "    test_agent.learn(True)\n",
    "    print(\"Episode\", episode, \"Score =\", episode_reward)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='g'), name='g', description=\"created by layer 'g'\")\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " frames (InputLayer)            [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 100)          500         ['frames[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 100)          10100       ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " g (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 2)            202         ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,802\n",
      "Trainable params: 10,802\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "============================== Play ============================\n",
      "============================== Learn ============================\n",
      "action_mb: tf.Tensor(\n",
      "[0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
      " 0 1 1], shape=(40,), dtype=int32)\n",
      "actions: tf.Tensor(\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]], shape=(40, 2), dtype=float32)\n",
      "reward_mb: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "discount_mb: tf.Tensor(\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -2 -5], shape=(40,), dtype=int32)\n",
      "state_mb: tf.Tensor(\n",
      "[[-9.04282182e-03  2.32900660e-02  1.37538845e-02 -2.68625957e-03]\n",
      " [-8.57702084e-03 -1.72026411e-01  1.37001593e-02  2.94304252e-01]\n",
      " [-1.20175490e-02  2.28975676e-02  1.95862446e-02  5.97345084e-03]\n",
      " [-1.15595972e-02  2.17733234e-01  1.97057147e-02 -2.80465990e-01]\n",
      " [-7.20493309e-03  2.23358050e-02  1.40963942e-02  1.83663387e-02]\n",
      " [-6.75821677e-03 -1.72985435e-01  1.44637208e-02  3.15463305e-01]\n",
      " [-1.02179255e-02  2.19275318e-02  2.07729861e-02  2.73766089e-02]\n",
      " [-9.77937505e-03 -1.73486069e-01  2.13205181e-02  3.26540589e-01]\n",
      " [-1.32490965e-02 -3.68904978e-01  2.78513301e-02  6.25870109e-01]\n",
      " [-2.06271950e-02 -5.64404428e-01  4.03687321e-02  9.27192748e-01]\n",
      " [-3.19152847e-02 -3.69850129e-01  5.89125864e-02  6.47464216e-01]\n",
      " [-3.93122882e-02 -1.75596297e-01  7.18618706e-02  3.73899311e-01]\n",
      " [-4.28242125e-02  1.84351951e-02  7.93398544e-02  1.04712449e-01]\n",
      " [-4.24555093e-02  2.12335795e-01  8.14341083e-02 -1.61922053e-01]\n",
      " [-3.82087938e-02  1.61481798e-02  7.81956688e-02  1.55299321e-01]\n",
      " [-3.78858298e-02  2.10068509e-01  8.13016519e-02 -1.11726217e-01]\n",
      " [-3.36844586e-02  4.03937042e-01  7.90671259e-02 -3.77692074e-01]\n",
      " [-2.56057177e-02  2.07786396e-01  7.15132877e-02 -6.11638166e-02]\n",
      " [-2.14499906e-02  4.01814014e-01  7.02900067e-02 -3.30454350e-01]\n",
      " [-1.34137096e-02  2.05765560e-01  6.36809245e-02 -1.64595824e-02]\n",
      " [-9.29839816e-03  3.99919182e-01  6.33517280e-02 -2.88390160e-01]\n",
      " [-1.30001479e-03  5.94083190e-01  5.75839281e-02 -5.60438991e-01]\n",
      " [ 1.05816489e-02  3.98202360e-01  4.63751480e-02 -2.50184387e-01]\n",
      " [ 1.85456965e-02  2.02449873e-01  4.13714610e-02  5.67580312e-02]\n",
      " [ 2.25946940e-02  3.96954954e-01  4.25066203e-02 -2.22590208e-01]\n",
      " [ 3.05337925e-02  5.91444373e-01  3.80548164e-02 -5.01567662e-01]\n",
      " [ 4.23626825e-02  3.95807266e-01  2.80234627e-02 -1.97139010e-01]\n",
      " [ 5.02788275e-02  5.90517402e-01  2.40806825e-02 -4.80851740e-01]\n",
      " [ 6.20891750e-02  3.95063967e-01  1.44636482e-02 -1.80677533e-01]\n",
      " [ 6.99904561e-02  1.99738055e-01  1.08500980e-02  1.16532847e-01]\n",
      " [ 7.39852116e-02  4.46234038e-03  1.31807551e-02  4.12619054e-01]\n",
      " [ 7.40744621e-02 -1.90843955e-01  2.14331355e-02  7.09428072e-01]\n",
      " [ 7.02575818e-02  3.97469010e-03  3.56216952e-02  4.23568100e-01]\n",
      " [ 7.03370795e-02 -1.91633314e-01  4.40930575e-02  7.27264524e-01]\n",
      " [ 6.65044114e-02 -3.87336224e-01  5.86383492e-02  1.03349268e+00]\n",
      " [ 5.87576851e-02 -5.83186924e-01  7.93082044e-02  1.34399366e+00]\n",
      " [ 4.70939465e-02 -7.79211879e-01  1.06188074e-01  1.66039896e+00]\n",
      " [ 3.15097086e-02 -5.85475564e-01  1.39396057e-01  1.40259206e+00]\n",
      " [ 1.98001992e-02 -7.82026529e-01  1.67447895e-01  1.73540854e+00]\n",
      " [ 4.15966799e-03 -5.89163661e-01  2.02156067e-01  1.49916053e+00]], shape=(40, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 03:53:27.736551: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [40,2]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2024-05-25 03:53:27.793116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf.range_4/range/custom_loss/weighted_loss/Const_1' with dtype int32\n",
      "\t [[{{node tf.range_4/range/custom_loss/weighted_loss/Const_1}}]]\n",
      "2024-05-25 03:53:27.822399: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf.range_5/range/custom_loss/weighted_loss/Const_2' with dtype int32\n",
      "\t [[{{node tf.range_5/range/custom_loss/weighted_loss/Const_2}}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 317, in __call__\n        self._total_loss_mean.update_state(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 477, in update_state  **\n        sample_weight = tf.__internal__.ops.broadcast_weights(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/keras_tensor.py\", line 283, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_8'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done: \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m============================== Learn ============================\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m \u001B[43mtest_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode\u001B[39m\u001B[38;5;124m\"\u001B[39m, episode, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScore =\u001B[39m\u001B[38;5;124m\"\u001B[39m, episode_reward)\n",
      "Cell \u001B[0;32mIn[52], line 19\u001B[0m, in \u001B[0;36mlearn\u001B[0;34m(self, print_variables)\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount_mb:\u001B[39m\u001B[38;5;124m\"\u001B[39m, discount_mb)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_mb:\u001B[39m\u001B[38;5;124m\"\u001B[39m, state_mb)\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstate_mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount_mb\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_hot\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/training.py:2510\u001B[0m, in \u001B[0;36mModel.train_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001B[0m\n\u001B[1;32m   2506\u001B[0m     iterator \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39msingle_batch_iterator(\n\u001B[1;32m   2507\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001B[1;32m   2508\u001B[0m     )\n\u001B[1;32m   2509\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_train_function()\n\u001B[0;32m-> 2510\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2512\u001B[0m logs \u001B[38;5;241m=\u001B[39m tf_utils\u001B[38;5;241m.\u001B[39msync_to_numpy_or_python_type(logs)\n\u001B[1;32m   2513\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/tmp/__autograph_generated_fileh0jmiaud.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/training.py:1268\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function\u001B[0;34m(model, iterator)\u001B[0m\n\u001B[1;32m   1264\u001B[0m     run_step \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mfunction(\n\u001B[1;32m   1265\u001B[0m         run_step, jit_compile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, reduce_retracing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1266\u001B[0m     )\n\u001B[1;32m   1267\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m-> 1268\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1269\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[1;32m   1270\u001B[0m     outputs,\n\u001B[1;32m   1271\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy,\n\u001B[1;32m   1272\u001B[0m     reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_reduction_method,\n\u001B[1;32m   1273\u001B[0m )\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/training.py:1249\u001B[0m, in \u001B[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m   1248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(data):\n\u001B[0;32m-> 1249\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1250\u001B[0m     \u001B[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001B[39;00m\n\u001B[1;32m   1251\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/training.py:1051\u001B[0m, in \u001B[0;36mModel.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m   1050\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(x, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m-> 1051\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1052\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_target_and_loss(y, loss)\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;66;03m# Run backwards pass.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/training.py:1109\u001B[0m, in \u001B[0;36mModel.compute_loss\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the total loss, validate it, and return it.\u001B[39;00m\n\u001B[1;32m   1059\u001B[0m \n\u001B[1;32m   1060\u001B[0m \u001B[38;5;124;03mSubclasses can optionally override this method to provide custom loss\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;124;03m  is the case when called by `Model.test_step`).\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m x  \u001B[38;5;66;03m# The default implementation does not use `x`.\u001B[39;00m\n\u001B[0;32m-> 1109\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1110\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregularization_losses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlosses\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/compile_utils.py:317\u001B[0m, in \u001B[0;36mLossesContainer.__call__\u001B[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001B[0m\n\u001B[1;32m    313\u001B[0m total_loss_mean_values \u001B[38;5;241m=\u001B[39m losses_utils\u001B[38;5;241m.\u001B[39mcast_losses_to_common_dtype(\n\u001B[1;32m    314\u001B[0m     total_loss_mean_values\n\u001B[1;32m    315\u001B[0m )\n\u001B[1;32m    316\u001B[0m total_total_loss_mean_value \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39madd_n(total_loss_mean_values)\n\u001B[0;32m--> 317\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_total_loss_mean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_total_loss_mean_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_dim\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m loss_values \u001B[38;5;241m=\u001B[39m losses_utils\u001B[38;5;241m.\u001B[39mcast_losses_to_common_dtype(loss_values)\n\u001B[1;32m    322\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39madd_n(loss_values)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/utils/metrics_utils.py:77\u001B[0m, in \u001B[0;36mupdate_state_wrapper.<locals>.decorated\u001B[0;34m(metric_obj, *args, **kwargs)\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying to run metric.update_state in replica context when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe metric was not created in TPUStrategy scope. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     72\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMake sure the keras Metric is created in TPUstrategy \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     73\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscope. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf_utils\u001B[38;5;241m.\u001B[39mgraph_context_for_symbolic_tensors(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 77\u001B[0m     update_op \u001B[38;5;241m=\u001B[39m \u001B[43mupdate_state_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m update_op \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# update_op will be None in eager execution.\u001B[39;00m\n\u001B[1;32m     79\u001B[0m     metric_obj\u001B[38;5;241m.\u001B[39madd_update(update_op)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/metrics/base_metric.py:140\u001B[0m, in \u001B[0;36mMetric.__new__.<locals>.update_state_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    136\u001B[0m control_status \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mautograph\u001B[38;5;241m.\u001B[39mcontrol_status_ctx()\n\u001B[1;32m    137\u001B[0m ag_update_state \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mautograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[1;32m    138\u001B[0m     obj_update_state, control_status\n\u001B[1;32m    139\u001B[0m )\n\u001B[0;32m--> 140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mag_update_state\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/metrics/base_metric.py:477\u001B[0m, in \u001B[0;36mReduce.update_state\u001B[0;34m(self, values, sample_weight)\u001B[0m\n\u001B[1;32m    468\u001B[0m (\n\u001B[1;32m    469\u001B[0m     values,\n\u001B[1;32m    470\u001B[0m     _,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m     values, sample_weight\u001B[38;5;241m=\u001B[39msample_weight\n\u001B[1;32m    474\u001B[0m )\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;66;03m# Broadcast weights if possible.\u001B[39;00m\n\u001B[0;32m--> 477\u001B[0m     sample_weight \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mbroadcast_weights(\n\u001B[1;32m    478\u001B[0m         sample_weight, values\n\u001B[1;32m    479\u001B[0m     )\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;66;03m# Reduce values to same ndim as weight array\u001B[39;00m\n\u001B[1;32m    482\u001B[0m     ndim \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mndim(values)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/engine/keras_tensor.py:283\u001B[0m, in \u001B[0;36mKerasTensor.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 283\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    284\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are passing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, an intermediate Keras symbolic \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput/output, to a TF API that does not allow registering custom \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdispatchers, such as `tf.cond`, `tf.function`, gradient tapes, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `tf.map_fn`. Keras Functional model construction only supports \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTF API calls that *do* support dispatching, such as `tf.math.add` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `tf.reshape`. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOther APIs cannot be called directly on symbolic Keras\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs/outputs. You can work around \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthis limitation by putting the operation in a custom Keras layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`call` and calling that layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon this symbolic input/output.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    295\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: in user code:\n\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 317, in __call__\n        self._total_loss_mean.update_state(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/conda/lib/python3.11/site-packages/keras/metrics/base_metric.py\", line 477, in update_state  **\n        sample_weight = tf.__internal__.ops.broadcast_weights(\n    File \"/opt/conda/lib/python3.11/site-packages/keras/engine/keras_tensor.py\", line 283, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_8'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Theory Behind Actor - Critic\n",
    "\n",
    "Now that we have the hang of Policy Gradients, let's combine this strategy with Deep Q Agents. We'll have one architecture to rule them all!\n",
    "\n",
    "Below is the setup for our neural networks. There are plenty of ways to go combining the two strategies. We'll be focusing on one varient called A2C, or Advantage Actor Critic. \n",
    "\n",
    "<img src=\"images/a2c_equation.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Here's the philosophy: We'll use our critic pathway to estimate the value of a state, or V(s). Given a state-action-new state transition, we can use our critic and the Bellman Equation to calculate the discounted value of the new state, or r + &gamma; * V(s').\n",
    "\n",
    "Like DQNs, this discounted value is the label the critic will train on. While that is happening, we can subtract V(s) and the discounted value of the new state to get the advantage, or A(s,a). In human terms, how much value was the action the agent took? This is what the actor, or the policy gradient portion or our network, will train on.\n",
    "\n",
    "Too long, didn't read: the critic's job is to learn how to asses the value of a state. The actor's job is to assign probabilities to it's available actions such that it increases its chance to move into a higher valued state.\n",
    "\n",
    "Below is our new `build_networks` function. Each line has been tagged with whether it comes from Deep Q Networks (`# DQN`), Policy Gradients (`# PG`), or is something new (`# New`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_networks(state_shape, action_size, learning_rate, critic_weight, hidden_neurons, entropy):\n",
    "    \"\"\"Creates Actor Critic Neural Networks.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by an action's advantage.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        critic_weight (float): how much to weigh the critic's training loss.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden layer.\n",
    "        entropy (float): how much to enourage exploration versus exploitation.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    advantages = layers.Input((1,), name='advantages')  # PG, A instead of G\n",
    "\n",
    "    # PG\n",
    "    actor_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    actor_2 = layers.Dense(hidden_neurons, activation='relu')(actor_1)\n",
    "    probabilities = layers.Dense(action_size, activation='softmax')(actor_2)\n",
    "\n",
    "    # DQN\n",
    "    critic_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    critic_2 = layers.Dense(hidden_neurons, activation='relu')(critic_1)\n",
    "    values = layers.Dense(1, activation='linear')(critic_2)\n",
    "\n",
    "    def actor_loss(y_true, y_pred):  # PG\n",
    "        y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1 - CLIP_EDGE)\n",
    "        log_lik = y_true * K.log(y_pred_clipped)\n",
    "        entropy_loss = y_pred * K.log(K.clip(y_pred, CLIP_EDGE, 1 - CLIP_EDGE))  # New\n",
    "        return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))\n",
    "\n",
    "    # Train both actor and critic at the same time.\n",
    "    actor = models.Model(\n",
    "        inputs=[state_input, advantages], outputs=[probabilities, values])\n",
    "    actor.compile(\n",
    "        loss=[actor_loss, 'mean_squared_error'],  # [PG, DQN]\n",
    "        loss_weights=[1, critic_weight],  # [PG, DQN]\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "    critic = models.Model(inputs=[state_input], outputs=[values])\n",
    "    policy = models.Model(inputs=[state_input], outputs=[probabilities])\n",
    "    return actor, critic, policy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is one way to go about combining both of the algorithms. Here, we're combining training of both pwathways into on operation. Keras allows for the [training against multiple outputs](https://keras.io/models/model/). They can even have their own loss functions as we have above. When minimizing the loss, Keras will take the weighted sum of all the losses, with the weights provided in `loss_weights`. The `critic_weight` is now another hyperparameter for us to tune.\n",
    "\n",
    "We could even have completely separate networks for the actor and the critic, and that type of design choice is going to be problem dependent. Having shared nodes and training between the two will be more efficient to train per batch, but more complicated problems could justify keeping the two separate.\n",
    "\n",
    "The loss function we used here is also slightly different than the one for Policy Gradients. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def actor_loss(y_true, y_pred):  # PG\n",
    "    y_pred_clipped = K.clip(y_pred, 1e-8, 1 - 1e-8)\n",
    "    log_lik = y_true * K.log(y_pred_clipped)\n",
    "    entropy_loss = y_pred * K.log(K.clip(y_pred, 1e-8, 1 - 1e-8))  # New\n",
    "    return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've added a new tool called [entropy](https://arxiv.org/pdf/1912.01557.pdf). We're calculating the [log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) again, but instead of comparing the probabilities of our actions versus the action that was taken, we calculating it for the probabilities of our actions against themselves.\n",
    "\n",
    "Certainly a mouthful, but the idea is to encourage exploration: if our probability prediction is very confident (or close to 1), our entropy will be close to 0. Similary, if our probability isn't confident at all (or close to 0), our entropy will again be zero. Anywhere inbetween, our entropy will be non-zero. This encourages exploration versus exploitation, as the entropy will discourage overconfident predictions.\n",
    "\n",
    "Now that the networks are out of the way, let's look at the `Memory`. We could go with Experience Replay, like with DQNs, or we could calculate TD(1) like with Policy Gradients. This time, we'll do something in between. We'll give our memory a `batch_size`. Once there are enough experiences in the buffer, we'll use all the experiences to train and then clear the buffer to start fresh.\n",
    "\n",
    "In order to speed up training, instead of recording state_prime, we'll record the value of state prime in `state_prime_values` or `next_values`. This will give us enough information to calculate the discounted values and advantages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay for actor-critic training.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess state values.\n",
    "        batch_size (int): The number of elements to include in the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma, batch_size):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: (state, action, reward, state_prime_value, done) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def check_full(self):\n",
    "        return len(self.buffer) >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns formated experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure [\n",
    "                [states], [actions], [rewards], [state_prime_values], [dones]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # Columns have different data types, so numpy array would be awkward.\n",
    "        batch = np.asarray(self.buffer, dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        dones_mb = np.array(batch[3], dtype=np.int8)\n",
    "        value_mb = np.squeeze(np.array(batch[4], dtype=np.float32))\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb, dones_mb, value_mb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to build out the agent! The `act` method is the exact same as it was for Policy Gradients. Nice! The `learn` method is where things get interesting. We'll find the discounted future state like we did for DQN to train our critic. We'll then subtract the value of the discount state from the value of the current state to find the advantage, which is what the actor will train on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "\n",
    "    def __init__(self, actor, critic, policy, memory, action_size):\n",
    "        \"\"\"Initializes the agent with DQN and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            network: A neural network created from deep_q_network().\n",
    "            memory: A Memory class object.\n",
    "            epsilon_decay (float): The rate at which to decay random actions.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.policy = policy\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "            traning (bool): True if the agent is training.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        probabilities = self.policy.predict(state_batch)[0]\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def learn(self, print_variables=False):\n",
    "        \"\"\"Trains the Deep Q Network based on stored experiences.\"\"\"\n",
    "        gamma = self.memory.gamma\n",
    "        experiences = self.memory.sample()\n",
    "        state_mb, action_mb, reward_mb, dones_mb, next_value = experiences\n",
    "\n",
    "        # One hot enocde actions\n",
    "        actions = np.zeros([len(action_mb), self.action_size])\n",
    "        actions[np.arange(len(action_mb)), action_mb] = 1\n",
    "\n",
    "        #Apply TD(0)\n",
    "        discount_mb = reward_mb + next_value * gamma * (1 - dones_mb)\n",
    "        state_values = self.critic.predict([state_mb])\n",
    "        advantages = discount_mb - np.squeeze(state_values)\n",
    "        if print_variables:\n",
    "            print(\"discount_mb\", discount_mb)\n",
    "            print(\"next_value\", next_value)\n",
    "            print(\"state_values\", state_values)\n",
    "            print(\"advantages\", advantages)\n",
    "        else:\n",
    "            self.actor.train_on_batch(\n",
    "                [state_mb, advantages], [actions, discount_mb])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to initialize an agent, and the cell after that to see the variables used for training. Since it's early, the critic hasn't learned to estimate the values yet, and the advatanges are mostly positive because of it.\n",
    "\n",
    "Once the crtic has learned how to properly assess states, the actor will start to see negative advantages. Try playing around with the variables to help the agent see this change sooner."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Change me please.\n",
    "test_gamma = .9\n",
    "test_batch_size = 32\n",
    "test_learning_rate = .02\n",
    "test_hidden_neurons = 50\n",
    "test_critic_weight = 0.5\n",
    "test_entropy = 0.0001\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma, test_batch_size)\n",
    "    test_actor, test_critic, test_policy = build_networks(\n",
    "        space_shape, action_size,\n",
    "        test_learning_rate, test_critic_weight,\n",
    "        test_hidden_neurons, test_entropy)\n",
    "    test_agent = Agent(\n",
    "        test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = test_agent.act(state)\n",
    "        state_prime, reward, done1, done2, _ = env.step(action)\n",
    "        done = done1 or done2\n",
    "        episode_reward += reward\n",
    "        next_value = test_agent.critic.predict([[state_prime]])\n",
    "        test_agent.memory.add((state, action, reward, done, next_value))\n",
    "        state = state_prime\n",
    "        test_agent.learn(print_variables=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a set of variables you're happy with? Ok, time to shine! Run the below cell to see how the agent trains."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma, test_batch_size)\n",
    "    test_actor, test_critic, test_policy = build_networks(\n",
    "        space_shape, action_size,\n",
    "        test_learning_rate, test_critic_weight,\n",
    "        test_hidden_neurons, test_entropy)\n",
    "    test_agent = Agent(\n",
    "        test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "    for episode in range(200):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = test_agent.act(state)\n",
    "            state_prime, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            next_value = test_agent.critic.predict([[state_prime]])\n",
    "            test_agent.memory.add((state, action, reward, done, next_value))\n",
    "\n",
    "            #if test_agent.memory.check_full():\n",
    "            #test_agent.learn(print_variables=True)\n",
    "            state = state_prime\n",
    "        test_agent.learn()\n",
    "        print(\"Episode\", episode, \"Score =\", episode_reward)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any luck? No sweat if not! It turns out that by combining the power of both algorithms, we also combined some of their setbacks. For instance, actor-critic can fall into local minimums like Policy Gradients, and has a large number of hyperparameters to tune like DQNs.\n",
    "\n",
    "Time to check how our agents did [in the cloud](https://console.cloud.google.com/ai-platform/jobs)! Any lucky winners? Find it in [your bucket](https://console.cloud.google.com/storage/browser) to watch a recording of it play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
