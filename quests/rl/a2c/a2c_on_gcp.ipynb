{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients and A2C\n",
    "\n",
    "In the <a href=\"../dqn/dqns_on_gcp.ipynb\">previous notebook</a>, we learned how to use hyperparameter tuning to help DQN agents balance a pole on a cart. In this notebook, we'll explore two other types of alogrithms: Policy Gradients and A2C.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Hypertuning takes some time, and in this case, it can take anywhere between **10 - 30 minutes**. If this hasn't been done already, run the cell below to kick off the training job now. We'll step through what the code is doing while our agents learn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !pip install tensorflow==2.5\n",
    "# !pip install numpy==1.21.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please ignore any incompatibility warnings and errors. Restart the kernel if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%bash\n",
    "BUCKET=<your-bucket-here> # Change to your bucket name\n",
    "JOB_NAME=pg_on_gcp_$(date -u +%y%m%d_%H%M%S)\n",
    "REGION='us-central1' # Change to your bucket region\n",
    "IMAGE_URI=gcr.io/cloud-training-prod-bucket/pg:latest\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --scale-tier=BASIC \\\n",
    "    --job-dir=gs://$BUCKET/$JOB_NAME \\\n",
    "    --config=templates/hyperparam.yaml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# !pip install gym==0.12.5 --user",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart the kernel if the above libraries needed to be installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, we can use the same environment for these algorithms as DQN, so this notebook will focus less on the operational work of feeding our agents the data, and more on the theory behind these algorthims. Let's start by loading our libraries and environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:21:44.372901Z",
     "start_time": "2024-05-24T17:21:39.607850Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "CLIP_EDGE = 1e-8\n",
    "\n",
    "\n",
    "def print_state(state, step, reward=None):\n",
    "    format_string = 'Step {0} - Cart X: {1:.3f}, Cart V: {2:.3f}, Pole A: {3:.3f}, Pole V:{4:.3f}, Reward:{5}'\n",
    "    print(format_string.format(step, *tuple(state), reward))\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 17:21:40.836925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 17:21:42.428163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory Behind Policy Gradients\n",
    "\n",
    "Whereas Q-learning attempts to assign each state a value, Policy Gradients tries to find actions directly, increasing or decreaing a chance to take an action depending on how an episode plays out.\n",
    "\n",
    "To compare, Q-learning has a table that keeps track of the value of each combination of state and action:\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 1 | .5 | -1 |\n",
    "| Hungry | .5 | 1 | 0 |\n",
    "| Full | -1 | -.5 | 1.5 |\n",
    "\n",
    "Instead for Policy Gradients, we can imagine that we have a similar table, but instead of recording the values, we'll keep track of the probability to take the column action given the row state.\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 70% | 20% | 10% |\n",
    "| Hungry | 30% | 50% | 20% |\n",
    "| Full | 5% | 15% | 80% |\n",
    "\n",
    "With Q learning, whenever we take one step in our environment, we can update the value of the old state based on the value of the new state plus any rewards we picked up based on the [Q equation](https://en.wikipedia.org/wiki/Q-learning):\n",
    "\n",
    "<img style=\"background-color:white;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637\">\n",
    "\n",
    "Could we do the same thing if we have a table of probabilities instead values? No, because we don't have a way to calculate the value of each state from our table. Instead, we'll use a different <a href=\"http://incompleteideas.net/papers/sutton-88-with-erratum.pdf\"> Temporal Difference Learning</a> strategy.\n",
    "\n",
    "Q Learning is an evolution of TD(0), and for Policy Gradients, we'll use TD(1). We'll calculate TD(1) accross and entire episode, and use that to indicate whether to increase or decrease the probability correspoding to the action we took. Let's look at a full day of eating.\n",
    "\n",
    "| Hour | State | Action | Reward |\n",
    "|-|-|-|-|\n",
    "|9| Hangry | Wait | -.9 |\n",
    "|10| Hangry | Meal | 1.2 |\n",
    "|11| Full | Wait | .5 |\n",
    "|12| Full | Snack | -.6 |\n",
    "|13| Full | Wait | 1 |\n",
    "|14| Full | Wait | .6 |\n",
    "|15| Full | Wait | .2 |\n",
    "|16| Hungry | Wait | 0 |\n",
    "|17| Hungry | Meal | .4 |\n",
    "|18| Full | Wait| .5 |\n",
    "\n",
    "We'll work backwards from the last day, using the same discount, or `gamma`, as we did with DQNs. The `total_rewards` variable is equivalent to the value of state prime. Using the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation), everytime we calculate the value of a state, s<sub>t</sub>, we'll set that as the value of state prime for the state before, s<sub>t-1</sub>. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:21:44.386064Z",
     "start_time": "2024-05-24T17:21:44.375251Z"
    }
   },
   "source": [
    "test_gamma = .5  # Please change me to be between zero and one\n",
    "episode_rewards = [-.9, 1.2, .5, -.6, 1, .6, .2, 0, .4, .5]\n",
    "\n",
    "\n",
    "def discount_episode(rewards, gamma):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        total_rewards = rewards[t] + total_rewards * gamma\n",
    "        discounted_rewards[t] = total_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "discount_episode(episode_rewards, test_gamma)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16308594,  1.47382812,  0.54765625,  0.0953125 ,  1.390625  ,\n",
       "        0.78125   ,  0.3625    ,  0.325     ,  0.65      ,  0.5       ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wherever our discounted reward is positive, we'll increase the probability corresponding to the action we took. Similarly, wherever our discounted reward is negative, we'll decrease the probabilty.\n",
    "\n",
    "However, with this strategy, any actions with a positive reward will have it's probability increase, not necessarily the most optimal action. This puts us in a feedback loop, where we're more likely to pick less optimal actions which could further increase their probability. To counter this, we'll divide the size of our increases by the probability to choose the corresponding action, which will slow the growth of popular actions to give other actions a chance.\n",
    "\n",
    "Here is our update rule for our neural network, where alpha is our learning rate, and pi is our optimal policy, or the probability to take the optimal action, a<sup>*</sup>, given our current state, s. \n",
    "\n",
    "<img src=\"images/weight_update.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "Doing some fancy calculus, we can combine the numerator and denominator with a log function. Since it's not clear what the optimal action is, we'll instead use our discounted rewards, or G, to increase or decrease the weights of the respective action the agent took. A full breakdown of the math can be found in [this article by Chris Yoon](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63).\n",
    "\n",
    "<img src=\"images/weight_update_calculus.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Below is what it looks like in code. `y_true` is the [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) of the action that was taken. `y_pred` is the probabilty to take each action given the state the agent was in."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:21:44.398401Z",
     "start_time": "2024-05-24T17:21:44.387675Z"
    }
   },
   "source": [
    "def custom_loss_test(y_true, y_pred):\n",
    "    y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1 - CLIP_EDGE)\n",
    "    log_likelihood = y_true * K.log(y_pred_clipped)\n",
    "    return K.sum(-log_likelihood * g)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have the discounted rewards, or `g`, when our agent is acting in the environment. No problem, we'll have one neural network with two types of pathways. One pathway, `predict`, will be the probability to take an action given an inputed state. It's only used for prediction and is not used for backpropogation. The other pathway, `policy`, will take both a state and a discounted reward, so it can be used for training.\n",
    "\n",
    "The code in its entirety looks like this. As with Deep Q Networks, the hidden layers of a Policy Gradient can use a CNN if the input state is pixels, but the last layer is typically a [Dense](https://keras.io/layers/core/) layer with a [Softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function to convert the output into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:09.470458Z",
     "start_time": "2024-05-24T17:32:09.464767Z"
    }
   },
   "source": [
    "def build_networks(\n",
    "        state_shape, action_size, learning_rate, hidden_neurons):\n",
    "    \"\"\"Creates a Policy Gradient Neural Network.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by the discounted reward, g.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden\n",
    "            layer.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    g = layers.Input((1,), name='g')\n",
    "\n",
    "    hidden_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    hidden_2 = layers.Dense(hidden_neurons, activation='relu')(hidden_1)\n",
    "    probabilities = layers.Dense(action_size, activation='softmax')(hidden_2)\n",
    "\n",
    "    # def custom_loss(y_true, y_pred):\n",
    "    #     y_pred_clipped = tf.clip_by_value(y_pred, CLIP_EDGE, 1 - CLIP_EDGE)\n",
    "    #     log_lik = y_true * tf.math.log(y_pred_clipped)\n",
    "    #     return -tf.reduce_sum(log_lik * g)\n",
    "    def custom_loss(g):\n",
    "        def loss(y_true, y_pred):\n",
    "            y_pred_clipped = tf.clip_by_value(y_pred, CLIP_EDGE, 1 - CLIP_EDGE)\n",
    "            log_lik = y_true * tf.math.log(y_pred_clipped)\n",
    "            return -tf.reduce_sum(log_lik * g)\n",
    "        return loss\n",
    "\n",
    "    policy = models.Model(\n",
    "        inputs=[state_input, g], outputs=probabilities)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    policy.compile(loss=custom_loss(g), optimizer=optimizer)\n",
    "\n",
    "    predict = models.Model(inputs=[state_input], outputs=probabilities)\n",
    "    return policy, predict"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a taste of how these networks function. Run the below cell to build our test networks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:09.950166Z",
     "start_time": "2024-05-24T17:32:09.921241Z"
    }
   },
   "source": [
    "space_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Feel free to play with these\n",
    "test_learning_rate = .2\n",
    "test_hidden_neurons = 10\n",
    "\n",
    "test_policy, test_predict = build_networks(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't use the policy network until we build our learning function, but we can feed a state to the predict network so we can see our chances to pick our actions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:10.425616Z",
     "start_time": "2024-05-24T17:32:10.345059Z"
    }
   },
   "source": [
    "state = env.reset()[0]\n",
    "test_predict.predict(np.expand_dims(state, axis=0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4921248, 0.5078752]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the numbers should be close to `[.5, .5]`, with a little bit of variance due to the randomization of initializing the weights and the cart's starting position. In order to train, we'll need some memories to train on. The memory buffer here is simpler than DQN, as we don't have to worry about random sampling. We'll clear the buffer every time we train as we'll only hold one episode's worth of memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:10.763798Z",
     "start_time": "2024-05-24T17:32:10.757922Z"
    }
   },
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay buffer for Policy Gradient methods.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess TD(1) values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: a (state, action, reward) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns the list of episode experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure (\n",
    "                [states], [actions], [rewards]\n",
    "            }\n",
    "        \"\"\"\n",
    "        batch = np.asarray(self.buffer, dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a fake buffer to get a sense of the data we'll be training on. The cell below initializes our memory and runs through one episode of the game by alternating pushing the cart left and right.\n",
    "\n",
    "Try running it to see the data we'll be using for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:11.114886Z",
     "start_time": "2024-05-24T17:32:11.103805Z"
    }
   },
   "source": [
    "test_memory = Memory(test_gamma)\n",
    "actions = [x % 2 for x in range(200)]\n",
    "state = env.reset()[0]\n",
    "step = 0\n",
    "episode_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done and step < len(actions):\n",
    "    action = actions[step]  # In the future, our agents will define this.\n",
    "    state_prime, reward, done1, done2, info = env.step(action)\n",
    "    done = done1 or done2\n",
    "    episode_reward += reward\n",
    "    test_memory.add((state, action, reward))\n",
    "    step += 1\n",
    "    state = state_prime\n",
    "\n",
    "test_memory.sample()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(51, 4), dtype=float32, numpy=\n",
       " array([[-0.00737301, -0.02503685, -0.04321902, -0.00539892],\n",
       "        [-0.00787375, -0.2195132 , -0.043327  ,  0.27334073],\n",
       "        [-0.01226401, -0.02380067, -0.03786018, -0.03268669],\n",
       "        [-0.01274002, -0.21835981, -0.03851392,  0.24781466],\n",
       "        [-0.01710722, -0.02270959, -0.03355762, -0.05676333],\n",
       "        [-0.01756141, -0.21733472, -0.03469289,  0.22514589],\n",
       "        [-0.0219081 , -0.02173455, -0.03018997, -0.07827546],\n",
       "        [-0.0223428 , -0.216411  , -0.03175548,  0.20473164],\n",
       "        [-0.02667102, -0.02084965, -0.02766085, -0.09779703],\n",
       "        [-0.02708801, -0.21556447, -0.02961679,  0.18603222],\n",
       "        [-0.0313993 , -0.02003156, -0.02589615, -0.11584458],\n",
       "        [-0.03179993, -0.21477307, -0.02821304,  0.16855714],\n",
       "        [-0.03609539, -0.01925889, -0.0248419 , -0.13289097],\n",
       "        [-0.03648057, -0.21401635, -0.02749971,  0.15185232],\n",
       "        [-0.0407609 , -0.01851165, -0.02446267, -0.14937775],\n",
       "        [-0.04113113, -0.21327491, -0.02745022,  0.13548844],\n",
       "        [-0.04539663, -0.01777076, -0.02474045, -0.16572668],\n",
       "        [-0.04575204, -0.21252997, -0.02805499,  0.11904979],\n",
       "        [-0.05000264, -0.01701753, -0.02567399, -0.18235055],\n",
       "        [-0.05034299, -0.21176289, -0.029321  ,  0.10212389],\n",
       "        [-0.05457825, -0.01623327, -0.02727853, -0.19966362],\n",
       "        [-0.05490291, -0.21095465, -0.0312718 ,  0.08429091],\n",
       "        [-0.05912201, -0.01539872, -0.02958598, -0.21809198],\n",
       "        [-0.05942998, -0.21008553, -0.03394782,  0.0651134 ],\n",
       "        [-0.06363169, -0.01449371, -0.03264555, -0.23808402],\n",
       "        [-0.06392157, -0.20913444, -0.03740723,  0.04412543],\n",
       "        [-0.06810426, -0.01349661, -0.03652472, -0.2601212 ],\n",
       "        [-0.06837419, -0.20807864, -0.04172715,  0.0208213 ],\n",
       "        [-0.07253576, -0.01238388, -0.04131072, -0.28472954],\n",
       "        [-0.07278344, -0.20689304, -0.04700531, -0.00535661],\n",
       "        [-0.0769213 , -0.01112959, -0.04711244, -0.31249192],\n",
       "        [-0.07714389, -0.2055498 , -0.05336228, -0.03503111],\n",
       "        [-0.08125488, -0.00970485, -0.0540629 , -0.34406146],\n",
       "        [-0.08144899, -0.2040177 , -0.06094413, -0.06890479],\n",
       "        [-0.08552934, -0.00807734, -0.06232223, -0.38017616],\n",
       "        [-0.08569089, -0.20226148, -0.06992576, -0.10777631],\n",
       "        [-0.08973612, -0.00621078, -0.07208128, -0.42167512],\n",
       "        [-0.08986033, -0.20024142, -0.08051478, -0.15255849],\n",
       "        [-0.09386516, -0.00406442, -0.08356595, -0.46951646],\n",
       "        [-0.09394645, -0.19791262, -0.09295628, -0.20429865],\n",
       "        [-0.0979047 , -0.00159275, -0.09704226, -0.5247969 ],\n",
       "        [-0.09793656, -0.19522472, -0.10753819, -0.26420113],\n",
       "        [-0.10184105,  0.00125469, -0.11282221, -0.5887731 ],\n",
       "        [-0.10181595, -0.19212165, -0.12459768, -0.3336521 ],\n",
       "        [-0.10565839,  0.00453304, -0.13127072, -0.66288435],\n",
       "        [-0.10556773, -0.1885419 , -0.1445284 , -0.4142465 ],\n",
       "        [-0.10933857,  0.00830116, -0.15281333, -0.74877626],\n",
       "        [-0.10917255, -0.1844197 , -0.16778886, -0.50781614],\n",
       "        [-0.11286094,  0.01261956, -0.17794518, -0.84832317],\n",
       "        [-0.11260854, -0.17968734, -0.19491164, -0.61645794],\n",
       "        [-0.11620229,  0.01754651, -0.2072408 , -0.9636486 ]],\n",
       "       dtype=float32)>,\n",
       " array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0], dtype=int8),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to start putting together the agent! Let's start by giving it the ability to act. Here, we don't need to worry about exploration vs exploitation because we already have a random chance to take each of our actions. As the agent learns, it will naturally shift from exploration to exploitation. How conveient!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:11.490695Z",
     "start_time": "2024-05-24T17:32:11.485598Z"
    }
   },
   "source": [
    "class Partial_Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "\n",
    "    def __init__(self, policy, predict, memory, action_size):\n",
    "        \"\"\"Initializes the agent with Policy Gradient networks\n",
    "            and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            policy: The policy network created from build_networks().\n",
    "            predict: The predict network created from build_networks().\n",
    "            memory: A Memory class object.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.policy = policy\n",
    "        self.predict = predict\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        result = self.predict.predict(state_batch)\n",
    "        print(result)\n",
    "        probabilities = result[0]\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the act function in action. First, let's build our agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:11.921791Z",
     "start_time": "2024-05-24T17:32:11.918016Z"
    }
   },
   "source": [
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the below cell a few times to test the `act` method. Is it about a 50/50 chance to push right instead of left?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:12.342297Z",
     "start_time": "2024-05-24T17:32:12.268891Z"
    }
   },
   "source": [
    "action = test_agent.act(state)\n",
    "print(\"Push Right\" if action else \"Push Left\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "[[0.5004567 0.4995433]]\n",
      "Push Right\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the most important part. We need to give our agent a way to learn! To start, we'll [one-hot encode](https://en.wikipedia.org/wiki/One-hot) our actions. Since the output of our network is a probability for each action, we'll have a 1 corresponding to the action that was taken and 0's for the actions we didn't take.\n",
    "\n",
    "That doesn't give our agent enough information on whether the action that was taken was actually a good idea, so we'll also use our `discount_episode` to calculate the TD(1) value of each step within the episode. \n",
    "\n",
    "One thing to note, is that CartPole doesn't have any negative rewards, meaning, even if it does terribly, the agent will still think the run went well. To help counter this, we'll take the mean and standard deviation of our discounted rewards, or `discount_mb`, and use that to find the [Standard Score](https://en.wikipedia.org/wiki/Standard_score) for each discounted reward. With this, steps close to dropping the poll will have a negative reward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:12.705192Z",
     "start_time": "2024-05-24T17:32:12.699611Z"
    }
   },
   "source": [
    "def learn(self, print_variables=False):\n",
    "    \"\"\"Trains a Policy Gradient policy network based on stored experiences.\"\"\"\n",
    "    state_mb, action_mb, reward_mb = self.memory.sample()\n",
    "    # One hot enocde actions\n",
    "    action_mb = tf.convert_to_tensor(action_mb, dtype=tf.int32)\n",
    "    action_hot = tf.one_hot(action_mb, self.action_size)\n",
    "    if print_variables:\n",
    "        print(\"action_mb:\", action_mb)\n",
    "        print(\"actions:\", action_hot)\n",
    "\n",
    "    # Apply TD(1) and normalize\n",
    "    discount_mb = discount_episode(reward_mb, self.memory.gamma)\n",
    "    discount_mb = (discount_mb - np.mean(discount_mb)) / np.std(discount_mb)\n",
    "    discount_mb = tf.convert_to_tensor(discount_mb, dtype=tf.int32)\n",
    "    if print_variables:\n",
    "        print(\"reward_mb:\", reward_mb)\n",
    "        print(\"discount_mb:\", discount_mb)\n",
    "        print(\"state_mb:\", state_mb)\n",
    "    return self.policy.train_on_batch([state_mb, discount_mb], action_hot)\n",
    "\n",
    "\n",
    "Partial_Agent.learn = learn\n",
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding in some print statements to the code above to get a sense of how the data is transformed before feeding it into the model, then run the below code to see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to put it all together. Policy Gradient Networks have less hypertuning parameters than DQNs, but since our custom loss constructs a [TensorFlow Graph](https://www.tensorflow.org/api_docs/python/tf/Graph) under the hood, we'll set up lazy execution by wrapping our traing steps in a default graph.\n",
    "\n",
    "By changing `test_gamma`, `test_learning_rate`, and `test_hidden_neurons`, can you help the agent reach a score of 200 within 200 episodes? It takes a little bit of thinking and a little bit of luck.\n",
    "\n",
    "Hover the curser  <b title=\"gamma=.9, learning rate=0.002, neurons=50\">on this bold text</b> to see a solution to the challenge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T17:32:15.816575Z",
     "start_time": "2024-05-24T17:32:14.011587Z"
    }
   },
   "source": [
    "test_gamma = .5\n",
    "test_learning_rate = .01\n",
    "test_hidden_neurons = 100\n",
    "\n",
    "test_memory = Memory(test_gamma)\n",
    "test_policy, test_predict = build_networks(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)\n",
    "test_agent = Partial_Agent(test_policy, test_predict, test_memory, action_size)\n",
    "for episode in range(200):\n",
    "    state = env.reset()[0]\n",
    "    print(state)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    for i in range(200):\n",
    "        action = test_agent.act(state)\n",
    "        state_prime, reward, done1, done2, info = env.step(action)\n",
    "        done = done1 or done2\n",
    "        episode_reward += reward\n",
    "        test_agent.memory.add((state, action, reward))\n",
    "        state = state_prime\n",
    "        if done: break\n",
    "\n",
    "    test_agent.learn(True)\n",
    "    print(\"Episode\", episode, \"Score =\", episode_reward)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04507281 -0.01257197  0.02351838 -0.01019544]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step\n",
      "[[0.50040627 0.4995938 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "[[0.4926686 0.5073313]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "[[0.49978313 0.5002169 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "[[0.49286383 0.50713617]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "[[0.4866883  0.51331174]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step\n",
      "[[0.4937982  0.50620186]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step\n",
      "[[0.49870163 0.5012983 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n",
      "[[0.50199616 0.49800378]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "[[0.49906802 0.500932  ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step\n",
      "[[0.5022738  0.49772617]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step\n",
      "[[0.49898615 0.50101376]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "[[0.49276045 0.50723946]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "[[0.4989295 0.5010705]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step\n",
      "[[0.50134736 0.49865267]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "[[0.49815264 0.5018474 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "[[0.49222162 0.50777835]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "[[0.4855715  0.51442856]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step\n",
      "[[0.49290723 0.5070927 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "[[0.486177 0.513823]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "[[0.47974128 0.5202588 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "[[0.47357622 0.5264238 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "[[0.46769756 0.5323025 ]]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "[[0.46232134 0.5376786 ]]\n",
      "action_mb: tf.Tensor([0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0], shape=(23,), dtype=int32)\n",
      "actions: tf.Tensor(\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]], shape=(23, 2), dtype=float32)\n",
      "reward_mb: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "discount_mb: tf.Tensor([ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -4], shape=(23,), dtype=int32)\n",
      "state_mb: tf.Tensor(\n",
      "[[ 0.04507281 -0.01257197  0.02351838 -0.01019544]\n",
      " [ 0.04482137 -0.20802318  0.02331447  0.28981405]\n",
      " [ 0.04066091 -0.0132413   0.02911075  0.00457448]\n",
      " [ 0.04039608 -0.20876838  0.02920224  0.3062982 ]\n",
      " [ 0.03622071 -0.40429404  0.0353282   0.6080459 ]\n",
      " [ 0.02813483 -0.20968334  0.04748912  0.3266963 ]\n",
      " [ 0.02394116 -0.01526855  0.05402305  0.04935944]\n",
      " [ 0.02363579  0.17903881  0.05501024 -0.22580135]\n",
      " [ 0.02721657 -0.01682441  0.05049421  0.08371393]\n",
      " [ 0.02688008  0.17753872  0.05216849 -0.19262019]\n",
      " [ 0.03043086 -0.0182892   0.04831608  0.11605264]\n",
      " [ 0.03006507 -0.21406893  0.05063714  0.42357895]\n",
      " [ 0.02578369 -0.01969954  0.05910872  0.14727952]\n",
      " [ 0.0253897   0.17452833  0.06205431 -0.12618566]\n",
      " [ 0.02888027 -0.02142518  0.05953059  0.18541114]\n",
      " [ 0.02845176 -0.21734613  0.06323881  0.49626362]\n",
      " [ 0.02410484 -0.41330016  0.07316409  0.80818695]\n",
      " [ 0.01583884 -0.21925303  0.08932783  0.53938603]\n",
      " [ 0.01145378 -0.41550967  0.10011555  0.85882473]\n",
      " [ 0.00314359 -0.6118424   0.11729204  1.181234  ]\n",
      " [-0.00909326 -0.808275    0.14091672  1.5082632 ]\n",
      " [-0.02525876 -1.0047959   0.17108199  1.841415  ]\n",
      " [-0.04535468 -1.2013438   0.20791028  2.1819859 ]], shape=(23, 4), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m     state \u001B[38;5;241m=\u001B[39m state_prime\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done: \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[43mtest_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode\u001B[39m\u001B[38;5;124m\"\u001B[39m, episode, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScore =\u001B[39m\u001B[38;5;124m\"\u001B[39m, episode_reward)\n",
      "Cell \u001B[0;32mIn[32], line 19\u001B[0m, in \u001B[0;36mlearn\u001B[0;34m(self, print_variables)\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount_mb:\u001B[39m\u001B[38;5;124m\"\u001B[39m, discount_mb)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_mb:\u001B[39m\u001B[38;5;124m\"\u001B[39m, state_mb)\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstate_mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscount_mb\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_hot\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:540\u001B[0m, in \u001B[0;36mTensorFlowTrainer.train_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdata\u001B[39m():\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (x, y, sample_weight)\n\u001B[0;32m--> 540\u001B[0m logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    541\u001B[0m logs \u001B[38;5;241m=\u001B[39m tree\u001B[38;5;241m.\u001B[39mmap_structure(\u001B[38;5;28;01mlambda\u001B[39;00m x: np\u001B[38;5;241m.\u001B[39marray(x), logs)\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:117\u001B[0m, in \u001B[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m--> 117\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mone_step_on_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[1;32m    121\u001B[0m     outputs,\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy,\n\u001B[1;32m    123\u001B[0m     reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    124\u001B[0m )\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:104\u001B[0m, in \u001B[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;129m@tf\u001B[39m\u001B[38;5;241m.\u001B[39mautograph\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mdo_not_convert\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mone_step_on_data\u001B[39m(data):\n\u001B[1;32m    103\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:54\u001B[0m, in \u001B[0;36mTensorFlowTrainer.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(x)\n\u001B[0;32m---> 54\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss_tracker\u001B[38;5;241m.\u001B[39mupdate_state(\n\u001B[1;32m     58\u001B[0m     loss, sample_weight\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mshape(tree\u001B[38;5;241m.\u001B[39mflatten(x)[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     59\u001B[0m )\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/trainers/trainer.py:316\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    314\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compile_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 316\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compile_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    318\u001B[0m         losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/trainers/compile_utils.py:609\u001B[0m, in \u001B[0;36mCompileLoss.__call__\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_true, y_pred, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname):\n\u001B[0;32m--> 609\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/trainers/compile_utils.py:645\u001B[0m, in \u001B[0;36mCompileLoss.call\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m loss, y_t, y_p, loss_weight, sample_weight \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    637\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflat_losses,\n\u001B[1;32m    638\u001B[0m     y_true,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    641\u001B[0m     sample_weight,\n\u001B[1;32m    642\u001B[0m ):\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loss:\n\u001B[1;32m    644\u001B[0m         value \u001B[38;5;241m=\u001B[39m loss_weight \u001B[38;5;241m*\u001B[39m ops\u001B[38;5;241m.\u001B[39mcast(\n\u001B[0;32m--> 645\u001B[0m             \u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m, dtype\u001B[38;5;241m=\u001B[39mbackend\u001B[38;5;241m.\u001B[39mfloatx()\n\u001B[1;32m    646\u001B[0m         )\n\u001B[1;32m    647\u001B[0m         loss_values\u001B[38;5;241m.\u001B[39mappend(value)\n\u001B[1;32m    648\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m loss_values:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/losses/loss.py:43\u001B[0m, in \u001B[0;36mLoss.__call__\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m     36\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m tree\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype), y_pred\n\u001B[1;32m     38\u001B[0m )\n\u001B[1;32m     39\u001B[0m y_true \u001B[38;5;241m=\u001B[39m tree\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype), y_true\n\u001B[1;32m     41\u001B[0m )\n\u001B[0;32m---> 43\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m out_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(losses, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_keras_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m in_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m out_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/losses/losses.py:22\u001B[0m, in \u001B[0;36mLossFunctionWrapper.call\u001B[0;34m(self, y_true, y_pred)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_true, y_pred):\n\u001B[1;32m     21\u001B[0m     y_true, y_pred \u001B[38;5;241m=\u001B[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[24], line 31\u001B[0m, in \u001B[0;36mbuild_networks.<locals>.custom_loss.<locals>.loss\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     29\u001B[0m y_pred_clipped \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mclip_by_value(y_pred, CLIP_EDGE, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m CLIP_EDGE)\n\u001B[1;32m     30\u001B[0m log_lik \u001B[38;5;241m=\u001B[39m y_true \u001B[38;5;241m*\u001B[39m tf\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mlog(y_pred_clipped)\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_lik\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/backend/common/keras_tensor.py:91\u001B[0m, in \u001B[0;36mKerasTensor.__tf_tensor__\u001B[0;34m(self, dtype, name)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__tf_tensor__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 91\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     92\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     93\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mused when constructing Keras Functional models \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     96\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand `keras.operations`). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     98\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are likely doing something like:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     99\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m```\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx = Input(...)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    102\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf_fn(x)  # Invalid.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m```\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    105\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m```\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass MyLayer(Layer):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    107\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    def call(self, x):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    108\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m        return tf_fn(x)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    109\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx = MyLayer()(x)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    110\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m```\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    111\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Theory Behind Actor - Critic\n",
    "\n",
    "Now that we have the hang of Policy Gradients, let's combine this strategy with Deep Q Agents. We'll have one architecture to rule them all!\n",
    "\n",
    "Below is the setup for our neural networks. There are plenty of ways to go combining the two strategies. We'll be focusing on one varient called A2C, or Advantage Actor Critic. \n",
    "\n",
    "<img src=\"images/a2c_equation.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Here's the philosophy: We'll use our critic pathway to estimate the value of a state, or V(s). Given a state-action-new state transition, we can use our critic and the Bellman Equation to calculate the discounted value of the new state, or r + &gamma; * V(s').\n",
    "\n",
    "Like DQNs, this discounted value is the label the critic will train on. While that is happening, we can subtract V(s) and the discounted value of the new state to get the advantage, or A(s,a). In human terms, how much value was the action the agent took? This is what the actor, or the policy gradient portion or our network, will train on.\n",
    "\n",
    "Too long, didn't read: the critic's job is to learn how to asses the value of a state. The actor's job is to assign probabilities to it's available actions such that it increases its chance to move into a higher valued state.\n",
    "\n",
    "Below is our new `build_networks` function. Each line has been tagged with whether it comes from Deep Q Networks (`# DQN`), Policy Gradients (`# PG`), or is something new (`# New`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_networks(state_shape, action_size, learning_rate, critic_weight, hidden_neurons, entropy):\n",
    "    \"\"\"Creates Actor Critic Neural Networks.\n",
    "\n",
    "    Creates a two hidden-layer Policy Gradient Neural Network. The loss\n",
    "    function is altered to be a log-likelihood function weighted\n",
    "    by an action's advantage.\n",
    "\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        critic_weight (float): how much to weigh the critic's training loss.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden layer.\n",
    "        entropy (float): how much to enourage exploration versus exploitation.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    advantages = layers.Input((1,), name='advantages')  # PG, A instead of G\n",
    "\n",
    "    # PG\n",
    "    actor_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    actor_2 = layers.Dense(hidden_neurons, activation='relu')(actor_1)\n",
    "    probabilities = layers.Dense(action_size, activation='softmax')(actor_2)\n",
    "\n",
    "    # DQN\n",
    "    critic_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    critic_2 = layers.Dense(hidden_neurons, activation='relu')(critic_1)\n",
    "    values = layers.Dense(1, activation='linear')(critic_2)\n",
    "\n",
    "    def actor_loss(y_true, y_pred):  # PG\n",
    "        y_pred_clipped = K.clip(y_pred, CLIP_EDGE, 1 - CLIP_EDGE)\n",
    "        log_lik = y_true * K.log(y_pred_clipped)\n",
    "        entropy_loss = y_pred * K.log(K.clip(y_pred, CLIP_EDGE, 1 - CLIP_EDGE))  # New\n",
    "        return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))\n",
    "\n",
    "    # Train both actor and critic at the same time.\n",
    "    actor = models.Model(\n",
    "        inputs=[state_input, advantages], outputs=[probabilities, values])\n",
    "    actor.compile(\n",
    "        loss=[actor_loss, 'mean_squared_error'],  # [PG, DQN]\n",
    "        loss_weights=[1, critic_weight],  # [PG, DQN]\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "    critic = models.Model(inputs=[state_input], outputs=[values])\n",
    "    policy = models.Model(inputs=[state_input], outputs=[probabilities])\n",
    "    return actor, critic, policy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is one way to go about combining both of the algorithms. Here, we're combining training of both pwathways into on operation. Keras allows for the [training against multiple outputs](https://keras.io/models/model/). They can even have their own loss functions as we have above. When minimizing the loss, Keras will take the weighted sum of all the losses, with the weights provided in `loss_weights`. The `critic_weight` is now another hyperparameter for us to tune.\n",
    "\n",
    "We could even have completely separate networks for the actor and the critic, and that type of design choice is going to be problem dependent. Having shared nodes and training between the two will be more efficient to train per batch, but more complicated problems could justify keeping the two separate.\n",
    "\n",
    "The loss function we used here is also slightly different than the one for Policy Gradients. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def actor_loss(y_true, y_pred):  # PG\n",
    "    y_pred_clipped = K.clip(y_pred, 1e-8, 1 - 1e-8)\n",
    "    log_lik = y_true * K.log(y_pred_clipped)\n",
    "    entropy_loss = y_pred * K.log(K.clip(y_pred, 1e-8, 1 - 1e-8))  # New\n",
    "    return K.sum(-log_lik * advantages) - (entropy * K.sum(entropy_loss))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've added a new tool called [entropy](https://arxiv.org/pdf/1912.01557.pdf). We're calculating the [log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) again, but instead of comparing the probabilities of our actions versus the action that was taken, we calculating it for the probabilities of our actions against themselves.\n",
    "\n",
    "Certainly a mouthful, but the idea is to encourage exploration: if our probability prediction is very confident (or close to 1), our entropy will be close to 0. Similary, if our probability isn't confident at all (or close to 0), our entropy will again be zero. Anywhere inbetween, our entropy will be non-zero. This encourages exploration versus exploitation, as the entropy will discourage overconfident predictions.\n",
    "\n",
    "Now that the networks are out of the way, let's look at the `Memory`. We could go with Experience Replay, like with DQNs, or we could calculate TD(1) like with Policy Gradients. This time, we'll do something in between. We'll give our memory a `batch_size`. Once there are enough experiences in the buffer, we'll use all the experiences to train and then clear the buffer to start fresh.\n",
    "\n",
    "In order to speed up training, instead of recording state_prime, we'll record the value of state prime in `state_prime_values` or `next_values`. This will give us enough information to calculate the discounted values and advantages."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay for actor-critic training.\n",
    "\n",
    "    Args:\n",
    "        gamma (float): The \"discount rate\" used to assess state values.\n",
    "        batch_size (int): The number of elements to include in the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma, batch_size):\n",
    "        self.buffer = []\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: (state, action, reward, state_prime_value, done) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def check_full(self):\n",
    "        return len(self.buffer) >= self.batch_size\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Returns formated experiences and clears the buffer.\n",
    "\n",
    "        Returns:\n",
    "            (list): A tuple of lists with structure [\n",
    "                [states], [actions], [rewards], [state_prime_values], [dones]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # Columns have different data types, so numpy array would be awkward.\n",
    "        batch = np.asarray(self.buffer, dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        dones_mb = np.array(batch[3], dtype=np.int8)\n",
    "        value_mb = np.squeeze(np.array(batch[4], dtype=np.float32))\n",
    "        self.buffer = []\n",
    "        return states_mb, actions_mb, rewards_mb, dones_mb, value_mb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time to build out the agent! The `act` method is the exact same as it was for Policy Gradients. Nice! The `learn` method is where things get interesting. We'll find the discounted future state like we did for DQN to train our critic. We'll then subtract the value of the discount state from the value of the current state to find the advantage, which is what the actor will train on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "\n",
    "    def __init__(self, actor, critic, policy, memory, action_size):\n",
    "        \"\"\"Initializes the agent with DQN and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            network: A neural network created from deep_q_network().\n",
    "            memory: A Memory class object.\n",
    "            epsilon_decay (float): The rate at which to decay random actions.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.policy = policy\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "            traning (bool): True if the agent is training.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        probabilities = self.policy.predict(state_batch)[0]\n",
    "        action = np.random.choice(self.action_size, p=probabilities)\n",
    "        return action\n",
    "\n",
    "    def learn(self, print_variables=False):\n",
    "        \"\"\"Trains the Deep Q Network based on stored experiences.\"\"\"\n",
    "        gamma = self.memory.gamma\n",
    "        experiences = self.memory.sample()\n",
    "        state_mb, action_mb, reward_mb, dones_mb, next_value = experiences\n",
    "\n",
    "        # One hot enocde actions\n",
    "        actions = np.zeros([len(action_mb), self.action_size])\n",
    "        actions[np.arange(len(action_mb)), action_mb] = 1\n",
    "\n",
    "        #Apply TD(0)\n",
    "        discount_mb = reward_mb + next_value * gamma * (1 - dones_mb)\n",
    "        state_values = self.critic.predict([state_mb])\n",
    "        advantages = discount_mb - np.squeeze(state_values)\n",
    "        if print_variables:\n",
    "            print(\"discount_mb\", discount_mb)\n",
    "            print(\"next_value\", next_value)\n",
    "            print(\"state_values\", state_values)\n",
    "            print(\"advantages\", advantages)\n",
    "        else:\n",
    "            self.actor.train_on_batch(\n",
    "                [state_mb, advantages], [actions, discount_mb])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to initialize an agent, and the cell after that to see the variables used for training. Since it's early, the critic hasn't learned to estimate the values yet, and the advatanges are mostly positive because of it.\n",
    "\n",
    "Once the crtic has learned how to properly assess states, the actor will start to see negative advantages. Try playing around with the variables to help the agent see this change sooner."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Change me please.\n",
    "test_gamma = .9\n",
    "test_batch_size = 32\n",
    "test_learning_rate = .02\n",
    "test_hidden_neurons = 50\n",
    "test_critic_weight = 0.5\n",
    "test_entropy = 0.0001\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma, test_batch_size)\n",
    "    test_actor, test_critic, test_policy = build_networks(\n",
    "        space_shape, action_size,\n",
    "        test_learning_rate, test_critic_weight,\n",
    "        test_hidden_neurons, test_entropy)\n",
    "    test_agent = Agent(\n",
    "        test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = test_agent.act(state)\n",
    "        state_prime, reward, done1, done2, _ = env.step(action)\n",
    "        done = done1 or done2\n",
    "        episode_reward += reward\n",
    "        next_value = test_agent.critic.predict([[state_prime]])\n",
    "        test_agent.memory.add((state, action, reward, done, next_value))\n",
    "        state = state_prime\n",
    "        test_agent.learn(print_variables=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a set of variables you're happy with? Ok, time to shine! Run the below cell to see how the agent trains."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with tf.Graph().as_default():\n",
    "    test_memory = Memory(test_gamma, test_batch_size)\n",
    "    test_actor, test_critic, test_policy = build_networks(\n",
    "        space_shape, action_size,\n",
    "        test_learning_rate, test_critic_weight,\n",
    "        test_hidden_neurons, test_entropy)\n",
    "    test_agent = Agent(\n",
    "        test_actor, test_critic, test_policy, test_memory, action_size)\n",
    "    for episode in range(200):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = test_agent.act(state)\n",
    "            state_prime, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            next_value = test_agent.critic.predict([[state_prime]])\n",
    "            test_agent.memory.add((state, action, reward, done, next_value))\n",
    "\n",
    "            #if test_agent.memory.check_full():\n",
    "            #test_agent.learn(print_variables=True)\n",
    "            state = state_prime\n",
    "        test_agent.learn()\n",
    "        print(\"Episode\", episode, \"Score =\", episode_reward)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any luck? No sweat if not! It turns out that by combining the power of both algorithms, we also combined some of their setbacks. For instance, actor-critic can fall into local minimums like Policy Gradients, and has a large number of hyperparameters to tune like DQNs.\n",
    "\n",
    "Time to check how our agents did [in the cloud](https://console.cloud.google.com/ai-platform/jobs)! Any lucky winners? Find it in [your bucket](https://console.cloud.google.com/storage/browser) to watch a recording of it play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
