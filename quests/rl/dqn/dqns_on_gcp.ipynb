{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQNs on GCP\n",
    "\n",
    "Reinforcement Learning (RL) Agents can be quite fickle. This is because the environment for an Agent is different than that of Supervised and Unsupervised algorithms.\n",
    "\n",
    "| Supervised / Unsupervised | Reinforcement Learning |\n",
    "| ----------- | ----------- |\n",
    "| Data is previously gathered | Data needs to be simulated |\n",
    "| Big Data: Many examples covering many siutations | Sparse Data: Agent trades off between exploring and exploiting | \n",
    "| The environment is assumed static | The environment may change in response to the agent |\n",
    "\n",
    "Because of this, hyperparameter tuning is even more crucial in RL as it not only impacts the training of the agent's neural network, but it also impacts how the data is gathered through simulation.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Hypertuning takes some time, and in this case, it can take anywhere between **10 - 30 minutes**. If this hasn't been done already, run the cell below to kick off the training job now. We'll step through what the code is doing while our agents learn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%bash\n",
    "BUCKET=<your-bucket-here> # Change to your bucket name\n",
    "JOB_NAME=dqn_on_gcp_$(date -u +%y%m%d_%H%M%S)\n",
    "REGION='us-central1' # Change to your bucket region\n",
    "IMAGE_URI=gcr.io/qwiklabs-resources/rl-qwikstart/dqn_on_gcp@sha256:326427527d07f30a0486ee05377d120cac1b9be8850b05f138fc9b53ac1dd2dc\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --job-dir=gs://$BUCKET/$JOB_NAME \\\n",
    "    --config=hyperparam.yaml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command sends a [hyperparameter tuning job](https://cloud.google.com/ml-engine/docs/hyperparameter-tuning-overview) to the [Google Cloud AI Platform](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training). It's a service that sets up scaling distributed training so data scientists and machine learning engineers do not have to worry about technical infrastructure. Usually, it automatically selects the [container environment](https://cloud.google.com/ml-engine/docs/runtime-version-list), but we're going to take advantage of a feature to specify our own environment with [Docker](https://www.docker.com/resources/what-container). Not only will this allow us to install our game environment to be deployed to the cloud, but it will also significantly speed up hyperparameter tuning time as each worker can skip the library installation steps.\n",
    "\n",
    "The <a href=\"Dockerfile\">Dockerfile</a> in this directory shows the steps taken to build this environment. First, we copy from a [Google Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers/docs/choosing-container) which already has Google Cloud Libraries installed. Then, we install our other desired modules and libraries. `ffmpeg`, `xvfb`, and `python-opengl` are needed in order to get video output from the server. Machines on the cloud don't typically have a display (why would they need one?), so we'll make a virtual display of our own.\n",
    "\n",
    "After we copy our code, we tell the container to be configured as an executable so we can pass our hyperparameter tuning flags to it with the [ENTRYPOINT](https://stackoverflow.com/questions/21553353/what-is-the-difference-between-cmd-and-entrypoint-in-a-dockerfile) command. In order to set up our virtual display, we can use the [xvfb-run](http://manpages.ubuntu.com/manpages/trusty/man1/xvfb-run.1.html) command. Unfortunately, Docker strips quotes from specified commands in ENTRYPOINT, so we'll make a super simple shell script, <a href=\"train_model.sh\">train_model.sh</a>, to specify our virtual display parameters. The `\"@\"` parameter is used to pass the flags called against the container to our python module, `trainer.trainer`.\n",
    "\n",
    "## CartPole-v0\n",
    "\n",
    "So what is the game we'll be solving for? We'll be playing with [AI Gym's CartPole Environment](https://gym.openai.com/envs/CartPole-v1/). As MNIST is the \"Hello World\" of image classification, CartPole is the \"Hello World\" of Deep Q Networks. Let's install [OpenAI Gym](https://gym.openai.com/) and play with the game ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !python3 -m pip freeze | grep gym || python3 -m pip install --user gym==0.26.2\n",
    "# !python3 -m pip freeze | grep 'tensorflow==2.5\\|tensorflow-gpu==2.1' || \\\n",
    "# !python3 -m pip install -U tensorflow==2.3.0\n",
    "# !python3 -m pip install pygame"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: Restart the kernel if the above libraries needed to be installed. Please ignore incompatibility errors.\n",
    "\n",
    "The `gym` library hosts a number of different gaming environments that our agents (and us humans) can play around in. To make an environment, we simply need to pass it what game we'd like to play with the `make` method.\n",
    "\n",
    "This will create an environment object with a number of useful methods and properties.\n",
    "* The `observation_space` parameter is the structure of observations about the environment.\n",
    "  - Each \"state\" or snapshot or our environment will follow this structure\n",
    "* The `action_space` parameter is the possible actions the agent can take\n",
    "\n",
    "So for example, with CartPole, there are 4 observation dimensions which represent `[Cart Position, Cart Velocity, Pole Angle, Pole Velocity At Tip]`. For the actions, there are 2 possible actions to take: 0 pushes the cart to the left, and 1 pushes the cart to the right. More detail is described in the game's code [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:21:11.312368Z",
     "start_time": "2024-05-23T11:21:11.305618Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"sdffffffffffffffdddddddddddddddddddddddd\")\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdffffffffffffffdddddddddddddddddddddddd\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:29:53.321949Z",
     "start_time": "2024-05-23T12:29:49.111782Z"
    }
   },
   "source": [
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print(\"The observation space is\", env.observation_space)\n",
    "print(\"The observation dimensions are\", env.observation_space.shape)\n",
    "print(\"The action space is\", env.action_space)\n",
    "print(\"The number of possible actions is\", env.action_space.n)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 12:29:50.642151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 12:29:51.719437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space is Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "The observation dimensions are (4,)\n",
      "The action space is Discrete(2)\n",
      "The number of possible actions is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001B[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001B[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `reset` method will restart the environment and return a starting state.\n",
    "* The `step` method takes an action, applies it to the environment and returns a new state. Each step returns a new state, the transition reward, whether the game is over or not, and game specific information. For CartPole, there is no extra info, so it returns a blank dictionary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:29:55.964622Z",
     "start_time": "2024-05-23T12:29:55.958850Z"
    }
   },
   "source": [
    "def print_state(state, step, reward=None):\n",
    "    format_string = 'Step {0} - Cart X: {1:.3f}, Cart V: {2:.3f}, Pole A: {3:.3f}, Pole V:{4:.3f}, Reward:{5}'\n",
    "    print(state)\n",
    "    print(format_string.format(step, *tuple(state), reward))\n",
    "\n",
    "\n",
    "state = env.reset()[0]\n",
    "step = 0\n",
    "print_state(state, step)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04168174  0.04805871 -0.01286838  0.0132282 ]\n",
      "Step 0 - Cart X: 0.042, Cart V: 0.048, Pole A: -0.013, Pole V:0.013, Reward:None\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:29:58.568473Z",
     "start_time": "2024-05-23T12:29:58.562884Z"
    }
   },
   "source": [
    "state = env.reset()\n",
    "step = 0\n",
    "action = 0\n",
    "step_result = env.step(action)\n",
    "print(step_result)\n",
    "observation, reward, terminated, truncated, info = step_result\n",
    "step += 1\n",
    "print_state(observation, step, reward)\n",
    "print(\"The game is over.\" if terminated or truncated else \"The game can continue.\")\n",
    "print(\"Info:\", info)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04612059, -0.1559109 , -0.00918362,  0.32008505], dtype=float32), 1.0, False, False, {})\n",
      "[ 0.04612059 -0.1559109  -0.00918362  0.32008505]\n",
      "Step 1 - Cart X: 0.046, Cart V: -0.156, Pole A: -0.009, Pole V:0.320, Reward:1.0\n",
      "The game can continue.\n",
      "Info: {}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below repeatedly until the game is over, changing the action to push the cart left (0) or right (1). The game is considered \"won\" when the pole can stay up for an average of steps 195 over 100 games. How far can you get? An agent acting randomly can only survive about 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:30:03.556605Z",
     "start_time": "2024-05-23T12:30:03.551854Z"
    }
   },
   "source": [
    "action = 1  # Change me: 0 Left, 1 Right\n",
    "state_prime, reward, done1, done2, info = env.step(action)\n",
    "step += 1\n",
    "\n",
    "print_state(state_prime, step, reward)\n",
    "print(\"The game is over.\" if done1 or done2 else \"The game can continue.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04300237  0.03934064 -0.00278192  0.02452016]\n",
      "Step 2 - Cart X: 0.043, Cart V: 0.039, Pole A: -0.003, Pole V:0.025, Reward:1.0\n",
      "The game can continue.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make our own policy and create a loop to play through an episode (one full simulation) of the game. Below, actions are generated to alternate between pushing the cart left and right. The code is very similar to how our agents will be interacting with the game environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T12:54:53.884313Z",
     "start_time": "2024-05-23T12:54:53.871546Z"
    }
   },
   "source": [
    "\n",
    "# [0, 1, 0, 1, 0, 1, ...]\n",
    "actions = [x % 2 for x in range(200)]\n",
    "state = env.reset()\n",
    "step = 0\n",
    "episode_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done and step < len(actions):\n",
    "    action = actions[step]  # In the future, our agents will define this.\n",
    "    state_prime, reward, done1, done2, info = env.step(action)\n",
    "    done = done1 or done2\n",
    "    episode_reward += reward\n",
    "    step += 1\n",
    "    state = state_prime\n",
    "    print_state(state, step, reward)\n",
    "\n",
    "end_statement = \"Game over!\" if done else \"Ran out of actions!\"\n",
    "print(end_statement, \"Score =\", episode_reward)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02258059 -0.17395383 -0.03433017  0.3087484 ]\n",
      "Step 1 - Cart X: -0.023, Cart V: -0.174, Pole A: -0.034, Pole V:0.309, Reward:1.0\n",
      "[-0.02605966  0.02164002 -0.02815521  0.00543941]\n",
      "Step 2 - Cart X: -0.026, Cart V: 0.022, Pole A: -0.028, Pole V:0.005, Reward:1.0\n",
      "[-0.02562686 -0.17306706 -0.02804642  0.28910774]\n",
      "Step 3 - Cart X: -0.026, Cart V: -0.173, Pole A: -0.028, Pole V:0.289, Reward:1.0\n",
      "[-0.0290882   0.02244337 -0.02226426 -0.01228713]\n",
      "Step 4 - Cart X: -0.029, Cart V: 0.022, Pole A: -0.022, Pole V:-0.012, Reward:1.0\n",
      "[-0.02863934 -0.17235233 -0.02251001  0.27328885]\n",
      "Step 5 - Cart X: -0.029, Cart V: -0.172, Pole A: -0.023, Pole V:0.273, Reward:1.0\n",
      "[-0.03208638  0.02308345 -0.01704423 -0.02640795]\n",
      "Step 6 - Cart X: -0.032, Cart V: 0.023, Pole A: -0.017, Pole V:-0.026, Reward:1.0\n",
      "[-0.03162472 -0.17178997 -0.01757239  0.26084897]\n",
      "Step 7 - Cart X: -0.032, Cart V: -0.172, Pole A: -0.018, Pole V:0.261, Reward:1.0\n",
      "[-0.03506051  0.02357836 -0.01235541 -0.03732429]\n",
      "Step 8 - Cart X: -0.035, Cart V: 0.024, Pole A: -0.012, Pole V:-0.037, Reward:1.0\n",
      "[-0.03458895 -0.17136426 -0.01310189  0.25143492]\n",
      "Step 9 - Cart X: -0.035, Cart V: -0.171, Pole A: -0.013, Pole V:0.251, Reward:1.0\n",
      "[-0.03801623  0.02394231 -0.0080732  -0.04535165]\n",
      "Step 10 - Cart X: -0.038, Cart V: 0.024, Pole A: -0.008, Pole V:-0.045, Reward:1.0\n",
      "[-0.03753738 -0.17106295 -0.00898023  0.24477322]\n",
      "Step 11 - Cart X: -0.038, Cart V: -0.171, Pole A: -0.009, Pole V:0.245, Reward:1.0\n",
      "[-0.04095865  0.02418611 -0.00408477 -0.05072871]\n",
      "Step 12 - Cart X: -0.041, Cart V: 0.024, Pole A: -0.004, Pole V:-0.051, Reward:1.0\n",
      "[-0.04047492 -0.17087704 -0.00509934  0.24066265]\n",
      "Step 13 - Cart X: -0.040, Cart V: -0.171, Pole A: -0.005, Pole V:0.241, Reward:1.0\n",
      "[-0.04389247  0.02431739 -0.00028609 -0.05362438]\n",
      "Step 14 - Cart X: -0.044, Cart V: 0.024, Pole A: -0.000, Pole V:-0.054, Reward:1.0\n",
      "[-0.04340612 -0.17080046 -0.00135857  0.23896827]\n",
      "Step 15 - Cart X: -0.043, Cart V: -0.171, Pole A: -0.001, Pole V:0.239, Reward:1.0\n",
      "[-0.04682212  0.02434088  0.00342079 -0.05414288]\n",
      "Step 16 - Cart X: -0.047, Cart V: 0.024, Pole A: 0.003, Pole V:-0.054, Reward:1.0\n",
      "[-0.04633531 -0.17082995  0.00233793  0.23961736]\n",
      "Step 17 - Cart X: -0.046, Cart V: -0.171, Pole A: 0.002, Pole V:0.240, Reward:1.0\n",
      "[-0.04975191  0.02425852  0.00713028 -0.05232719]\n",
      "Step 18 - Cart X: -0.050, Cart V: 0.024, Pole A: 0.007, Pole V:-0.052, Reward:1.0\n",
      "[-0.04926674 -0.17096494  0.00608374  0.24259683]\n",
      "Step 19 - Cart X: -0.049, Cart V: -0.171, Pole A: 0.006, Pole V:0.243, Reward:1.0\n",
      "[-0.05268604  0.02406958  0.01093567 -0.04816093]\n",
      "Step 20 - Cart X: -0.053, Cart V: 0.024, Pole A: 0.011, Pole V:-0.048, Reward:1.0\n",
      "[-0.05220464 -0.17120746  0.00997245  0.24795213]\n",
      "Step 21 - Cart X: -0.052, Cart V: -0.171, Pole A: 0.010, Pole V:0.248, Reward:1.0\n",
      "[-0.05562879  0.02377066  0.0149315  -0.04156864]\n",
      "Step 22 - Cart X: -0.056, Cart V: 0.024, Pole A: 0.015, Pole V:-0.042, Reward:1.0\n",
      "[-0.05515338 -0.17156218  0.01410013  0.25578767]\n",
      "Step 23 - Cart X: -0.055, Cart V: -0.172, Pole A: 0.014, Pole V:0.256, Reward:1.0\n",
      "[-0.05858462  0.02335564  0.01921588 -0.03241472]\n",
      "Step 24 - Cart X: -0.059, Cart V: 0.023, Pole A: 0.019, Pole V:-0.032, Reward:1.0\n",
      "[-0.05811751 -0.17203653  0.01856758  0.26626855]\n",
      "Step 25 - Cart X: -0.058, Cart V: -0.172, Pole A: 0.019, Pole V:0.266, Reward:1.0\n",
      "[-0.06155824  0.02281557  0.02389295 -0.02050066]\n",
      "Step 26 - Cart X: -0.062, Cart V: 0.023, Pole A: 0.024, Pole V:-0.021, Reward:1.0\n",
      "[-0.06110193 -0.17264074  0.02348294  0.279624  ]\n",
      "Step 27 - Cart X: -0.061, Cart V: -0.173, Pole A: 0.023, Pole V:0.280, Reward:1.0\n",
      "[-0.06455474  0.02213849  0.02907542 -0.00556095]\n",
      "Step 28 - Cart X: -0.065, Cart V: 0.022, Pole A: 0.029, Pole V:-0.006, Reward:1.0\n",
      "[-0.06411198 -0.17338812  0.0289642   0.29615197]\n",
      "Step 29 - Cart X: -0.064, Cart V: -0.173, Pole A: 0.029, Pole V:0.296, Reward:1.0\n",
      "[-0.06757974  0.0213092   0.03488724  0.01274276]\n",
      "Step 30 - Cart X: -0.068, Cart V: 0.021, Pole A: 0.035, Pole V:0.013, Reward:1.0\n",
      "[-0.06715355 -0.17429526  0.0351421   0.31622568]\n",
      "Step 31 - Cart X: -0.067, Cart V: -0.174, Pole A: 0.035, Pole V:0.316, Reward:1.0\n",
      "[-0.07063946  0.02030897  0.04146661  0.03482922]\n",
      "Step 32 - Cart X: -0.071, Cart V: 0.020, Pole A: 0.041, Pole V:0.035, Reward:1.0\n",
      "[-0.07023328 -0.17538233  0.04216319  0.34030154]\n",
      "Step 33 - Cart X: -0.070, Cart V: -0.175, Pole A: 0.042, Pole V:0.340, Reward:1.0\n",
      "[-0.07374092  0.01911513  0.04896922  0.06120694]\n",
      "Step 34 - Cart X: -0.074, Cart V: 0.019, Pole A: 0.049, Pole V:0.061, Reward:1.0\n",
      "[-0.07335863 -0.17667349  0.05019337  0.368929  ]\n",
      "Step 35 - Cart X: -0.073, Cart V: -0.177, Pole A: 0.050, Pole V:0.369, Reward:1.0\n",
      "[-0.07689209  0.01770069  0.05757194  0.09248559]\n",
      "Step 36 - Cart X: -0.077, Cart V: 0.018, Pole A: 0.058, Pole V:0.092, Reward:1.0\n",
      "[-0.07653808 -0.17819718  0.05942165  0.40276232]\n",
      "Step 37 - Cart X: -0.077, Cart V: -0.178, Pole A: 0.059, Pole V:0.403, Reward:1.0\n",
      "[-0.08010202  0.01603387  0.06747691  0.12938967]\n",
      "Step 38 - Cart X: -0.080, Cart V: 0.016, Pole A: 0.067, Pole V:0.129, Reward:1.0\n",
      "[-0.07978135 -0.17998655  0.07006469  0.44257435]\n",
      "Step 39 - Cart X: -0.080, Cart V: -0.180, Pole A: 0.070, Pole V:0.443, Reward:1.0\n",
      "[-0.08338108  0.01407763  0.07891618  0.17277445]\n",
      "Step 40 - Cart X: -0.083, Cart V: 0.014, Pole A: 0.079, Pole V:0.173, Reward:1.0\n",
      "[-0.08309952 -0.1820799   0.08237167  0.4892723 ]\n",
      "Step 41 - Cart X: -0.083, Cart V: -0.182, Pole A: 0.082, Pole V:0.489, Reward:1.0\n",
      "[-0.08674112  0.01178924  0.09215712  0.22364448]\n",
      "Step 42 - Cart X: -0.087, Cart V: 0.012, Pole A: 0.092, Pole V:0.224, Reward:1.0\n",
      "[-0.08650534 -0.18452077  0.09663001  0.5439158 ]\n",
      "Step 43 - Cart X: -0.087, Cart V: -0.185, Pole A: 0.097, Pole V:0.544, Reward:1.0\n",
      "[-0.09019575  0.00911994  0.10750832  0.28317478]\n",
      "Step 44 - Cart X: -0.090, Cart V: 0.009, Pole A: 0.108, Pole V:0.283, Reward:1.0\n",
      "[-0.09001336 -0.18735807  0.11317182  0.6077369 ]\n",
      "Step 45 - Cart X: -0.090, Cart V: -0.187, Pole A: 0.113, Pole V:0.608, Reward:1.0\n",
      "[-0.09376051  0.0060149   0.12532656  0.35273448]\n",
      "Step 46 - Cart X: -0.094, Cart V: 0.006, Pole A: 0.125, Pole V:0.353, Reward:1.0\n",
      "[-0.09364022 -0.19064565  0.13238125  0.6821613 ]\n",
      "Step 47 - Cart X: -0.094, Cart V: -0.191, Pole A: 0.132, Pole V:0.682, Reward:1.0\n",
      "[-0.09745313  0.00241367  0.14602447  0.4339126 ]\n",
      "Step 48 - Cart X: -0.097, Cart V: 0.002, Pole A: 0.146, Pole V:0.434, Reward:1.0\n",
      "[-0.09740486 -0.19444115  0.15470272  0.768831  ]\n",
      "Step 49 - Cart X: -0.097, Cart V: -0.194, Pole A: 0.155, Pole V:0.769, Reward:1.0\n",
      "[-0.10129368 -0.00174853  0.17007935  0.5285454 ]\n",
      "Step 50 - Cart X: -0.101, Cart V: -0.002, Pole A: 0.170, Pole V:0.529, Reward:1.0\n",
      "[-0.10132865 -0.1988037   0.18065025  0.86962587]\n",
      "Step 51 - Cart X: -0.101, Cart V: -0.199, Pole A: 0.181, Pole V:0.870, Reward:1.0\n",
      "[-0.10530473 -0.0065381   0.19804277  0.63874334]\n",
      "Step 52 - Cart X: -0.105, Cart V: -0.007, Pole A: 0.198, Pole V:0.639, Reward:1.0\n",
      "[-0.10543548 -0.20378956  0.21081764  0.9866819 ]\n",
      "Step 53 - Cart X: -0.105, Cart V: -0.204, Pole A: 0.211, Pole V:0.987, Reward:1.0\n",
      "Game over! Score = 53.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a challenge to get to 200! We could repeatedly experiment to find the best heuristics to beat the game, or we could leave all that work to the robot. Let's create an intelligence to figure this out for us.\n",
    "\n",
    "## The Theory Behind Deep Q Networks\n",
    "\n",
    "The fundamental principle behind RL is we have two entities: the **agent** and the **environment**. The agent takes state and reward information about the envionment and chooses an action. The environment takes that action and will change to be in a new state.\n",
    "\n",
    "<img src=\"images/agent_and_environment.jpg\" width=\"476\" height=\"260\">\n",
    "\n",
    "RL assumes that the environment follows a [Markov Decision Process (MDP)](https://en.wikipedia.org/wiki/Markov_decision_process). That means the state is dependent partially on the agent's actions, and partially on chance. MDPs can be represented by a graph, with states and actions as nodes, and rewards and path probabilities on the edges.\n",
    "\n",
    "<img src=\"images/mdp.jpg\" width=\"471\" height=\"243\">\n",
    "\n",
    "So what would be the best path through the graph above? Or perhaps a more difficult question, what would be our expected winnings if we played optimally? The probability introduced in this problem has inspired multiple strategies over the years, but all of them boil down to the idea of discounted future rewards.\n",
    "\n",
    "Would you rather have `$100` now or `$105` a year from now? With inflation, there's no definitive answer, but each of us has a threshold that we use to determine the value of something now versus the value of something later. In psychology, this is called [Delayed Gratification](https://en.wikipedia.org/wiki/Delayed_gratification). Richard E. Bellman expressed this theory in an equation widely used in RL called the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation). Let's introduce some vocab to better define it.\n",
    "\n",
    "| Symbol | Name | Definition | Example |\n",
    "| - | - | - | - |\n",
    "| | agent | An entity that can act and transition between states | Us when we play CartPole |\n",
    "| s | state | The environmental parameters describing where the agent is | The position of the cart and angle of the pole |\n",
    "| a | action | What the agent can do within a state | Pushing the cart left or right |\n",
    "| t | time / step | One transition between states | One push of the cart |\n",
    "|| episode | One full simulation run | From the start of the game to game over |\n",
    "| v, V(s) | value | How much a state is worth | V(last state dropping the pole) = 0\n",
    "| r, R(s, a) | reward | Value gained or lost transitioning between states through an action | R(keeping the pole up) = 1 |\n",
    "| Œ≥ | gamma | How much to value a current state based on a future state | Coming up soon |\n",
    "| ùúã, ùúã(s) | policy |The recommended action to the agent based on the current state | œÄ(in trouble) = honesty |\n",
    "\n",
    "Bellman realized this: The value of our current state should the discounted value of the next state the agent will be in plus any rewards picked up along the way, given the agent takes the best action to maximize this.\n",
    "\n",
    "Using all the symbols from above, we get:\n",
    "\n",
    "<img src=\"images/bellman_equation.jpg\" width=\"260\" height=\"50\">\n",
    "\n",
    "However, this is assuming we know all the states, their corresponding actions, and their rewards. If we don't know this in advance, we can explore and simulate this equation with what is called the [Q equation](https://en.wikipedia.org/wiki/Q-learning):\n",
    "\n",
    "<img style=\"background-color:white;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637\">\n",
    "\n",
    "Here, the value function is replaced with the Q value, which is a function of a state and action. The learning rate is how much we want to change our old Q value with new information found during simulation. Visually, this results in a Q-table, where rows are the states, actions are the columns, and each cell is the value found through simulation.\n",
    "\n",
    "|| Meal | Snack | Wait |\n",
    "|-|-|-|-|\n",
    "| Hangry | 1 | .5 | -1 |\n",
    "| Hungry | .5 | 1 | 0 |\n",
    "| Full | -1 | -.5 | 1.5 |\n",
    "\n",
    "So this is cool and all, but how exactly does this fit in with CartPole? Here, MDPs are discrete states. CartPole has multidimensional states on a continuous scale. This is where neural networks save the day! Rather than categorize each state, we can feed state properties into our network. By having the same number of output nodes as possible actions, our network can be used to predict the value of the next state given the current state and action.\n",
    "\n",
    "## Building the Agent\n",
    "\n",
    "These networks can be configured with the same architectures and tools as other problems, such as CNNs. However, the one gotcha is that uses a specialized loss function. We'll instead be using the derivative of the Bellman Equation. Let's go ahead and define our model function as it is in trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:48:11.444283Z",
     "start_time": "2024-05-23T14:48:11.438865Z"
    }
   },
   "source": [
    "def deep_q_network(state_shape, action_size, learning_rate, hidden_neurons):\n",
    "    \"\"\"Creates a Deep Q Network to emulate Q-learning.\n",
    "\n",
    "    Creates a two hidden-layer Deep Q Network. Similar to a typical nueral\n",
    "    network, the loss function is altered to reduce the difference between\n",
    "    predicted Q-values and Target Q-values.\n",
    "    Args:\n",
    "        space_shape: a tuple of ints representing the observation space.\n",
    "        action_size (int): the number of possible actions.\n",
    "        learning_rate (float): the nueral network's learning rate.\n",
    "        hidden_neurons (int): the number of neurons to use per hidden\n",
    "            layer.\n",
    "    \"\"\"\n",
    "    state_input = layers.Input(state_shape, name='frames')\n",
    "    actions_input = layers.Input((action_size,), name='mask')\n",
    "\n",
    "    hidden_1 = layers.Dense(hidden_neurons, activation='relu')(state_input)\n",
    "    hidden_2 = layers.Dense(hidden_neurons, activation='relu')(hidden_1)\n",
    "    q_values = layers.Dense(action_size)(hidden_2)\n",
    "    masked_q_values = layers.Multiply()([q_values, actions_input])\n",
    "\n",
    "    model = models.Model(\n",
    "        inputs=[state_input, actions_input], outputs=masked_q_values)\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice any other atypical aspects of this network?\n",
    "\n",
    "Here, we take in both state and actions as inputs to our network. The states are fed in as normal, but the actions are used to \"mask\" the output. This is actually used for faster training, as we'd only want to update the nodes correspnding to the action that we simulated.\n",
    "\n",
    "The Bellman Equation actually isn't in the network. That's because this is only the \"brain\" of our agent. As an intelligence, it has much more! Before we get to how exactly the agent learns, let's looks at the other aspects of its body: \"Memory\" and \"Exploration\".\n",
    "\n",
    "Just like other neural network algorithms, we need data to train on. However, this data is the result of our simulations, not something previously stored in a table. Thus, we're going to give our agent a memory where we can store state - action - new state transitions to learn on.\n",
    "\n",
    "Each time the agent takes a step in gym, we'll save `(state, action, reward, state_prime, done)` to our buffer, which is defined like so."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:25:48.205585Z",
     "start_time": "2024-05-23T14:25:48.197628Z"
    }
   },
   "source": [
    "class Memory():\n",
    "    \"\"\"Sets up a memory replay buffer for a Deep Q Network.\n",
    "\n",
    "    A simple memory buffer for a DQN. This one randomly selects state\n",
    "    transitions with uniform probability, but research has gone into\n",
    "    other methods. For instance, a weight could be given to each memory\n",
    "    depending on how big of a difference there is between predicted Q values\n",
    "    and target Q values.\n",
    "\n",
    "    Args:\n",
    "        memory_size (int): How many elements to hold in the memory buffer.\n",
    "        batch_size (int): The number of elements to include in a replay batch.\n",
    "        gamma (float): The \"discount rate\" used to assess Q values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, memory_size, batch_size, gamma):\n",
    "        self.buffer = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Adds an experience into the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: a (state, action, reward, state_prime, done) tuple.\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Uniformally selects from the replay memory buffer.\n",
    "\n",
    "        Uniformally and randomly selects experiences to train the nueral\n",
    "        network on. Transposes the experiences to allow batch math on\n",
    "        the experience components.\n",
    "\n",
    "        Returns:\n",
    "            (list): A list of lists with structure [\n",
    "                [states], [actions], [rewards], [state_primes], [dones]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(\n",
    "            np.arange(buffer_size), size=self.batch_size, replace=False)\n",
    "\n",
    "        # Columns have different data types, so numpy array would be awkward.\n",
    "        batch = np.asarray([self.buffer[i] for i in index], dtype=np.object_).T.tolist()\n",
    "        states_mb = tf.convert_to_tensor(np.array(batch[0], dtype=np.float32))\n",
    "        actions_mb = np.array(batch[1], dtype=np.int8)\n",
    "        rewards_mb = np.array(batch[2], dtype=np.float32)\n",
    "        states_prime_mb = np.array(batch[3], dtype=np.float32)\n",
    "        dones_mb = batch[4]\n",
    "        return states_mb, actions_mb, rewards_mb, states_prime_mb, dones_mb"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a fake buffer and play around with it! We'll add the memory into our game play code to start collecting experiences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:25:49.599656Z",
     "start_time": "2024-05-23T14:25:49.594665Z"
    }
   },
   "source": [
    "test_memory_size = 20\n",
    "test_batch_size = 4\n",
    "test_gamma = .9  # Unused here. For learning.\n",
    "\n",
    "test_memory = Memory(test_memory_size, test_batch_size, test_gamma)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:25:51.020596Z",
     "start_time": "2024-05-23T14:25:51.008001Z"
    }
   },
   "source": [
    "actions = [x % 2 for x in range(200)]\n",
    "state = env.reset()\n",
    "step = 0\n",
    "episode_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done and step < len(actions):\n",
    "    action = actions[step]  # In the future, our agents will define this.\n",
    "    state_prime, reward, done1, done2, info = env.step(action)\n",
    "    done = done1 or done2\n",
    "    episode_reward += reward\n",
    "    test_memory.add((state, action, reward, state_prime, done))  # New line here\n",
    "    step += 1\n",
    "    state = state_prime\n",
    "    print_state(state, step, reward)\n",
    "\n",
    "end_statement = \"Game over!\" if done else \"Ran out of actions!\"\n",
    "print(end_statement, \"Score =\", episode_reward)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.016568   -0.20033568  0.0124312   0.28521073]\n",
      "Step 1 - Cart X: 0.017, Cart V: -0.200, Pole A: 0.012, Pole V:0.285, Reward:1.0\n",
      "[ 0.01256128 -0.00539321  0.01813542 -0.0035257 ]\n",
      "Step 2 - Cart X: 0.013, Cart V: -0.005, Pole A: 0.018, Pole V:-0.004, Reward:1.0\n",
      "[ 0.01245342 -0.2007705   0.0180649   0.29482356]\n",
      "Step 3 - Cart X: 0.012, Cart V: -0.201, Pole A: 0.018, Pole V:0.295, Reward:1.0\n",
      "[ 0.00843801 -0.00591069  0.02396138  0.00789232]\n",
      "Step 4 - Cart X: 0.008, Cart V: -0.006, Pole A: 0.024, Pole V:0.008, Reward:1.0\n",
      "[ 0.00831979 -0.20136793  0.02411922  0.308038  ]\n",
      "Step 5 - Cart X: 0.008, Cart V: -0.201, Pole A: 0.024, Pole V:0.308, Reward:1.0\n",
      "[ 0.00429244 -0.00659781  0.03027998  0.02305816]\n",
      "Step 6 - Cart X: 0.004, Cart V: -0.007, Pole A: 0.030, Pole V:0.023, Reward:1.0\n",
      "[ 0.00416048 -0.20214063  0.03074115  0.32513887]\n",
      "Step 7 - Cart X: 0.004, Cart V: -0.202, Pole A: 0.031, Pole V:0.325, Reward:1.0\n",
      "[ 0.00011767 -0.00746956  0.03724392  0.04230671]\n",
      "Step 8 - Cart X: 0.000, Cart V: -0.007, Pole A: 0.037, Pole V:0.042, Reward:1.0\n",
      "[-3.1724769e-05 -2.0310524e-01  3.8090058e-02  3.4650391e-01]\n",
      "Step 9 - Cart X: -0.000, Cart V: -0.203, Pole A: 0.038, Pole V:0.347, Reward:1.0\n",
      "[-0.00409383 -0.00854521  0.04502014  0.06607132]\n",
      "Step 10 - Cart X: -0.004, Cart V: -0.009, Pole A: 0.045, Pole V:0.066, Reward:1.0\n",
      "[-0.00426473 -0.20428275  0.04634156  0.37261158]\n",
      "Step 11 - Cart X: -0.004, Cart V: -0.204, Pole A: 0.046, Pole V:0.373, Reward:1.0\n",
      "[-0.00835039 -0.0098487   0.05379379  0.09489316]\n",
      "Step 12 - Cart X: -0.008, Cart V: -0.010, Pole A: 0.054, Pole V:0.095, Reward:1.0\n",
      "[-0.00854736 -0.20569874  0.05569166  0.40405104]\n",
      "Step 13 - Cart X: -0.009, Cart V: -0.206, Pole A: 0.056, Pole V:0.404, Reward:1.0\n",
      "[-0.01266134 -0.01140902  0.06377268  0.12943317]\n",
      "Step 14 - Cart X: -0.013, Cart V: -0.011, Pole A: 0.064, Pole V:0.129, Reward:1.0\n",
      "[-0.01288952 -0.20738381  0.06636134  0.44153425]\n",
      "Step 15 - Cart X: -0.013, Cart V: -0.207, Pole A: 0.066, Pole V:0.442, Reward:1.0\n",
      "[-0.01703719 -0.01326063  0.07519203  0.17048632]\n",
      "Step 16 - Cart X: -0.017, Cart V: -0.013, Pole A: 0.075, Pole V:0.170, Reward:1.0\n",
      "[-0.01730241 -0.20937377  0.07860176  0.48591048]\n",
      "Step 17 - Cart X: -0.017, Cart V: -0.209, Pole A: 0.079, Pole V:0.486, Reward:1.0\n",
      "[-0.02148988 -0.01544386  0.08831996  0.21899886]\n",
      "Step 18 - Cart X: -0.021, Cart V: -0.015, Pole A: 0.088, Pole V:0.219, Reward:1.0\n",
      "[-0.02179876 -0.21171     0.09269994  0.5381829 ]\n",
      "Step 19 - Cart X: -0.022, Cart V: -0.212, Pole A: 0.093, Pole V:0.538, Reward:1.0\n",
      "[-0.02603296 -0.0180052   0.1034636   0.27608803]\n",
      "Step 20 - Cart X: -0.026, Cart V: -0.018, Pole A: 0.103, Pole V:0.276, Reward:1.0\n",
      "[-0.02639306 -0.2144394   0.10898536  0.5995277 ]\n",
      "Step 21 - Cart X: -0.026, Cart V: -0.214, Pole A: 0.109, Pole V:0.600, Reward:1.0\n",
      "[-0.03068185 -0.02099744  0.12097591  0.3430646 ]\n",
      "Step 22 - Cart X: -0.031, Cart V: -0.021, Pole A: 0.121, Pole V:0.343, Reward:1.0\n",
      "[-0.0311018  -0.2176141   0.12783721  0.6713143 ]\n",
      "Step 23 - Cart X: -0.031, Cart V: -0.218, Pole A: 0.128, Pole V:0.671, Reward:1.0\n",
      "[-0.03545408 -0.02447925  0.14126348  0.4214579 ]\n",
      "Step 24 - Cart X: -0.035, Cart V: -0.024, Pole A: 0.141, Pole V:0.421, Reward:1.0\n",
      "[-0.03594367 -0.22129041  0.14969265  0.75512743]\n",
      "Step 25 - Cart X: -0.036, Cart V: -0.221, Pole A: 0.150, Pole V:0.755, Reward:1.0\n",
      "[-0.04036948 -0.02851411  0.16479519  0.51304215]\n",
      "Step 26 - Cart X: -0.040, Cart V: -0.029, Pole A: 0.165, Pole V:0.513, Reward:1.0\n",
      "[-0.04093976 -0.22552682  0.17505604  0.8527883 ]\n",
      "Step 27 - Cart X: -0.041, Cart V: -0.226, Pole A: 0.175, Pole V:0.853, Reward:1.0\n",
      "[-0.04545029 -0.03316782  0.1921118   0.6198636 ]\n",
      "Step 28 - Cart X: -0.045, Cart V: -0.033, Pole A: 0.192, Pole V:0.620, Reward:1.0\n",
      "[-0.04611365 -0.23038001  0.20450908  0.9663739 ]\n",
      "Step 29 - Cart X: -0.046, Cart V: -0.230, Pole A: 0.205, Pole V:0.966, Reward:1.0\n",
      "[-0.05072125 -0.03850395  0.22383656  0.744265  ]\n",
      "Step 30 - Cart X: -0.051, Cart V: -0.039, Pole A: 0.224, Pole V:0.744, Reward:1.0\n",
      "Game over! Score = 30.0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sample the memory by running the cell below multiple times. It's different each call, and that's on purpose. Just like with other neural networks, it's important to randomly sample so that our agent can learn from many different situations.\n",
    "\n",
    "The use of a memory buffer is called [Experience Replay](https://arxiv.org/pdf/1511.05952.pdf). The above technique of a uniform random sample is a quick and computationally efficient way to get the job done, but RL researchers often look into other sampling methods. For instance, maybe there's a way to weight memories based on their rarity or loss when the agent learns with it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T14:26:27.417013Z",
     "start_time": "2024-05-23T14:26:27.408796Z"
    }
   },
   "source": [
    "test_memory.sample()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[-0.04611365, -0.23038001,  0.20450908,  0.9663739 ],\n",
       "        [-0.02148988, -0.01544386,  0.08831996,  0.21899886],\n",
       "        [-0.03068185, -0.02099744,  0.12097591,  0.3430646 ],\n",
       "        [-0.01730241, -0.20937377,  0.07860176,  0.48591048]],\n",
       "       dtype=float32)>,\n",
       " array([1, 0, 0, 1], dtype=int8),\n",
       " array([1., 1., 1., 1.], dtype=float32),\n",
       " array([[-0.05072125, -0.03850395,  0.22383656,  0.744265  ],\n",
       "        [-0.02179876, -0.21171   ,  0.09269994,  0.5381829 ],\n",
       "        [-0.0311018 , -0.2176141 ,  0.12783721,  0.6713143 ],\n",
       "        [-0.02148988, -0.01544386,  0.08831996,  0.21899886]],\n",
       "       dtype=float32),\n",
       " [True, False, False, False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before the agent has any memories and has learned anything, how is it supposed to act? That comes down to [Exploration vs Exploitation](https://en.wikipedia.org/wiki/Multi-armed_bandit). The trouble is that in order to learn, risks with the unknown need to be made. There's no right answer, but there is a popular answer. We'll start by acting randomly, and over time, we will slowly decay our chance to act randomly.\n",
    "\n",
    "Below is a partial version of the agent. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:17:47.247535Z",
     "start_time": "2024-05-23T16:17:47.240493Z"
    }
   },
   "source": [
    "class Partial_Agent():\n",
    "    \"\"\"Sets up a reinforcement learning agent to play in a game environment.\"\"\"\n",
    "\n",
    "    def __init__(self, network, memory, epsilon_decay, action_size):\n",
    "        \"\"\"Initializes the agent with DQN and memory sub-classes.\n",
    "\n",
    "        Args:\n",
    "            network: A neural network created from deep_q_network().\n",
    "            memory: A Memory class object.\n",
    "            epsilon_decay (float): The rate at which to decay random actions.\n",
    "            action_size (int): The number of possible actions to take.\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.action_size = action_size\n",
    "        self.memory = memory\n",
    "        self.epsilon = 1  # The chance to take a random action.\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def act(self, state, training=False):\n",
    "        \"\"\"Selects an action for the agent to take given a game state.\n",
    "\n",
    "        Args:\n",
    "            state (list of numbers): The state of the environment to act on.\n",
    "            traning (bool): True if the agent is training.\n",
    "\n",
    "        Returns:\n",
    "            (int) The index of the action to take.\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            # Random actions until enough simulations to train the model.\n",
    "            if len(self.memory.buffer) >= self.memory.batch_size:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "            if self.epsilon > np.random.rand():\n",
    "                # print(\"Exploration!\")\n",
    "                return random.randint(0, self.action_size - 1)\n",
    "\n",
    "        # If not acting randomly, take action with highest predicted value.\n",
    "        # print(\"Exploitation!\", state)\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        predict_mask = np.ones((1, self.action_size,))\n",
    "        action_qs = self.network.predict([state_batch, predict_mask], verbose=0)\n",
    "        return np.argmax(action_qs[0])"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the agent and get a starting state to see how it would act without any training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:17:48.878533Z",
     "start_time": "2024-05-23T16:17:48.831119Z"
    }
   },
   "source": [
    "state = env.reset()[0]\n",
    "\n",
    "# Define \"brain\"\n",
    "space_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Feel free to play with these\n",
    "test_learning_rate = .2\n",
    "test_hidden_neurons = 10\n",
    "test_epsilon_decay = .95\n",
    "\n",
    "test_network = deep_q_network(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)\n",
    "\n",
    "test_agent = Partial_Agent(\n",
    "    test_network, test_memory, test_epsilon_decay, action_size)"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below multiple times. Since we're decaying the random action rate after every action, it's only a matter a time before the agent exploits more than it explores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:17:49.806576Z",
     "start_time": "2024-05-23T16:17:49.801294Z"
    }
   },
   "source": [
    "action = test_agent.act(state, training=True)\n",
    "print(\"Push Right\" if action else \"Push Left\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push Left\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memories, a brain, and a healthy dose of curiosity. We finally have all the ingredient for our agent to learn. After all, as the Scarecrow from the Wizard of Oz said:\n",
    "\n",
    "\"Everything in life is unusual until you get accustomed to it.\"  \n",
    "~L. Frank Baum\n",
    "\n",
    "Below is the code used by our agent to learn, where the Bellman Equation at last makes an appearance. We'll run through the following steps.\n",
    "\n",
    "1. Pull a batch from memory\n",
    "2. Get the Q value (the output of the neural network) based on the memory's ending state\n",
    "    - Assume the Q value of the action with the highest Q value (test all actions)\n",
    "4. Update these Q values with the Bellman Equation\n",
    "    - `target_qs = (next_q_mb * self.memory.gamma) + reward_mb`\n",
    "    - If the state is the end of the game, set the target_q to the reward for entering the final state.\n",
    "5. Reshape the target_qs to match the networks output\n",
    "    - Only learn on the memory's corresponding action by setting all action nodes to zero besides the action node taken.\n",
    "6. Fit Target Qs as the label to our model against the memory's starting state and action as the inputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:21:50.710461Z",
     "start_time": "2024-05-23T16:21:50.703478Z"
    }
   },
   "source": [
    "def learn(self):\n",
    "    \"\"\"Trains the Deep Q Network based on stored experiences.\"\"\"\n",
    "    batch_size = self.memory.batch_size\n",
    "    if len(self.memory.buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # Obtain random mini-batch from memory.\n",
    "    state_mb, action_mb, reward_mb, next_state_mb, done_mb = (\n",
    "        self.memory.sample())\n",
    "\n",
    "    # Get Q values for next_state.\n",
    "    predict_mask = np.ones(action_mb.shape + (self.action_size,))\n",
    "    next_q_mb = self.network.predict([next_state_mb, predict_mask], verbose=0)\n",
    "    next_q_mb = tf.math.reduce_max(next_q_mb, axis=1)\n",
    "\n",
    "    # Apply the Bellman Equation\n",
    "    target_qs = (next_q_mb * self.memory.gamma) + reward_mb\n",
    "    target_qs = tf.where(done_mb, reward_mb, target_qs)\n",
    "\n",
    "    # Match training batch to network output:\n",
    "    # target_q where action taken, 0 otherwise.\n",
    "    action_mb = tf.convert_to_tensor(action_mb, dtype=tf.int32)\n",
    "    action_hot = tf.one_hot(action_mb, self.action_size)\n",
    "    target_mask = tf.multiply(tf.expand_dims(target_qs, -1), action_hot)\n",
    "\n",
    "    return self.network.train_on_batch(\n",
    "        [state_mb, action_hot], target_mask)\n",
    "\n",
    "\n",
    "Partial_Agent.learn = learn\n",
    "test_agent = Partial_Agent(\n",
    "    test_network, test_memory, test_epsilon_decay, action_size)"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We finally have an intelligence that can walk and talk and... well ok, this intelligence is too simple to be able to do those things, but maybe it can learn to push a cart with a pole on it. Let's update our training loop to use our new agent.\n",
    "\n",
    "Run the below cell over and over up to ten times to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:19:28.386503Z",
     "start_time": "2024-05-23T17:16:20.165487Z"
    }
   },
   "source": [
    "test_learning_rate = 0.0005\n",
    "test_hidden_neurons = 128\n",
    "test_epsilon_decay = .95\n",
    "test_memory_size = 102400\n",
    "test_batch_size = 64\n",
    "test_gamma = .5\n",
    "\n",
    "test_network = deep_q_network(\n",
    "    space_shape, action_size, test_learning_rate, test_hidden_neurons)\n",
    "test_memory = Memory(test_memory_size, test_batch_size, test_gamma)\n",
    "\n",
    "test_agent = Partial_Agent(\n",
    "    test_network, test_memory, test_epsilon_decay, action_size)\n",
    "total_step = 0\n",
    "for episode in range(300):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = test_agent.act(state, training=True)\n",
    "        state_prime, reward, done1, done2, info = env.step(action)\n",
    "        done = done1 or done2\n",
    "        episode_reward += reward\n",
    "        test_agent.memory.add((state, action, reward, state_prime, done))  # New line here\n",
    "        step += 1\n",
    "        state = state_prime\n",
    "    total_step+=step\n",
    "    print(\"Train episode:\", episode, \" loss:\", test_agent.learn())\n",
    "    print(\"Episode:\", episode, \"Game over! Score =\", episode_reward)\n",
    "    print(\"total_step:\", total_step)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train episode: 0  loss: None\n",
      "Episode: 0 Game over! Score = 21.0\n",
      "total_step: 21\n",
      "Train episode: 1  loss: None\n",
      "Episode: 1 Game over! Score = 13.0\n",
      "total_step: 34\n",
      "Train episode: 2  loss: None\n",
      "Episode: 2 Game over! Score = 23.0\n",
      "total_step: 57\n",
      "Train episode: 3  loss: 0.52035046\n",
      "Episode: 3 Game over! Score = 16.0\n",
      "total_step: 73\n",
      "Train episode: 4  loss: 0.5026705\n",
      "Episode: 4 Game over! Score = 16.0\n",
      "total_step: 89\n",
      "Train episode: 5  loss: 0.48969746\n",
      "Episode: 5 Game over! Score = 9.0\n",
      "total_step: 98\n",
      "Train episode: 6  loss: 0.47305655\n",
      "Episode: 6 Game over! Score = 8.0\n",
      "total_step: 106\n",
      "Train episode: 7  loss: 0.4610901\n",
      "Episode: 7 Game over! Score = 9.0\n",
      "total_step: 115\n",
      "Train episode: 8  loss: 0.44685975\n",
      "Episode: 8 Game over! Score = 11.0\n",
      "total_step: 126\n",
      "Train episode: 9  loss: 0.43617252\n",
      "Episode: 9 Game over! Score = 10.0\n",
      "total_step: 136\n",
      "Train episode: 10  loss: 0.4286897\n",
      "Episode: 10 Game over! Score = 9.0\n",
      "total_step: 145\n",
      "Train episode: 11  loss: 0.42176694\n",
      "Episode: 11 Game over! Score = 8.0\n",
      "total_step: 153\n",
      "Train episode: 12  loss: 0.41357908\n",
      "Episode: 12 Game over! Score = 10.0\n",
      "total_step: 163\n",
      "Train episode: 13  loss: 0.40824205\n",
      "Episode: 13 Game over! Score = 10.0\n",
      "total_step: 173\n",
      "Train episode: 14  loss: 0.40020487\n",
      "Episode: 14 Game over! Score = 9.0\n",
      "total_step: 182\n",
      "Train episode: 15  loss: 0.39479896\n",
      "Episode: 15 Game over! Score = 10.0\n",
      "total_step: 192\n",
      "Train episode: 16  loss: 0.38861418\n",
      "Episode: 16 Game over! Score = 9.0\n",
      "total_step: 201\n",
      "Train episode: 17  loss: 0.3835146\n",
      "Episode: 17 Game over! Score = 8.0\n",
      "total_step: 209\n",
      "Train episode: 18  loss: 0.3796953\n",
      "Episode: 18 Game over! Score = 9.0\n",
      "total_step: 218\n",
      "Train episode: 19  loss: 0.37457028\n",
      "Episode: 19 Game over! Score = 9.0\n",
      "total_step: 227\n",
      "Train episode: 20  loss: 0.37029114\n",
      "Episode: 20 Game over! Score = 10.0\n",
      "total_step: 237\n",
      "Train episode: 21  loss: 0.36603284\n",
      "Episode: 21 Game over! Score = 9.0\n",
      "total_step: 246\n",
      "Train episode: 22  loss: 0.3626218\n",
      "Episode: 22 Game over! Score = 9.0\n",
      "total_step: 255\n",
      "Train episode: 23  loss: 0.35956442\n",
      "Episode: 23 Game over! Score = 8.0\n",
      "total_step: 263\n",
      "Train episode: 24  loss: 0.35547045\n",
      "Episode: 24 Game over! Score = 10.0\n",
      "total_step: 273\n",
      "Train episode: 25  loss: 0.351475\n",
      "Episode: 25 Game over! Score = 9.0\n",
      "total_step: 282\n",
      "Train episode: 26  loss: 0.34932718\n",
      "Episode: 26 Game over! Score = 10.0\n",
      "total_step: 292\n",
      "Train episode: 27  loss: 0.34663755\n",
      "Episode: 27 Game over! Score = 10.0\n",
      "total_step: 302\n",
      "Train episode: 28  loss: 0.345884\n",
      "Episode: 28 Game over! Score = 10.0\n",
      "total_step: 312\n",
      "Train episode: 29  loss: 0.3439646\n",
      "Episode: 29 Game over! Score = 8.0\n",
      "total_step: 320\n",
      "Train episode: 30  loss: 0.34209663\n",
      "Episode: 30 Game over! Score = 10.0\n",
      "total_step: 330\n",
      "Train episode: 31  loss: 0.3387221\n",
      "Episode: 31 Game over! Score = 11.0\n",
      "total_step: 341\n",
      "Train episode: 32  loss: 0.3338783\n",
      "Episode: 32 Game over! Score = 9.0\n",
      "total_step: 350\n",
      "Train episode: 33  loss: 0.3342535\n",
      "Episode: 33 Game over! Score = 9.0\n",
      "total_step: 359\n",
      "Train episode: 34  loss: 0.33240464\n",
      "Episode: 34 Game over! Score = 9.0\n",
      "total_step: 368\n",
      "Train episode: 35  loss: 0.33111462\n",
      "Episode: 35 Game over! Score = 8.0\n",
      "total_step: 376\n",
      "Train episode: 36  loss: 0.32995304\n",
      "Episode: 36 Game over! Score = 10.0\n",
      "total_step: 386\n",
      "Train episode: 37  loss: 0.32618567\n",
      "Episode: 37 Game over! Score = 9.0\n",
      "total_step: 395\n",
      "Train episode: 38  loss: 0.32566136\n",
      "Episode: 38 Game over! Score = 10.0\n",
      "total_step: 405\n",
      "Train episode: 39  loss: 0.32324132\n",
      "Episode: 39 Game over! Score = 9.0\n",
      "total_step: 414\n",
      "Train episode: 40  loss: 0.3212947\n",
      "Episode: 40 Game over! Score = 9.0\n",
      "total_step: 423\n",
      "Train episode: 41  loss: 0.31962806\n",
      "Episode: 41 Game over! Score = 10.0\n",
      "total_step: 433\n",
      "Train episode: 42  loss: 0.31785947\n",
      "Episode: 42 Game over! Score = 9.0\n",
      "total_step: 442\n",
      "Train episode: 43  loss: 0.3169546\n",
      "Episode: 43 Game over! Score = 10.0\n",
      "total_step: 452\n",
      "Train episode: 44  loss: 0.31508097\n",
      "Episode: 44 Game over! Score = 8.0\n",
      "total_step: 460\n",
      "Train episode: 45  loss: 0.31267962\n",
      "Episode: 45 Game over! Score = 8.0\n",
      "total_step: 468\n",
      "Train episode: 46  loss: 0.31173012\n",
      "Episode: 46 Game over! Score = 10.0\n",
      "total_step: 478\n",
      "Train episode: 47  loss: 0.30953202\n",
      "Episode: 47 Game over! Score = 8.0\n",
      "total_step: 486\n",
      "Train episode: 48  loss: 0.3072201\n",
      "Episode: 48 Game over! Score = 10.0\n",
      "total_step: 496\n",
      "Train episode: 49  loss: 0.30689156\n",
      "Episode: 49 Game over! Score = 8.0\n",
      "total_step: 504\n",
      "Train episode: 50  loss: 0.30551678\n",
      "Episode: 50 Game over! Score = 9.0\n",
      "total_step: 513\n",
      "Train episode: 51  loss: 0.3033074\n",
      "Episode: 51 Game over! Score = 8.0\n",
      "total_step: 521\n",
      "Train episode: 52  loss: 0.3012165\n",
      "Episode: 52 Game over! Score = 10.0\n",
      "total_step: 531\n",
      "Train episode: 53  loss: 0.29877245\n",
      "Episode: 53 Game over! Score = 8.0\n",
      "total_step: 539\n",
      "Train episode: 54  loss: 0.2970509\n",
      "Episode: 54 Game over! Score = 10.0\n",
      "total_step: 549\n",
      "Train episode: 55  loss: 0.29617533\n",
      "Episode: 55 Game over! Score = 10.0\n",
      "total_step: 559\n",
      "Train episode: 56  loss: 0.29430225\n",
      "Episode: 56 Game over! Score = 9.0\n",
      "total_step: 568\n",
      "Train episode: 57  loss: 0.29180738\n",
      "Episode: 57 Game over! Score = 10.0\n",
      "total_step: 578\n",
      "Train episode: 58  loss: 0.29065222\n",
      "Episode: 58 Game over! Score = 9.0\n",
      "total_step: 587\n",
      "Train episode: 59  loss: 0.2881133\n",
      "Episode: 59 Game over! Score = 9.0\n",
      "total_step: 596\n",
      "Train episode: 60  loss: 0.2866338\n",
      "Episode: 60 Game over! Score = 9.0\n",
      "total_step: 605\n",
      "Train episode: 61  loss: 0.28513378\n",
      "Episode: 61 Game over! Score = 9.0\n",
      "total_step: 614\n",
      "Train episode: 62  loss: 0.2833376\n",
      "Episode: 62 Game over! Score = 10.0\n",
      "total_step: 624\n",
      "Train episode: 63  loss: 0.28234303\n",
      "Episode: 63 Game over! Score = 8.0\n",
      "total_step: 632\n",
      "Train episode: 64  loss: 0.28077233\n",
      "Episode: 64 Game over! Score = 9.0\n",
      "total_step: 641\n",
      "Train episode: 65  loss: 0.27962837\n",
      "Episode: 65 Game over! Score = 9.0\n",
      "total_step: 650\n",
      "Train episode: 66  loss: 0.2773375\n",
      "Episode: 66 Game over! Score = 8.0\n",
      "total_step: 658\n",
      "Train episode: 67  loss: 0.27628234\n",
      "Episode: 67 Game over! Score = 9.0\n",
      "total_step: 667\n",
      "Train episode: 68  loss: 0.2746038\n",
      "Episode: 68 Game over! Score = 9.0\n",
      "total_step: 676\n",
      "Train episode: 69  loss: 0.27272412\n",
      "Episode: 69 Game over! Score = 10.0\n",
      "total_step: 686\n",
      "Train episode: 70  loss: 0.27222794\n",
      "Episode: 70 Game over! Score = 8.0\n",
      "total_step: 694\n",
      "Train episode: 71  loss: 0.26998544\n",
      "Episode: 71 Game over! Score = 9.0\n",
      "total_step: 703\n",
      "Train episode: 72  loss: 0.2685926\n",
      "Episode: 72 Game over! Score = 10.0\n",
      "total_step: 713\n",
      "Train episode: 73  loss: 0.26627398\n",
      "Episode: 73 Game over! Score = 10.0\n",
      "total_step: 723\n",
      "Train episode: 74  loss: 0.2652423\n",
      "Episode: 74 Game over! Score = 9.0\n",
      "total_step: 732\n",
      "Train episode: 75  loss: 0.26371548\n",
      "Episode: 75 Game over! Score = 10.0\n",
      "total_step: 742\n",
      "Train episode: 76  loss: 0.26281753\n",
      "Episode: 76 Game over! Score = 9.0\n",
      "total_step: 751\n",
      "Train episode: 77  loss: 0.2613441\n",
      "Episode: 77 Game over! Score = 9.0\n",
      "total_step: 760\n",
      "Train episode: 78  loss: 0.260025\n",
      "Episode: 78 Game over! Score = 10.0\n",
      "total_step: 770\n",
      "Train episode: 79  loss: 0.25837532\n",
      "Episode: 79 Game over! Score = 9.0\n",
      "total_step: 779\n",
      "Train episode: 80  loss: 0.25678265\n",
      "Episode: 80 Game over! Score = 9.0\n",
      "total_step: 788\n",
      "Train episode: 81  loss: 0.25504193\n",
      "Episode: 81 Game over! Score = 9.0\n",
      "total_step: 797\n",
      "Train episode: 82  loss: 0.25330085\n",
      "Episode: 82 Game over! Score = 10.0\n",
      "total_step: 807\n",
      "Train episode: 83  loss: 0.25248566\n",
      "Episode: 83 Game over! Score = 8.0\n",
      "total_step: 815\n",
      "Train episode: 84  loss: 0.25125033\n",
      "Episode: 84 Game over! Score = 10.0\n",
      "total_step: 825\n",
      "Train episode: 85  loss: 0.24960043\n",
      "Episode: 85 Game over! Score = 11.0\n",
      "total_step: 836\n",
      "Train episode: 86  loss: 0.24801025\n",
      "Episode: 86 Game over! Score = 10.0\n",
      "total_step: 846\n",
      "Train episode: 87  loss: 0.24630313\n",
      "Episode: 87 Game over! Score = 10.0\n",
      "total_step: 856\n",
      "Train episode: 88  loss: 0.24564551\n",
      "Episode: 88 Game over! Score = 8.0\n",
      "total_step: 864\n",
      "Train episode: 89  loss: 0.24383828\n",
      "Episode: 89 Game over! Score = 9.0\n",
      "total_step: 873\n",
      "Train episode: 90  loss: 0.24219471\n",
      "Episode: 90 Game over! Score = 8.0\n",
      "total_step: 881\n",
      "Train episode: 91  loss: 0.24048585\n",
      "Episode: 91 Game over! Score = 9.0\n",
      "total_step: 890\n",
      "Train episode: 92  loss: 0.23925996\n",
      "Episode: 92 Game over! Score = 9.0\n",
      "total_step: 899\n",
      "Train episode: 93  loss: 0.23774336\n",
      "Episode: 93 Game over! Score = 9.0\n",
      "total_step: 908\n",
      "Train episode: 94  loss: 0.23593801\n",
      "Episode: 94 Game over! Score = 11.0\n",
      "total_step: 919\n",
      "Train episode: 95  loss: 0.23428294\n",
      "Episode: 95 Game over! Score = 9.0\n",
      "total_step: 928\n",
      "Train episode: 96  loss: 0.23255034\n",
      "Episode: 96 Game over! Score = 8.0\n",
      "total_step: 936\n",
      "Train episode: 97  loss: 0.23148595\n",
      "Episode: 97 Game over! Score = 10.0\n",
      "total_step: 946\n",
      "Train episode: 98  loss: 0.22992043\n",
      "Episode: 98 Game over! Score = 10.0\n",
      "total_step: 956\n",
      "Train episode: 99  loss: 0.22808449\n",
      "Episode: 99 Game over! Score = 11.0\n",
      "total_step: 967\n",
      "Train episode: 100  loss: 0.2264212\n",
      "Episode: 100 Game over! Score = 9.0\n",
      "total_step: 976\n",
      "Train episode: 101  loss: 0.22486968\n",
      "Episode: 101 Game over! Score = 9.0\n",
      "total_step: 985\n",
      "Train episode: 102  loss: 0.22430421\n",
      "Episode: 102 Game over! Score = 9.0\n",
      "total_step: 994\n",
      "Train episode: 103  loss: 0.22266833\n",
      "Episode: 103 Game over! Score = 9.0\n",
      "total_step: 1003\n",
      "Train episode: 104  loss: 0.22145662\n",
      "Episode: 104 Game over! Score = 10.0\n",
      "total_step: 1013\n",
      "Train episode: 105  loss: 0.21981382\n",
      "Episode: 105 Game over! Score = 10.0\n",
      "total_step: 1023\n",
      "Train episode: 106  loss: 0.21827625\n",
      "Episode: 106 Game over! Score = 10.0\n",
      "total_step: 1033\n",
      "Train episode: 107  loss: 0.21661651\n",
      "Episode: 107 Game over! Score = 9.0\n",
      "total_step: 1042\n",
      "Train episode: 108  loss: 0.21545807\n",
      "Episode: 108 Game over! Score = 9.0\n",
      "total_step: 1051\n",
      "Train episode: 109  loss: 0.21387106\n",
      "Episode: 109 Game over! Score = 10.0\n",
      "total_step: 1061\n",
      "Train episode: 110  loss: 0.21265614\n",
      "Episode: 110 Game over! Score = 8.0\n",
      "total_step: 1069\n",
      "Train episode: 111  loss: 0.21135451\n",
      "Episode: 111 Game over! Score = 10.0\n",
      "total_step: 1079\n",
      "Train episode: 112  loss: 0.20969059\n",
      "Episode: 112 Game over! Score = 8.0\n",
      "total_step: 1087\n",
      "Train episode: 113  loss: 0.20829928\n",
      "Episode: 113 Game over! Score = 10.0\n",
      "total_step: 1097\n",
      "Train episode: 114  loss: 0.20686796\n",
      "Episode: 114 Game over! Score = 10.0\n",
      "total_step: 1107\n",
      "Train episode: 115  loss: 0.20546168\n",
      "Episode: 115 Game over! Score = 9.0\n",
      "total_step: 1116\n",
      "Train episode: 116  loss: 0.2039804\n",
      "Episode: 116 Game over! Score = 10.0\n",
      "total_step: 1126\n",
      "Train episode: 117  loss: 0.20278941\n",
      "Episode: 117 Game over! Score = 10.0\n",
      "total_step: 1136\n",
      "Train episode: 118  loss: 0.20171276\n",
      "Episode: 118 Game over! Score = 8.0\n",
      "total_step: 1144\n",
      "Train episode: 119  loss: 0.20031826\n",
      "Episode: 119 Game over! Score = 10.0\n",
      "total_step: 1154\n",
      "Train episode: 120  loss: 0.19893043\n",
      "Episode: 120 Game over! Score = 10.0\n",
      "total_step: 1164\n",
      "Train episode: 121  loss: 0.197483\n",
      "Episode: 121 Game over! Score = 8.0\n",
      "total_step: 1172\n",
      "Train episode: 122  loss: 0.19620325\n",
      "Episode: 122 Game over! Score = 9.0\n",
      "total_step: 1181\n",
      "Train episode: 123  loss: 0.1948005\n",
      "Episode: 123 Game over! Score = 10.0\n",
      "total_step: 1191\n",
      "Train episode: 124  loss: 0.19365537\n",
      "Episode: 124 Game over! Score = 10.0\n",
      "total_step: 1201\n",
      "Train episode: 125  loss: 0.19237034\n",
      "Episode: 125 Game over! Score = 9.0\n",
      "total_step: 1210\n",
      "Train episode: 126  loss: 0.19126976\n",
      "Episode: 126 Game over! Score = 9.0\n",
      "total_step: 1219\n",
      "Train episode: 127  loss: 0.19038051\n",
      "Episode: 127 Game over! Score = 11.0\n",
      "total_step: 1230\n",
      "Train episode: 128  loss: 0.18916231\n",
      "Episode: 128 Game over! Score = 10.0\n",
      "total_step: 1240\n",
      "Train episode: 129  loss: 0.18790182\n",
      "Episode: 129 Game over! Score = 9.0\n",
      "total_step: 1249\n",
      "Train episode: 130  loss: 0.18666825\n",
      "Episode: 130 Game over! Score = 9.0\n",
      "total_step: 1258\n",
      "Train episode: 131  loss: 0.18539326\n",
      "Episode: 131 Game over! Score = 10.0\n",
      "total_step: 1268\n",
      "Train episode: 132  loss: 0.18411723\n",
      "Episode: 132 Game over! Score = 10.0\n",
      "total_step: 1278\n",
      "Train episode: 133  loss: 0.1830035\n",
      "Episode: 133 Game over! Score = 11.0\n",
      "total_step: 1289\n",
      "Train episode: 134  loss: 0.18180175\n",
      "Episode: 134 Game over! Score = 11.0\n",
      "total_step: 1300\n",
      "Train episode: 135  loss: 0.18061244\n",
      "Episode: 135 Game over! Score = 9.0\n",
      "total_step: 1309\n",
      "Train episode: 136  loss: 0.17946206\n",
      "Episode: 136 Game over! Score = 10.0\n",
      "total_step: 1319\n",
      "Train episode: 137  loss: 0.17831598\n",
      "Episode: 137 Game over! Score = 9.0\n",
      "total_step: 1328\n",
      "Train episode: 138  loss: 0.17721108\n",
      "Episode: 138 Game over! Score = 8.0\n",
      "total_step: 1336\n",
      "Train episode: 139  loss: 0.17608263\n",
      "Episode: 139 Game over! Score = 9.0\n",
      "total_step: 1345\n",
      "Train episode: 140  loss: 0.17511581\n",
      "Episode: 140 Game over! Score = 8.0\n",
      "total_step: 1353\n",
      "Train episode: 141  loss: 0.1740109\n",
      "Episode: 141 Game over! Score = 9.0\n",
      "total_step: 1362\n",
      "Train episode: 142  loss: 0.1731181\n",
      "Episode: 142 Game over! Score = 10.0\n",
      "total_step: 1372\n",
      "Train episode: 143  loss: 0.17217682\n",
      "Episode: 143 Game over! Score = 9.0\n",
      "total_step: 1381\n",
      "Train episode: 144  loss: 0.17128064\n",
      "Episode: 144 Game over! Score = 8.0\n",
      "total_step: 1389\n",
      "Train episode: 145  loss: 0.17028925\n",
      "Episode: 145 Game over! Score = 11.0\n",
      "total_step: 1400\n",
      "Train episode: 146  loss: 0.16923398\n",
      "Episode: 146 Game over! Score = 11.0\n",
      "total_step: 1411\n",
      "Train episode: 147  loss: 0.16823295\n",
      "Episode: 147 Game over! Score = 10.0\n",
      "total_step: 1421\n",
      "Train episode: 148  loss: 0.16736358\n",
      "Episode: 148 Game over! Score = 10.0\n",
      "total_step: 1431\n",
      "Train episode: 149  loss: 0.1664627\n",
      "Episode: 149 Game over! Score = 9.0\n",
      "total_step: 1440\n",
      "Train episode: 150  loss: 0.16560712\n",
      "Episode: 150 Game over! Score = 8.0\n",
      "total_step: 1448\n",
      "Train episode: 151  loss: 0.16469069\n",
      "Episode: 151 Game over! Score = 9.0\n",
      "total_step: 1457\n",
      "Train episode: 152  loss: 0.16370618\n",
      "Episode: 152 Game over! Score = 11.0\n",
      "total_step: 1468\n",
      "Train episode: 153  loss: 0.1627757\n",
      "Episode: 153 Game over! Score = 9.0\n",
      "total_step: 1477\n",
      "Train episode: 154  loss: 0.16179785\n",
      "Episode: 154 Game over! Score = 9.0\n",
      "total_step: 1486\n",
      "Train episode: 155  loss: 0.1608625\n",
      "Episode: 155 Game over! Score = 9.0\n",
      "total_step: 1495\n",
      "Train episode: 156  loss: 0.15996666\n",
      "Episode: 156 Game over! Score = 9.0\n",
      "total_step: 1504\n",
      "Train episode: 157  loss: 0.15903364\n",
      "Episode: 157 Game over! Score = 8.0\n",
      "total_step: 1512\n",
      "Train episode: 158  loss: 0.15812455\n",
      "Episode: 158 Game over! Score = 10.0\n",
      "total_step: 1522\n",
      "Train episode: 159  loss: 0.15723607\n",
      "Episode: 159 Game over! Score = 8.0\n",
      "total_step: 1530\n",
      "Train episode: 160  loss: 0.1563653\n",
      "Episode: 160 Game over! Score = 10.0\n",
      "total_step: 1540\n",
      "Train episode: 161  loss: 0.15546389\n",
      "Episode: 161 Game over! Score = 10.0\n",
      "total_step: 1550\n",
      "Train episode: 162  loss: 0.15460409\n",
      "Episode: 162 Game over! Score = 10.0\n",
      "total_step: 1560\n",
      "Train episode: 163  loss: 0.15371984\n",
      "Episode: 163 Game over! Score = 10.0\n",
      "total_step: 1570\n",
      "Train episode: 164  loss: 0.15301241\n",
      "Episode: 164 Game over! Score = 10.0\n",
      "total_step: 1580\n",
      "Train episode: 165  loss: 0.15213512\n",
      "Episode: 165 Game over! Score = 9.0\n",
      "total_step: 1589\n",
      "Train episode: 166  loss: 0.15129134\n",
      "Episode: 166 Game over! Score = 10.0\n",
      "total_step: 1599\n",
      "Train episode: 167  loss: 0.15041874\n",
      "Episode: 167 Game over! Score = 10.0\n",
      "total_step: 1609\n",
      "Train episode: 168  loss: 0.14967917\n",
      "Episode: 168 Game over! Score = 8.0\n",
      "total_step: 1617\n",
      "Train episode: 169  loss: 0.14891335\n",
      "Episode: 169 Game over! Score = 10.0\n",
      "total_step: 1627\n",
      "Train episode: 170  loss: 0.14814012\n",
      "Episode: 170 Game over! Score = 9.0\n",
      "total_step: 1636\n",
      "Train episode: 171  loss: 0.14740771\n",
      "Episode: 171 Game over! Score = 10.0\n",
      "total_step: 1646\n",
      "Train episode: 172  loss: 0.14666913\n",
      "Episode: 172 Game over! Score = 10.0\n",
      "total_step: 1656\n",
      "Train episode: 173  loss: 0.14587541\n",
      "Episode: 173 Game over! Score = 9.0\n",
      "total_step: 1665\n",
      "Train episode: 174  loss: 0.14512187\n",
      "Episode: 174 Game over! Score = 10.0\n",
      "total_step: 1675\n",
      "Train episode: 175  loss: 0.14439262\n",
      "Episode: 175 Game over! Score = 10.0\n",
      "total_step: 1685\n",
      "Train episode: 176  loss: 0.1437554\n",
      "Episode: 176 Game over! Score = 9.0\n",
      "total_step: 1694\n",
      "Train episode: 177  loss: 0.14299108\n",
      "Episode: 177 Game over! Score = 9.0\n",
      "total_step: 1703\n",
      "Train episode: 178  loss: 0.14222571\n",
      "Episode: 178 Game over! Score = 9.0\n",
      "total_step: 1712\n",
      "Train episode: 179  loss: 0.14148547\n",
      "Episode: 179 Game over! Score = 9.0\n",
      "total_step: 1721\n",
      "Train episode: 180  loss: 0.14078955\n",
      "Episode: 180 Game over! Score = 10.0\n",
      "total_step: 1731\n",
      "Train episode: 181  loss: 0.14005384\n",
      "Episode: 181 Game over! Score = 9.0\n",
      "total_step: 1740\n",
      "Train episode: 182  loss: 0.13935605\n",
      "Episode: 182 Game over! Score = 10.0\n",
      "total_step: 1750\n",
      "Train episode: 183  loss: 0.13869104\n",
      "Episode: 183 Game over! Score = 9.0\n",
      "total_step: 1759\n",
      "Train episode: 184  loss: 0.13799092\n",
      "Episode: 184 Game over! Score = 10.0\n",
      "total_step: 1769\n",
      "Train episode: 185  loss: 0.13739094\n",
      "Episode: 185 Game over! Score = 10.0\n",
      "total_step: 1779\n",
      "Train episode: 186  loss: 0.13676782\n",
      "Episode: 186 Game over! Score = 9.0\n",
      "total_step: 1788\n",
      "Train episode: 187  loss: 0.13609228\n",
      "Episode: 187 Game over! Score = 8.0\n",
      "total_step: 1796\n",
      "Train episode: 188  loss: 0.13542226\n",
      "Episode: 188 Game over! Score = 9.0\n",
      "total_step: 1805\n",
      "Train episode: 189  loss: 0.13479705\n",
      "Episode: 189 Game over! Score = 9.0\n",
      "total_step: 1814\n",
      "Train episode: 190  loss: 0.13417046\n",
      "Episode: 190 Game over! Score = 9.0\n",
      "total_step: 1823\n",
      "Train episode: 191  loss: 0.13352528\n",
      "Episode: 191 Game over! Score = 11.0\n",
      "total_step: 1834\n",
      "Train episode: 192  loss: 0.13286988\n",
      "Episode: 192 Game over! Score = 9.0\n",
      "total_step: 1843\n",
      "Train episode: 193  loss: 0.13227053\n",
      "Episode: 193 Game over! Score = 10.0\n",
      "total_step: 1853\n",
      "Train episode: 194  loss: 0.13165753\n",
      "Episode: 194 Game over! Score = 11.0\n",
      "total_step: 1864\n",
      "Train episode: 195  loss: 0.13107882\n",
      "Episode: 195 Game over! Score = 10.0\n",
      "total_step: 1874\n",
      "Train episode: 196  loss: 0.13045138\n",
      "Episode: 196 Game over! Score = 10.0\n",
      "total_step: 1884\n",
      "Train episode: 197  loss: 0.12984294\n",
      "Episode: 197 Game over! Score = 8.0\n",
      "total_step: 1892\n",
      "Train episode: 198  loss: 0.12923655\n",
      "Episode: 198 Game over! Score = 10.0\n",
      "total_step: 1902\n",
      "Train episode: 199  loss: 0.12865469\n",
      "Episode: 199 Game over! Score = 9.0\n",
      "total_step: 1911\n",
      "Train episode: 200  loss: 0.12806043\n",
      "Episode: 200 Game over! Score = 9.0\n",
      "total_step: 1920\n",
      "Train episode: 201  loss: 0.12747987\n",
      "Episode: 201 Game over! Score = 10.0\n",
      "total_step: 1930\n",
      "Train episode: 202  loss: 0.12700681\n",
      "Episode: 202 Game over! Score = 10.0\n",
      "total_step: 1940\n",
      "Train episode: 203  loss: 0.12643628\n",
      "Episode: 203 Game over! Score = 9.0\n",
      "total_step: 1949\n",
      "Train episode: 204  loss: 0.12586862\n",
      "Episode: 204 Game over! Score = 10.0\n",
      "total_step: 1959\n",
      "Train episode: 205  loss: 0.12531358\n",
      "Episode: 205 Game over! Score = 10.0\n",
      "total_step: 1969\n",
      "Train episode: 206  loss: 0.12475099\n",
      "Episode: 206 Game over! Score = 10.0\n",
      "total_step: 1979\n",
      "Train episode: 207  loss: 0.1242156\n",
      "Episode: 207 Game over! Score = 10.0\n",
      "total_step: 1989\n",
      "Train episode: 208  loss: 0.12365913\n",
      "Episode: 208 Game over! Score = 10.0\n",
      "total_step: 1999\n",
      "Train episode: 209  loss: 0.12311491\n",
      "Episode: 209 Game over! Score = 10.0\n",
      "total_step: 2009\n",
      "Train episode: 210  loss: 0.12257344\n",
      "Episode: 210 Game over! Score = 11.0\n",
      "total_step: 2020\n",
      "Train episode: 211  loss: 0.12204821\n",
      "Episode: 211 Game over! Score = 10.0\n",
      "total_step: 2030\n",
      "Train episode: 212  loss: 0.12151313\n",
      "Episode: 212 Game over! Score = 8.0\n",
      "total_step: 2038\n",
      "Train episode: 213  loss: 0.120991334\n",
      "Episode: 213 Game over! Score = 9.0\n",
      "total_step: 2047\n",
      "Train episode: 214  loss: 0.12049371\n",
      "Episode: 214 Game over! Score = 10.0\n",
      "total_step: 2057\n",
      "Train episode: 215  loss: 0.12008011\n",
      "Episode: 215 Game over! Score = 9.0\n",
      "total_step: 2066\n",
      "Train episode: 216  loss: 0.119611554\n",
      "Episode: 216 Game over! Score = 10.0\n",
      "total_step: 2076\n",
      "Train episode: 217  loss: 0.11910131\n",
      "Episode: 217 Game over! Score = 10.0\n",
      "total_step: 2086\n",
      "Train episode: 218  loss: 0.118603475\n",
      "Episode: 218 Game over! Score = 9.0\n",
      "total_step: 2095\n",
      "Train episode: 219  loss: 0.11812321\n",
      "Episode: 219 Game over! Score = 9.0\n",
      "total_step: 2104\n",
      "Train episode: 220  loss: 0.11763017\n",
      "Episode: 220 Game over! Score = 9.0\n",
      "total_step: 2113\n",
      "Train episode: 221  loss: 0.11713034\n",
      "Episode: 221 Game over! Score = 10.0\n",
      "total_step: 2123\n",
      "Train episode: 222  loss: 0.11665557\n",
      "Episode: 222 Game over! Score = 10.0\n",
      "total_step: 2133\n",
      "Train episode: 223  loss: 0.11618442\n",
      "Episode: 223 Game over! Score = 10.0\n",
      "total_step: 2143\n",
      "Train episode: 224  loss: 0.115726\n",
      "Episode: 224 Game over! Score = 10.0\n",
      "total_step: 2153\n",
      "Train episode: 225  loss: 0.115238525\n",
      "Episode: 225 Game over! Score = 10.0\n",
      "total_step: 2163\n",
      "Train episode: 226  loss: 0.11479361\n",
      "Episode: 226 Game over! Score = 9.0\n",
      "total_step: 2172\n",
      "Train episode: 227  loss: 0.11435551\n",
      "Episode: 227 Game over! Score = 10.0\n",
      "total_step: 2182\n",
      "Train episode: 228  loss: 0.11391687\n",
      "Episode: 228 Game over! Score = 10.0\n",
      "total_step: 2192\n",
      "Train episode: 229  loss: 0.11345214\n",
      "Episode: 229 Game over! Score = 9.0\n",
      "total_step: 2201\n",
      "Train episode: 230  loss: 0.11299558\n",
      "Episode: 230 Game over! Score = 8.0\n",
      "total_step: 2209\n",
      "Train episode: 231  loss: 0.11253614\n",
      "Episode: 231 Game over! Score = 10.0\n",
      "total_step: 2219\n",
      "Train episode: 232  loss: 0.11209925\n",
      "Episode: 232 Game over! Score = 10.0\n",
      "total_step: 2229\n",
      "Train episode: 233  loss: 0.11166936\n",
      "Episode: 233 Game over! Score = 9.0\n",
      "total_step: 2238\n",
      "Train episode: 234  loss: 0.11121785\n",
      "Episode: 234 Game over! Score = 9.0\n",
      "total_step: 2247\n",
      "Train episode: 235  loss: 0.110828705\n",
      "Episode: 235 Game over! Score = 8.0\n",
      "total_step: 2255\n",
      "Train episode: 236  loss: 0.11039549\n",
      "Episode: 236 Game over! Score = 11.0\n",
      "total_step: 2266\n",
      "Train episode: 237  loss: 0.10996448\n",
      "Episode: 237 Game over! Score = 10.0\n",
      "total_step: 2276\n",
      "Train episode: 238  loss: 0.10954989\n",
      "Episode: 238 Game over! Score = 10.0\n",
      "total_step: 2286\n",
      "Train episode: 239  loss: 0.10915266\n",
      "Episode: 239 Game over! Score = 9.0\n",
      "total_step: 2295\n",
      "Train episode: 240  loss: 0.108738475\n",
      "Episode: 240 Game over! Score = 9.0\n",
      "total_step: 2304\n",
      "Train episode: 241  loss: 0.10841232\n",
      "Episode: 241 Game over! Score = 9.0\n",
      "total_step: 2313\n",
      "Train episode: 242  loss: 0.10798805\n",
      "Episode: 242 Game over! Score = 9.0\n",
      "total_step: 2322\n",
      "Train episode: 243  loss: 0.10757189\n",
      "Episode: 243 Game over! Score = 8.0\n",
      "total_step: 2330\n",
      "Train episode: 244  loss: 0.10725091\n",
      "Episode: 244 Game over! Score = 9.0\n",
      "total_step: 2339\n",
      "Train episode: 245  loss: 0.106845036\n",
      "Episode: 245 Game over! Score = 9.0\n",
      "total_step: 2348\n",
      "Train episode: 246  loss: 0.106479995\n",
      "Episode: 246 Game over! Score = 10.0\n",
      "total_step: 2358\n",
      "Train episode: 247  loss: 0.10606772\n",
      "Episode: 247 Game over! Score = 8.0\n",
      "total_step: 2366\n",
      "Train episode: 248  loss: 0.10566605\n",
      "Episode: 248 Game over! Score = 9.0\n",
      "total_step: 2375\n",
      "Train episode: 249  loss: 0.105293326\n",
      "Episode: 249 Game over! Score = 10.0\n",
      "total_step: 2385\n",
      "Train episode: 250  loss: 0.10489554\n",
      "Episode: 250 Game over! Score = 9.0\n",
      "total_step: 2394\n",
      "Train episode: 251  loss: 0.104511105\n",
      "Episode: 251 Game over! Score = 8.0\n",
      "total_step: 2402\n",
      "Train episode: 252  loss: 0.104176685\n",
      "Episode: 252 Game over! Score = 9.0\n",
      "total_step: 2411\n",
      "Train episode: 253  loss: 0.10383727\n",
      "Episode: 253 Game over! Score = 8.0\n",
      "total_step: 2419\n",
      "Train episode: 254  loss: 0.10345474\n",
      "Episode: 254 Game over! Score = 9.0\n",
      "total_step: 2428\n",
      "Train episode: 255  loss: 0.10307449\n",
      "Episode: 255 Game over! Score = 9.0\n",
      "total_step: 2437\n",
      "Train episode: 256  loss: 0.10273453\n",
      "Episode: 256 Game over! Score = 10.0\n",
      "total_step: 2447\n",
      "Train episode: 257  loss: 0.10236715\n",
      "Episode: 257 Game over! Score = 9.0\n",
      "total_step: 2456\n",
      "Train episode: 258  loss: 0.10199643\n",
      "Episode: 258 Game over! Score = 10.0\n",
      "total_step: 2466\n",
      "Train episode: 259  loss: 0.101640336\n",
      "Episode: 259 Game over! Score = 10.0\n",
      "total_step: 2476\n",
      "Train episode: 260  loss: 0.101288095\n",
      "Episode: 260 Game over! Score = 11.0\n",
      "total_step: 2487\n",
      "Train episode: 261  loss: 0.10093535\n",
      "Episode: 261 Game over! Score = 10.0\n",
      "total_step: 2497\n",
      "Train episode: 262  loss: 0.10060258\n",
      "Episode: 262 Game over! Score = 8.0\n",
      "total_step: 2505\n",
      "Train episode: 263  loss: 0.10024514\n",
      "Episode: 263 Game over! Score = 10.0\n",
      "total_step: 2515\n",
      "Train episode: 264  loss: 0.09991595\n",
      "Episode: 264 Game over! Score = 9.0\n",
      "total_step: 2524\n",
      "Train episode: 265  loss: 0.09956165\n",
      "Episode: 265 Game over! Score = 9.0\n",
      "total_step: 2533\n",
      "Train episode: 266  loss: 0.09924026\n",
      "Episode: 266 Game over! Score = 9.0\n",
      "total_step: 2542\n",
      "Train episode: 267  loss: 0.09889708\n",
      "Episode: 267 Game over! Score = 9.0\n",
      "total_step: 2551\n",
      "Train episode: 268  loss: 0.098556034\n",
      "Episode: 268 Game over! Score = 9.0\n",
      "total_step: 2560\n",
      "Train episode: 269  loss: 0.098226555\n",
      "Episode: 269 Game over! Score = 9.0\n",
      "total_step: 2569\n",
      "Train episode: 270  loss: 0.09793124\n",
      "Episode: 270 Game over! Score = 10.0\n",
      "total_step: 2579\n",
      "Train episode: 271  loss: 0.09759887\n",
      "Episode: 271 Game over! Score = 10.0\n",
      "total_step: 2589\n",
      "Train episode: 272  loss: 0.09728736\n",
      "Episode: 272 Game over! Score = 8.0\n",
      "total_step: 2597\n",
      "Train episode: 273  loss: 0.096963465\n",
      "Episode: 273 Game over! Score = 10.0\n",
      "total_step: 2607\n",
      "Train episode: 274  loss: 0.09665305\n",
      "Episode: 274 Game over! Score = 10.0\n",
      "total_step: 2617\n",
      "Train episode: 275  loss: 0.09633068\n",
      "Episode: 275 Game over! Score = 10.0\n",
      "total_step: 2627\n",
      "Train episode: 276  loss: 0.09602954\n",
      "Episode: 276 Game over! Score = 8.0\n",
      "total_step: 2635\n",
      "Train episode: 277  loss: 0.09571946\n",
      "Episode: 277 Game over! Score = 10.0\n",
      "total_step: 2645\n",
      "Train episode: 278  loss: 0.0953939\n",
      "Episode: 278 Game over! Score = 9.0\n",
      "total_step: 2654\n",
      "Train episode: 279  loss: 0.09508002\n",
      "Episode: 279 Game over! Score = 10.0\n",
      "total_step: 2664\n",
      "Train episode: 280  loss: 0.09478808\n",
      "Episode: 280 Game over! Score = 8.0\n",
      "total_step: 2672\n",
      "Train episode: 281  loss: 0.09447598\n",
      "Episode: 281 Game over! Score = 10.0\n",
      "total_step: 2682\n",
      "Train episode: 282  loss: 0.094162874\n",
      "Episode: 282 Game over! Score = 9.0\n",
      "total_step: 2691\n",
      "Train episode: 283  loss: 0.093864985\n",
      "Episode: 283 Game over! Score = 8.0\n",
      "total_step: 2699\n",
      "Train episode: 284  loss: 0.093557306\n",
      "Episode: 284 Game over! Score = 8.0\n",
      "total_step: 2707\n",
      "Train episode: 285  loss: 0.09325742\n",
      "Episode: 285 Game over! Score = 9.0\n",
      "total_step: 2716\n",
      "Train episode: 286  loss: 0.0929448\n",
      "Episode: 286 Game over! Score = 8.0\n",
      "total_step: 2724\n",
      "Train episode: 287  loss: 0.09266883\n",
      "Episode: 287 Game over! Score = 8.0\n",
      "total_step: 2732\n",
      "Train episode: 288  loss: 0.09238438\n",
      "Episode: 288 Game over! Score = 8.0\n",
      "total_step: 2740\n",
      "Train episode: 289  loss: 0.09209535\n",
      "Episode: 289 Game over! Score = 10.0\n",
      "total_step: 2750\n",
      "Train episode: 290  loss: 0.09182467\n",
      "Episode: 290 Game over! Score = 10.0\n",
      "total_step: 2760\n",
      "Train episode: 291  loss: 0.09156638\n",
      "Episode: 291 Game over! Score = 9.0\n",
      "total_step: 2769\n",
      "Train episode: 292  loss: 0.09130346\n",
      "Episode: 292 Game over! Score = 9.0\n",
      "total_step: 2778\n",
      "Train episode: 293  loss: 0.091013566\n",
      "Episode: 293 Game over! Score = 10.0\n",
      "total_step: 2788\n",
      "Train episode: 294  loss: 0.090739004\n",
      "Episode: 294 Game over! Score = 9.0\n",
      "total_step: 2797\n",
      "Train episode: 295  loss: 0.090458706\n",
      "Episode: 295 Game over! Score = 9.0\n",
      "total_step: 2806\n",
      "Train episode: 296  loss: 0.090192325\n",
      "Episode: 296 Game over! Score = 8.0\n",
      "total_step: 2814\n",
      "Train episode: 297  loss: 0.089922935\n",
      "Episode: 297 Game over! Score = 8.0\n",
      "total_step: 2822\n",
      "Train episode: 298  loss: 0.08963511\n",
      "Episode: 298 Game over! Score = 11.0\n",
      "total_step: 2833\n",
      "Train episode: 299  loss: 0.08935322\n",
      "Episode: 299 Game over! Score = 9.0\n",
      "total_step: 2842\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:01:56.561843Z",
     "start_time": "2024-05-23T17:01:54.978765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(3):\n",
    "    for mini in range(0) :\n",
    "        print(\"Train i:\", i, \" loss:\", test_agent.learn())\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "            action = test_agent.act(state, training=False)\n",
    "            state_prime, reward, done1, done2, info = env.step(action)\n",
    "            done = done1 or done2\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "            state = state_prime\n",
    "    print(\"Run:\",i, \" Game over! Score =\", episode_reward)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0  Game over! Score = 9.0\n",
      "Run: 1  Game over! Score = 10.0\n",
      "Run: 2  Game over! Score = 10.0\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:34:13.394831Z",
     "start_time": "2024-05-23T16:34:12.667957Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03877981 -0.1802956  -0.02313159  0.28733253]\n",
      "Step 1 - Cart X: -0.039, Cart V: -0.180, Pole A: -0.023, Pole V:0.287, Reward:1.0\n",
      "[-0.04238572 -0.37508017 -0.01738494  0.5726311 ]\n",
      "Step 2 - Cart X: -0.042, Cart V: -0.375, Pole A: -0.017, Pole V:0.573, Reward:1.0\n",
      "[-0.04988733 -0.5699541  -0.00593232  0.8597869 ]\n",
      "Step 3 - Cart X: -0.050, Cart V: -0.570, Pole A: -0.006, Pole V:0.860, Reward:1.0\n",
      "[-0.06128641 -0.76499474  0.01126342  1.1505986 ]\n",
      "Step 4 - Cart X: -0.061, Cart V: -0.765, Pole A: 0.011, Pole V:1.151, Reward:1.0\n",
      "[-0.07658631 -0.9602619   0.03427539  1.4467921 ]\n",
      "Step 5 - Cart X: -0.077, Cart V: -0.960, Pole A: 0.034, Pole V:1.447, Reward:1.0\n",
      "[-0.09579154 -1.1557882   0.06321123  1.7499844 ]\n",
      "Step 6 - Cart X: -0.096, Cart V: -1.156, Pole A: 0.063, Pole V:1.750, Reward:1.0\n",
      "[-0.1189073  -1.3515683   0.09821092  2.0616398 ]\n",
      "Step 7 - Cart X: -0.119, Cart V: -1.352, Pole A: 0.098, Pole V:2.062, Reward:1.0\n",
      "[-0.14593868 -1.5475453   0.13944373  2.3830163 ]\n",
      "Step 8 - Cart X: -0.146, Cart V: -1.548, Pole A: 0.139, Pole V:2.383, Reward:1.0\n",
      "[-0.17688958 -1.743594    0.18710405  2.7150989 ]\n",
      "Step 9 - Cart X: -0.177, Cart V: -1.744, Pole A: 0.187, Pole V:2.715, Reward:1.0\n",
      "[-0.21176146 -1.9395034   0.24140602  3.0585222 ]\n",
      "Step 10 - Cart X: -0.212, Cart V: -1.940, Pole A: 0.241, Pole V:3.059, Reward:1.0\n",
      "Game over! Score = 10.0\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning\n",
    "\n",
    "Chances are, at this point, the agent is having a tough time learning. Why is that? Well, remember that hyperparameter tuning job we kicked off at the start of this notebook?\n",
    "\n",
    "The are many parameters that need adjusting with our agent. Let's recap:\n",
    "* The number of `episodes` or full runs of the game to train on\n",
    "* The neural networks `learning_rate`\n",
    "* The number of `hidden_neurons` to use in our network\n",
    "* `gamma`, or how much we want to discount the future value of states\n",
    "* How quickly we want to switch from explore to exploit with `explore_decay`\n",
    "* The size of the memory buffer, `memory_size`\n",
    "* The number of memories to pull from the buffer when training, `memory_batch_size`\n",
    "\n",
    "These all have been added as flags to pass to the model in `trainer/trainer.py`'s `_parse_arguments` method. For the most part, `trainer/trainer.py` follows the structure of the training loop that we have above, but it does have a few extra bells and whistles, like a hook into TensorBoard and video output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T15:30:32.420382Z",
     "start_time": "2024-05-23T15:30:32.411433Z"
    }
   },
   "source": [
    "def _parse_arguments(argv):\n",
    "    \"\"\"Parses command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--game',\n",
    "        help='Which open ai gym game to play',\n",
    "        type=str,\n",
    "        default='CartPole-v0')\n",
    "    parser.add_argument(\n",
    "        '--episodes',\n",
    "        help='The number of episodes to simulate',\n",
    "        type=int,\n",
    "        default=200)\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='Learning rate for the nueral network',\n",
    "        type=float,\n",
    "        default=0.2)\n",
    "    parser.add_argument(\n",
    "        '--hidden_neurons',\n",
    "        help='The number of nuerons to use per layer',\n",
    "        type=int,\n",
    "        default=30)\n",
    "    parser.add_argument(\n",
    "        '--gamma',\n",
    "        help='The gamma or \"discount\" factor to discount future states',\n",
    "        type=float,\n",
    "        default=0.5)\n",
    "    parser.add_argument(\n",
    "        '--explore_decay',\n",
    "        help='The rate at which to decay the probability of a random action',\n",
    "        type=float,\n",
    "        default=0.1)\n",
    "    parser.add_argument(\n",
    "        '--memory_size',\n",
    "        help='Size of the memory buffer',\n",
    "        type=int,\n",
    "        default=100000)\n",
    "    parser.add_argument(\n",
    "        '--memory_batch_size',\n",
    "        help='The amount of memories to sample from the buffer while training',\n",
    "        type=int,\n",
    "        default=8)\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='Directory where to save the given model',\n",
    "        type=str,\n",
    "        default='models/')\n",
    "    parser.add_argument(\n",
    "        '--print_rate',\n",
    "        help='How often to print the score, 0 if never',\n",
    "        type=int,\n",
    "        default=0)\n",
    "    parser.add_argument(\n",
    "        '--eval_rate',\n",
    "        help=\"\"\"While training, perform an on-policy simulation and record\n",
    "        metrics to tensorboard every <record_rate> steps, 0 if never. Use\n",
    "        higher values to avoid hyperparameter tuning \"too many metrics\"\n",
    "        error\"\"\",\n",
    "        type=int,\n",
    "        default=20)\n",
    "    return parser.parse_known_args(argv)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geez, that's a lot. And like with other machine learning methods, there's no hard and fast rule and is problem dependent. Plus, there are many more paramaters we could explore, like the number of layers, learning rate decay, and so on.\n",
    "\n",
    "We can tell Google Cloud how to explore the hyperparameter tuning space with a [config file](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec). The `hyperparam.yaml` file in this directory is exactly that. It specifies which parameter to tune on (in this case, the `episode_reward`) and the range for the different flags we want to tune on.\n",
    "\n",
    "In our code, we'll add the following\n",
    "\n",
    "`import hypertune  #From cloudml-hypertune library`\n",
    "\n",
    "`hpt = hypertune.HyperTune()  # Initialized before looping through episodes`\n",
    "\n",
    "`# Placed right before the end of the training loop\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='episode_reward',\n",
    "    metric_value=reward,\n",
    "    global_step=episode)`\n",
    "  \n",
    "This way, at the end of every episode, we can send information to the tuning service on how the agent is doing. The service can only handle so much information being thrown at it at once, so we'll add a `eval_rate` flag to throttle information to every `eval_rate` episodes.\n",
    "\n",
    "It is definately a worthwhile exercise to try and find the optimal set of parameters on one's on, but if life is too short, and there isn't time for that, the hyperparameter tuning job should now be complete. Head on over to [Google Cloud's AI Platform](https://console.cloud.google.com/ai-platform/jobs) to see the job labeled `dqn_on_gcp_<time_this_lab_was_started>`\n",
    "\n",
    "Click on the job name to see the results. Information comes in as each trial is complete, and the best performing trial will be listed on the top.\n",
    "\n",
    "<img src=\"images/hypertune_trials.jpg\" width=\"966\" height=\"464\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs can be invaluable when debugging. Click the three dots to the right of one of the trials to filter logs by that particular trial.\n",
    "\n",
    "At last, let's see the results of the best trial. Keep in mind the best trial number and navigate over to [your bucket](https://console.cloud.google.com/storage/browser). The results will be in a file with the same Job Name as your hyperparameter tuning job. In that folder, there will be a number of subfolders equal to the number of hyperparameter tuning trials. Select the folder with your best performing `Trial Id`\n",
    "\n",
    "<img src=\"images/best_trial.jpg\" width=\"956\" height=\"456\">\n",
    "\n",
    "There should be a number of goodies in the file including TensorBoard information in `/train`, a saved model in `saved_model.pb`, and a recording of the model in `recording.mp4`.\n",
    "\n",
    "Open the [Google Cloud Shell](https://console.cloud.google.com/home/dashboard?cloudshell=true&_ga=2.207467987.-157492093.1570741979) and run Tensorboard with\n",
    "\n",
    "`tensorboard --logdir=gs://<your-bucket>/<job-name>/<path-best-trial>`\n",
    "\n",
    "The episode rewards and training loss are displayed for the trial in intervals of 20 episodes.\n",
    "\n",
    "<img src=\"images/tensorboard.jpg\" width=\"910\" height=\"708\">\n",
    "\n",
    "Click `recording.mp4` in your bucket to visually see how the model performed! How did it do? If you're not proud of your little robot, check out the recordings of the other trials to see how it decimates the competition.\n",
    "\n",
    "Congratulations on making a Deep Q Agent! That's it for now, but this is just scratching the surface for Reinforcement Learning. AI Gym has plenty of other [environments](https://gym.openai.com/envs/#classic_control), see if you can conquer them with your new skills!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
